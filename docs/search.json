[
  {
    "objectID": "projects/Supply_Chain_Analytics/Throughput/throughput.html",
    "href": "projects/Supply_Chain_Analytics/Throughput/throughput.html",
    "title": "Throughput",
    "section": "",
    "text": "Throughput is a commonly used metric in supply chain analytics. It is the rate at which products, material, people, or information move through the supply chain from point of origin to the point of consumption.\nIf we were speaking of a product or material, you could measure throughput from the supplier (origin) to the customer (end point). In manufacturing, it could be from when the process starts of building or assembling the material, until that process is complete.\nThroughput is essential to a number of practical uses, some of them being customer experience, staffing levels, store layout, or queue management.\nThe formula for throughput is fairly simple:\n\\[ L = \\lambda * W \\]\nWhere:\nToday, we’ll be calculating the parameters within Little’s Law and comapring them to our data, while also creating an inventory build up diagram for a small grocery store named Roger’s Market. Roger’s Market uses Amazon Walk Out technology to track a customers time in a store. We’ll be using the data from the store and the Walk Out technology to determine our average trip duration per customer."
  },
  {
    "objectID": "projects/Supply_Chain_Analytics/Throughput/throughput.html#converting-datetime",
    "href": "projects/Supply_Chain_Analytics/Throughput/throughput.html#converting-datetime",
    "title": "Throughput",
    "section": "Converting Datetime",
    "text": "Converting Datetime\nTo find throughput, we want to know how many customers are present in the store at every minute and the rate of their arrival. Knowing how many customers are in the store every minute will allow us to make an inventory build-up diagram.\nBut first, we need to know what each individual’s entry time is before we can know how many are in the store every minute.\nTo calculate entry time, we’ll need to subtract the trip duration in minutes from the purchase_datetime.\nKnowing python formatting and working through this in the past, I know the current format of purchase_datetime (M/DD/YYYY HH:MM) will cause issues when subtracting another datetime if they’re not in the same format. The default format in python for datetime is (YYYY-MM-DD HH:MM:SS), so we’ll need to convert that.\nI also know that we’ll need to convert trip duration in minutes from its current decimal form to datetime format.\n\nrogers['purchase_datetime'] = pd.to_datetime(rogers['purchase_datetime'])\n\nrogers['entry_datetime'] = rogers['purchase_datetime'] - pd.to_timedelta(rogers['trip_duration_mins'], unit='m')\n\nrogers.head()\n\n\n\n\n\n\n\n\nstore_id\npurchase_datetime\nproduct_title\nsku\nquantity\ntype_of_transaction\ntransaction_id\nsession_id\nproduct_category\nproduct_subcategory\nentry/exit_method\ntrip_duration_mins\ngroup_size\nentry_datetime\n\n\n\n\n0\nHDHRoger'sMarket\n2024-02-28 07:03:00\nCheetos Flamin Hot Limon\n2.840005e+10\n1.0\norder\naadae6df-aeb4-4d42-86c4-4cb41dee44a6\nee05da65-08a6-40e8-bad0-cc97cbb979b0\nNTPM-JWO-GR CHIPS\nNTPM-JWO-GR CHIPS\nApp\n1.0666\n1.0\n2024-02-28 07:01:56.004\n\n\n3\nHDHRoger'sMarket\n2024-02-28 07:09:00\nPepperoni Protein Pack\n9.999910e+10\n3.0\norder\n01a6c049-a7f4-45ee-84fc-3fb8c9d7fc7a\n51be7741-b716-4478-be50-ef2b5f52d69d\nNTPM-JWO-Market Kitchen Active\nNTPM-JWO-Market Kitchen Active\nApp\n0.9500\n1.0\n2024-02-28 07:08:03.000\n\n\n4\nHDHRoger'sMarket\n2024-02-28 07:09:00\nDnkn Dnts Iced Coffee Vanilla\n4.900007e+10\n1.0\norder\n20bc02c6-bd1f-45c3-89c9-7dc1d9f6f259\ne63a092b-2814-4358-a2f9-e9354c72d829\nNTPM-JWO-BEVERAGES\nNTPM-JWO-BEVERAGES\nApp\n0.8500\n1.0\n2024-02-28 07:08:09.000\n\n\n5\nHDHRoger'sMarket\n2024-02-28 07:17:00\nBanana\n9.999000e+10\n1.0\norder\nfa0414df-e886-4a07-9410-ba87ecfda3a5\n92e469b4-d9e7-4028-bb22-c7fb6fde6b8c\nNTPM-JWO-RE-Produce\nNTPM-JWO-RE-Produce\nApp\n0.7666\n1.0\n2024-02-28 07:16:14.004\n\n\n6\nHDHRoger'sMarket\n2024-02-28 07:26:00\nAlta Dena Milk Whole 64oz\n7.039910e+10\n1.0\norder\n4bf7b510-5141-4597-b71f-06d2389c5b70\nedeec561-2161-417a-8099-fd16999a961d\nNTPM-JWO-BEV SOY MILK ALTERNATIVES\nNTPM-JWO-BEV SOY MILK ALTERNATIVES\nApp\n2.7666\n1.0\n2024-02-28 07:23:14.004"
  },
  {
    "objectID": "projects/Supply_Chain_Analytics/Throughput/throughput.html#calculating-inventory-and-creating-inventory-build-up-diagram",
    "href": "projects/Supply_Chain_Analytics/Throughput/throughput.html#calculating-inventory-and-creating-inventory-build-up-diagram",
    "title": "Throughput",
    "section": "Calculating Inventory and creating Inventory Build-Up Diagram",
    "text": "Calculating Inventory and creating Inventory Build-Up Diagram\nNow to create an inventory build-up diagram of the amount of customers in the store by minute, we’ll need to create a dataframe which lists every single possible minute in our time range from our data.\nFor our largest valued time, we’ll want to find the maximum in the purchase_datetime column, since that time will always come after the largest entry time.\nFor our smallest valued time, we’ll want to look at entry time, since that will always be smaller than the smallest purchase time.\n\nrogers['purchase_datetime'].max(), rogers['entry_datetime'].min()\n\n(Timestamp('2024-02-28 23:01:00'), Timestamp('2024-02-28 07:01:56.004000'))\n\n\nThe time range of the data is 07:01 to 23:01. We’ll do a minute before and after for our range to ensure we don’t miss anything that may be rounding up or down due to the number of seconds.\n\ntime_seq = pd.date_range(start=\"2024-02-28 07:00:00\", end=\"2024-02-28 23:05:00\", freq=\"T\")\ndt_customer_count = pd.DataFrame({'time': time_seq, 'customer_count': 0})\n\ndt_customer_count.head()\n\n\n\n\n\n\n\n\ntime\ncustomer_count\n\n\n\n\n0\n2024-02-28 07:00:00\n0\n\n\n1\n2024-02-28 07:01:00\n0\n\n\n2\n2024-02-28 07:02:00\n0\n\n\n3\n2024-02-28 07:03:00\n0\n\n\n4\n2024-02-28 07:04:00\n0\n\n\n\n\n\n\n\nThe “freq = ‘T’” indicates we want the date_range by minute. Don’t ask me why it’s T for minute. “M” must be on a break…\nNow, we must count each customer which is in the store at each minute listed in our dataframe. This gets slightly complicated for our code.\nBelow is just one way to go about it.\n\nfor index, row in rogers.iterrows():\n    entry_time_rounded = row['entry_datetime'].floor('min')\n    mask = (dt_customer_count['time'] &gt;= entry_time_rounded) & (dt_customer_count['time'] &lt; row['purchase_datetime'])\n    dt_customer_count.loc[mask, 'customer_count'] += row['group_size']\n\nFor the above code, first we are iterrating over all the rows in the rogers dataframe, meaning we are getting the index for the row, and all the data in the row itself. We want to do this in order to go row by row, seeing if that person is either in the store, or not in the store based on their entry and purchase time.\nWe then round down the entry time to the nearest minute using the floor function.\nThis is where it can get tricky. The mask variable will return a boolean value (True or False) for every row in dt_customer_count dataframe where the time in the dt_customer_count dataframe is equal to or greater than the entry time of the customer, and where the the dt_customer_count time is less than the purchase time for that customer. Overall, this is returning a True or False for if the customer is in the store at that time.\nFinally, we use the .loc function to locate rows where this is True, and add the group size to the customer count, since some customers come in groups.\nBelow is what the dataframe looks like:\n\ndt_customer_count.head()\n\n\n\n\n\n\n\n\ntime\ncustomer_count\n\n\n\n\n0\n2024-02-28 07:00:00\n0\n\n\n1\n2024-02-28 07:01:00\n1\n\n\n2\n2024-02-28 07:02:00\n1\n\n\n3\n2024-02-28 07:03:00\n0\n\n\n4\n2024-02-28 07:04:00\n0\n\n\n\n\n\n\n\nWhat is the average number of customers in the app every minute?\n\navg_inventory = dt_customer_count['customer_count'].mean()\n\navg_inventory\n\n6.3395445134575565\n\n\nNow, let’s plot the inventory build-up diagram.\n\nplt.figure(figsize=(10, 6))\nplt.plot(dt_customer_count['time'], dt_customer_count['customer_count'])\nplt.title(\"Inventory Build-up Diagram\")\nplt.xlabel(\"Time\")\nplt.ylabel(\"Number of Customers in the Store\")\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\nGoing from 7:00 AM to 10:00 PM, it looks like there is slowly more and more people entering the store. There was also a large influx of people just around 4:00 PM (maybe that’s when people are out of work?)."
  },
  {
    "objectID": "resume.html",
    "href": "resume.html",
    "title": "Nick Shuckerow’s Resume",
    "section": "",
    "text": "Download PDF file."
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Projects",
    "section": "",
    "text": "Click on the below image to show projects for that subject"
  },
  {
    "objectID": "projects.html#projects-by-subject",
    "href": "projects.html#projects-by-subject",
    "title": "Projects",
    "section": "",
    "text": "Click on the below image to show projects for that subject"
  },
  {
    "objectID": "projects.html#marketing-analytics",
    "href": "projects.html#marketing-analytics",
    "title": "Projects",
    "section": "Marketing Analytics",
    "text": "Marketing Analytics"
  },
  {
    "objectID": "projects.html#supply-chain-analytics",
    "href": "projects.html#supply-chain-analytics",
    "title": "Projects",
    "section": "Supply Chain Analytics",
    "text": "Supply Chain Analytics"
  },
  {
    "objectID": "projects/Supply_Chain_Analytics/index.html",
    "href": "projects/Supply_Chain_Analytics/index.html",
    "title": "Supply Chain Projects",
    "section": "",
    "text": "Forecasting Methods\n\n\n\n\n\n\nNicholas Shuckerow\n\n\nJul 1, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThroughput\n\n\n\n\n\n\nNicholas Shuckerow\n\n\nJul 1, 2024\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "projects/Marketing_Analytics/HW1/hw1_questions.html",
    "href": "projects/Marketing_Analytics/HW1/hw1_questions.html",
    "title": "A Replication of Karlan and List (2007)",
    "section": "",
    "text": "Dean Karlan at Yale and John List at the University of Chicago conducted a field experiment to test the effectiveness of different fundraising letters. They sent out 50,000 fundraising letters to potential donors, randomly assigning each letter to one of three treatments: a standard letter, a matching grant letter, or a challenge grant letter. They published the results of this experiment in the American Economic Review in 2007. The article and supporting data are available from the AEA website and from Innovations for Poverty Action as part of Harvard’s Dataverse.\nThe experiment was conducted through a direct mail campaign for a liberal nonprofit organization. The authors assigned 67% of the 50,000 to the treatment group and 33% to the control group.\nThe letters sent to the treatment and control group were the same except in the following regards:\n\nThe treatment group letter had an additional paragraph saying that their donation will be matched.\nThe reply card for the treatment group had details on the matching grant.\n\nThe letters were randomized through 3 different dimensions.\n\nmatching price ratio\nmaximum match amount\nsuggested donation amount\n\nThe experiment found that using matching donations increases the revenue per solicitation and the probability that someone donates.\nIt also found that larger match ratios relative to smaller match ratios have no additional impact. This project seeks to replicate their results.\nOverall, the authors found a unique way to measure the scope of a public good (donations) in a real world setting, rather than hypothetical scenarios. This is a valuable contribution to the literature on charitable giving and public goods."
  },
  {
    "objectID": "projects/Marketing_Analytics/HW1/hw1_questions.html#introduction",
    "href": "projects/Marketing_Analytics/HW1/hw1_questions.html#introduction",
    "title": "A Replication of Karlan and List (2007)",
    "section": "",
    "text": "Dean Karlan at Yale and John List at the University of Chicago conducted a field experiment to test the effectiveness of different fundraising letters. They sent out 50,000 fundraising letters to potential donors, randomly assigning each letter to one of three treatments: a standard letter, a matching grant letter, or a challenge grant letter. They published the results of this experiment in the American Economic Review in 2007. The article and supporting data are available from the AEA website and from Innovations for Poverty Action as part of Harvard’s Dataverse.\nThe experiment was conducted through a direct mail campaign for a liberal nonprofit organization. The authors assigned 67% of the 50,000 to the treatment group and 33% to the control group.\nThe letters sent to the treatment and control group were the same except in the following regards:\n\nThe treatment group letter had an additional paragraph saying that their donation will be matched.\nThe reply card for the treatment group had details on the matching grant.\n\nThe letters were randomized through 3 different dimensions.\n\nmatching price ratio\nmaximum match amount\nsuggested donation amount\n\nThe experiment found that using matching donations increases the revenue per solicitation and the probability that someone donates.\nIt also found that larger match ratios relative to smaller match ratios have no additional impact. This project seeks to replicate their results.\nOverall, the authors found a unique way to measure the scope of a public good (donations) in a real world setting, rather than hypothetical scenarios. This is a valuable contribution to the literature on charitable giving and public goods."
  },
  {
    "objectID": "projects/Marketing_Analytics/HW1/hw1_questions.html#data",
    "href": "projects/Marketing_Analytics/HW1/hw1_questions.html#data",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Data",
    "text": "Data\n\nDescription\nThe dataset seen below which Karlan and List gathered on the 50,083 subjects has over 52 columns or variables. The primary variables we are concerned with for our research are ‘gave’ (if the individual donated any amount), ‘amount’ (what amount they donated), ‘treatment’ (if they were a part of the treatment or control group), and ‘ratio’ (matching ratio given to that subject).\nMany of the columns are redundant (treatmeant vs control columns, ratio columns vs ratio1, ratio2, ratio3 columns), however they allow us to more easily filter the data and make calculations since they are in binary format, rather than categorical (text).\nIn the second cell, you can see the description of variables, including the first two variables, ‘treatment’ and ‘control’. Through this, it is shown that 66% of the participants were a part of the treatment group, and 33% were a part of the control group.\nSince many of these variables are binary, a 1 or 0 is used to describe whether the participant had that treatment, variables, or attribute, which allows analysts to easily calculate proportions or percentages like with treatment and control.\n\nimport pandas as pd\nkarlan = pd.read_stata(\"karlan_list_2007.dta\")\nkarlan\n\n\n\n\n\n\n\n\ntreatment\ncontrol\nratio\nratio2\nratio3\nsize\nsize25\nsize50\nsize100\nsizeno\n...\nredcty\nbluecty\npwhite\npblack\npage18_39\nave_hh_sz\nmedian_hhincome\npowner\npsch_atlstba\npop_propurban\n\n\n\n\n0\n0\n1\nControl\n0\n0\nControl\n0\n0\n0\n0\n...\n0.0\n1.0\n0.446493\n0.527769\n0.317591\n2.10\n28517.0\n0.499807\n0.324528\n1.000000\n\n\n1\n0\n1\nControl\n0\n0\nControl\n0\n0\n0\n0\n...\n1.0\n0.0\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n2\n1\n0\n1\n0\n0\n$100,000\n0\n0\n1\n0\n...\n0.0\n1.0\n0.935706\n0.011948\n0.276128\n2.48\n51175.0\n0.721941\n0.192668\n1.000000\n\n\n3\n1\n0\n1\n0\n0\nUnstated\n0\n0\n0\n1\n...\n1.0\n0.0\n0.888331\n0.010760\n0.279412\n2.65\n79269.0\n0.920431\n0.412142\n1.000000\n\n\n4\n1\n0\n1\n0\n0\n$50,000\n0\n1\n0\n0\n...\n0.0\n1.0\n0.759014\n0.127421\n0.442389\n1.85\n40908.0\n0.416072\n0.439965\n1.000000\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n50078\n1\n0\n1\n0\n0\n$25,000\n1\n0\n0\n0\n...\n0.0\n1.0\n0.872797\n0.089959\n0.257265\n2.13\n45047.0\n0.771316\n0.263744\n1.000000\n\n\n50079\n0\n1\nControl\n0\n0\nControl\n0\n0\n0\n0\n...\n0.0\n1.0\n0.688262\n0.108889\n0.288792\n2.67\n74655.0\n0.741931\n0.586466\n1.000000\n\n\n50080\n0\n1\nControl\n0\n0\nControl\n0\n0\n0\n0\n...\n1.0\n0.0\n0.900000\n0.021311\n0.178689\n2.36\n26667.0\n0.778689\n0.107930\n0.000000\n\n\n50081\n1\n0\n3\n0\n1\nUnstated\n0\n0\n0\n1\n...\n1.0\n0.0\n0.917206\n0.008257\n0.225619\n2.57\n39530.0\n0.733988\n0.184768\n0.634903\n\n\n50082\n1\n0\n3\n0\n1\n$25,000\n1\n0\n0\n0\n...\n0.0\n1.0\n0.530023\n0.074112\n0.340698\n3.70\n48744.0\n0.717843\n0.127941\n0.994181\n\n\n\n\n50083 rows × 51 columns\n\n\n\n\n\n\n\n\n\n\nkarlan.describe()\n\n\n\n\n\n\n\n\ntreatment\ncontrol\nratio2\nratio3\nsize25\nsize50\nsize100\nsizeno\naskd1\naskd2\n...\nredcty\nbluecty\npwhite\npblack\npage18_39\nave_hh_sz\nmedian_hhincome\npowner\npsch_atlstba\npop_propurban\n\n\n\n\ncount\n50083.000000\n50083.000000\n50083.000000\n50083.000000\n50083.000000\n50083.000000\n50083.000000\n50083.000000\n50083.000000\n50083.000000\n...\n49978.000000\n49978.000000\n48217.000000\n48047.000000\n48217.000000\n48221.000000\n48209.000000\n48214.000000\n48215.000000\n48217.000000\n\n\nmean\n0.666813\n0.333187\n0.222311\n0.222211\n0.166723\n0.166623\n0.166723\n0.166743\n0.222311\n0.222291\n...\n0.510245\n0.488715\n0.819599\n0.086710\n0.321694\n2.429012\n54815.700533\n0.669418\n0.391661\n0.871968\n\n\nstd\n0.471357\n0.471357\n0.415803\n0.415736\n0.372732\n0.372643\n0.372732\n0.372750\n0.415803\n0.415790\n...\n0.499900\n0.499878\n0.168560\n0.135868\n0.103039\n0.378105\n22027.316665\n0.193405\n0.186599\n0.258633\n\n\nmin\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n...\n0.000000\n0.000000\n0.009418\n0.000000\n0.000000\n0.000000\n5000.000000\n0.000000\n0.000000\n0.000000\n\n\n25%\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n...\n0.000000\n0.000000\n0.755845\n0.014729\n0.258311\n2.210000\n39181.000000\n0.560222\n0.235647\n0.884929\n\n\n50%\n1.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n...\n1.000000\n0.000000\n0.872797\n0.036554\n0.305534\n2.440000\n50673.000000\n0.712296\n0.373744\n1.000000\n\n\n75%\n1.000000\n1.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n...\n1.000000\n1.000000\n0.938827\n0.090882\n0.369132\n2.660000\n66005.000000\n0.816798\n0.530036\n1.000000\n\n\nmax\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n...\n1.000000\n1.000000\n1.000000\n0.989622\n0.997544\n5.270000\n200001.000000\n1.000000\n1.000000\n1.000000\n\n\n\n\n8 rows × 48 columns\n\n\n\n\nVariable Definitions\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\ntreatment\nTreatment\n\n\ncontrol\nControl\n\n\nratio\nMatch ratio\n\n\nratio2\n2:1 match ratio\n\n\nratio3\n3:1 match ratio\n\n\nsize\nMatch threshold\n\n\nsize25\n$25,000 match threshold\n\n\nsize50\n$50,000 match threshold\n\n\nsize100\n$100,000 match threshold\n\n\nsizeno\nUnstated match threshold\n\n\nask\nSuggested donation amount\n\n\naskd1\nSuggested donation was highest previous contribution\n\n\naskd2\nSuggested donation was 1.25 x highest previous contribution\n\n\naskd3\nSuggested donation was 1.50 x highest previous contribution\n\n\nask1\nHighest previous contribution (for suggestion)\n\n\nask2\n1.25 x highest previous contribution (for suggestion)\n\n\nask3\n1.50 x highest previous contribution (for suggestion)\n\n\namount\nDollars given\n\n\ngave\nGave anything\n\n\namountchange\nChange in amount given\n\n\nhpa\nHighest previous contribution\n\n\nltmedmra\nSmall prior donor: last gift was less than median $35\n\n\nfreq\nNumber of prior donations\n\n\nyears\nNumber of years since initial donation\n\n\nyear5\nAt least 5 years since initial donation\n\n\nmrm2\nNumber of months since last donation\n\n\ndormant\nAlready donated in 2005\n\n\nfemale\nFemale\n\n\ncouple\nCouple\n\n\nstate50one\nState tag: 1 for one observation of each of 50 states; 0 otherwise\n\n\nnonlit\nNonlitigation\n\n\ncases\nCourt cases from state in 2004-5 in which organization was involved\n\n\nstatecnt\nPercent of sample from state\n\n\nstateresponse\nProportion of sample from the state who gave\n\n\nstateresponset\nProportion of treated sample from the state who gave\n\n\nstateresponsec\nProportion of control sample from the state who gave\n\n\nstateresponsetminc\nstateresponset - stateresponsec\n\n\nperbush\nState vote share for Bush\n\n\nclose25\nState vote share for Bush between 47.5% and 52.5%\n\n\nred0\nRed state\n\n\nblue0\nBlue state\n\n\nredcty\nRed county\n\n\nbluecty\nBlue county\n\n\npwhite\nProportion white within zip code\n\n\npblack\nProportion black within zip code\n\n\npage18_39\nProportion age 18-39 within zip code\n\n\nave_hh_sz\nAverage household size within zip code\n\n\nmedian_hhincome\nMedian household income within zip code\n\n\npowner\nProportion house owner within zip code\n\n\npsch_atlstba\nProportion who finished college within zip code\n\n\npop_propurban\nProportion of population urban within zip code\n\n\n\n\n\n\n\n\n\nBalance Test\nAs an ad hoc test of the randomization mechanism, I provide a series of tests that compare aspects of the treatment and control groups to assess whether they are statistically significantly different from one another.\n\n\nMonths since last Donation Variable\n\n# Months since last donation t-test\n\nmrm2_treat = karlan[karlan['treatment'] == 1]['mrm2']\nmrm2_control = karlan[karlan['treatment'] == 0]['mrm2']\n\nmrm2_t_mean = mrm2_treat.mean()\nmrm2_c_mean = mrm2_control.mean()\n\nmrm2_t_std = mrm2_treat.std()\nmrm2_c_std = mrm2_control.std()\n\nmrm2_t_n = mrm2_treat.count()\nmrm2_c_n = mrm2_control.count()\n\nt_mrm2 = (mrm2_t_mean - mrm2_c_mean) / ((mrm2_t_std**2/mrm2_t_n) + (mrm2_c_std**2/mrm2_c_n))**0.5\n\nprint(t_mrm2)\n\n0.11953155228176905\n\n\n\n# calculate p-value\nfrom scipy import stats\n\np_mrm2 = stats.t.sf(abs(t_mrm2), mrm2_t_n + mrm2_c_n - 2) * 2\nprint(p_mrm2)\n\n# Not statistically significant\n\n0.9048547235822526\n\n\n\n# Linear Regression - mrm2\n\nimport pyrsm as rsm\n\nlr_mrm2 = rsm.regress(\n    data = karlan[['treatment', 'mrm2']],\n    evar = \"treatment\",\n    rvar = \"mrm2\"\n    )\n\nlr_mrm2.summary()\n\nLinear regression (OLS)\nData                 : Not provided\nResponse variable    : mrm2\nExplanatory variables: treatment\nNull hyp.: the effect of x on mrm2 is zero\nAlt. hyp.: the effect of x on mrm2 is not zero\n\n           coefficient  std.error  t.value p.value     \nIntercept       12.998      0.094  138.979  &lt; .001  ***\ntreatment        0.014      0.115    0.119   0.905     \n\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nR-squared: 0.0, Adjusted R-squared: -0.0\nF-statistic: 0.014 df(1, 50080), p.value 0.905\nNr obs: 50,082\n\n\n\n\nCouple Variable\n\ncouple_treat = karlan[karlan['treatment'] == 1]['couple']\ncouple_control = karlan[karlan['treatment'] == 0]['couple']\n\ncouple_t_mean = couple_treat.mean()\ncouple_c_mean = couple_control.mean()\n\ncouple_t_std = couple_treat.std()\ncouple_c_std = couple_control.std()\n\ncouple_t_n = couple_treat.count()\ncouple_c_n = couple_control.count()\n\nt_couple = (couple_t_mean - couple_c_mean) / ((couple_t_std**2/couple_t_n) + (couple_c_std**2/couple_c_n))**0.5\n\nprint(t_couple)\n\n-0.5822577486768489\n\n\n\n# calculate p-value\n\np_couple = stats.t.sf(abs(t_couple), couple_t_n + couple_c_n - 2) * 2\nprint(p_couple)\n\n# Not statistically significant\n\n0.5603957630249871\n\n\n\nlr_couple = rsm.regress(\n    data = karlan[['treatment', 'couple']],\n    evar = \"treatment\",\n    rvar = \"couple\"\n    )\n\nlr_couple.summary()\n\nLinear regression (OLS)\nData                 : Not provided\nResponse variable    : couple\nExplanatory variables: treatment\nNull hyp.: the effect of x on couple is zero\nAlt. hyp.: the effect of x on couple is not zero\n\n           coefficient  std.error  t.value p.value     \nIntercept        0.093      0.002   41.124  &lt; .001  ***\ntreatment       -0.002      0.003   -0.584   0.559     \n\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nR-squared: 0.0, Adjusted R-squared: -0.0\nF-statistic: 0.341 df(1, 48933), p.value 0.559\nNr obs: 48,935\n\n\n\n\nBalance Test Results\nThe intercept coefficients calculated for mrm2 and couple variables are exactly the same as in table 1 of the paper for the control group. Also, when incorporating the coefficients for treatment (treatment = 1), they also equal the mean values in table 1 for the treatment group.\nFor mrm2 (months since last donation), the control group mean was 12.998 and that increases to 13.012 if they were a part of the treatment group. This is not a large disparity and as such did not prove to be statistically significant on a 95% confidence interval.\nFor couple (whether the donor was a couple), the control group mean was 0.093 (interpreted as 9.3% of donors in the control group were couples) and that decreases to 0.091 if they were a part of the treatment group. This is also not a large disparity and as such did not prove to be statistically significant on a 95% confidence interval."
  },
  {
    "objectID": "projects/Marketing_Analytics/HW1/hw1_questions.html#experimental-results",
    "href": "projects/Marketing_Analytics/HW1/hw1_questions.html#experimental-results",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Experimental Results",
    "text": "Experimental Results\n\nCharitable Contribution Made\nFirst, I analyze whether matched donations lead to an increased response rate of making a donation.\n\n\n# find proportion of treatment group that gave money\n\ntreat_gave = karlan[karlan['treatment'] == 1]['gave'].mean()\ncontrol_gave = karlan[karlan['treatment'] == 0]['gave'].mean()\n\nprint(treat_gave, control_gave)\n\n0.02203856749311295 0.017858212980164198\n\n\n\n# create bar graph for proportion of treatment/control group that gave money\n\nimport matplotlib.pyplot as plt\n\nfig, ax = plt.subplots()\nax.bar(['Treatment', 'Control'], [treat_gave, control_gave])\nax.set_ylabel('Proportion of Group that Gave Money')\nax.set_title('Proportion of Treatment and Control Group that Gave Money')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n# Calculate t-stat for those who donated if they were a part of treatment or control group\n\ngave_treat = karlan[karlan['treatment'] == 1]['gave']\ngave_control = karlan[karlan['treatment'] == 0]['gave']\n\ngave_t_mean = gave_treat.mean()\ngave_c_mean = gave_control.mean()\n\ngave_t_std = gave_treat.std()\ngave_c_std = gave_control.std()\n\ngave_t_n = gave_treat.count()\ngave_c_n = gave_control.count()\n\nt_gave = (gave_t_mean - gave_c_mean) / ((gave_t_std**2/gave_t_n) + (gave_c_std**2/gave_c_n))**0.5\n\nprint(t_gave)\n\n3.2094621908279835\n\n\n\n# Calculate p-value on 95% confidence interval\n\np_gave = stats.t.sf(abs(t_gave), gave_t_n + gave_c_n - 2) * 2\np_gave\n\n# Statistically significant\n\n0.0013306730060655475\n\n\n\n# Run a linear regression which 'gave' is the response variable and 'treatment' is the explanatory variable\n\nlr_gave = rsm.regress(\n    data = karlan[['treatment', 'gave']],\n    evar = \"treatment\",\n    rvar = \"gave\"\n    )\n\nlr_gave.summary()\n\nLinear regression (OLS)\nData                 : Not provided\nResponse variable    : gave\nExplanatory variables: treatment\nNull hyp.: the effect of x on gave is zero\nAlt. hyp.: the effect of x on gave is not zero\n\n           coefficient  std.error  t.value p.value     \nIntercept        0.018      0.001   16.225  &lt; .001  ***\ntreatment        0.004      0.001    3.101   0.002   **\n\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nR-squared: 0.0, Adjusted R-squared: 0.0\nF-statistic: 9.618 df(1, 50081), p.value 0.002\nNr obs: 50,083\n\n\nThe outcome of our t-test and linear regression were both similar in that it was found at a 95% confidence interval that the treatment did cause an increase in the number of donations. The linear regression did not explain any variance in the data, however that is not as important because we are using what should be a logistic regression (binary outcome of 1 or 0 if they donated or not) with a linear regression model.\n\n\n# Run probit regression on gave and treatment variables\n\nimport statsmodels.formula.api as smf\n\nmod = smf.probit('gave ~ treatment', data=karlan)\nres = mod.fit()\nres.summary()\n\nOptimization terminated successfully.\n         Current function value: 0.100443\n         Iterations 7\n\n\n\nProbit Regression Results\n\n\nDep. Variable:\ngave\nNo. Observations:\n50083\n\n\nModel:\nProbit\nDf Residuals:\n50081\n\n\nMethod:\nMLE\nDf Model:\n1\n\n\nDate:\nWed, 12 Jun 2024\nPseudo R-squ.:\n0.0009783\n\n\nTime:\n12:17:44\nLog-Likelihood:\n-5030.5\n\n\nconverged:\nTrue\nLL-Null:\n-5035.4\n\n\nCovariance Type:\nnonrobust\nLLR p-value:\n0.001696\n\n\n\n\n\n\n\ncoef\nstd err\nz\nP&gt;|z|\n[0.025\n0.975]\n\n\nIntercept\n-2.1001\n0.023\n-90.073\n0.000\n-2.146\n-2.054\n\n\ntreatment\n0.0868\n0.028\n3.113\n0.002\n0.032\n0.141\n\n\n\n\n\n\n\nDifferences between Match Rates\nNext, I assess the effectiveness of different sizes of matched donations on the response rate.\n\nRatio of 1:1 vs Ratio of 2:1\n\n# Calculate t-stat for Ratio 1:1 vs Ratio 2:1\n\ngave_ratio1 = karlan[karlan['ratio'] == 1]['gave']\ngave_ratio2 = karlan[karlan['ratio'] == 2]['gave']\n\ngave_1_mean = gave_ratio1.mean()\ngave_2_mean = gave_ratio2.mean()\n\ngave_1_std = gave_ratio1.std()\ngave_2_std = gave_ratio2.std()\n\ngave_1_n = gave_ratio1.count()\ngave_2_n = gave_ratio2.count()\n\nt_gave_ratio = (gave_2_mean - gave_1_mean) / ((gave_2_std**2/gave_2_n) + (gave_1_std**2/gave_1_n))**0.5\n\nprint(t_gave_ratio)\n\n0.965048975142932\n\n\n\np_gave_ratio = stats.t.sf(abs(t_gave_ratio), gave_2_n + gave_1_n - 2) * 2\np_gave_ratio\n\n# Not statistically significant\n\n0.3345307635444439\n\n\n\n\nRatio of 2:1 vs Ratio of 3:1\n\ngave_ratio3 = karlan[karlan['ratio'] == 3]['gave']\ngave_ratio2 = karlan[karlan['ratio'] == 2]['gave']\n\ngave_3_mean = gave_ratio3.mean()\ngave_2_mean = gave_ratio2.mean()\n\ngave_3_std = gave_ratio3.std()\ngave_2_std = gave_ratio2.std()\n\ngave_3_n = gave_ratio3.count()\ngave_2_n = gave_ratio2.count()\n\nt_gave_ratio_23 = (gave_2_mean - gave_3_mean) / ((gave_2_std**2/gave_2_n) + (gave_3_std**2/gave_3_n))**0.5\n\nprint(t_gave_ratio_23)\n\n-0.05011581369764474\n\n\n\np_gave_ratio_23 = stats.t.sf(abs(t_gave_ratio_23), gave_2_n + gave_3_n - 2) * 2\np_gave_ratio_23\n\n# Not statistically significant\n\n0.9600305476910405\n\n\nThe above calculations matches what Karlan suggests in his paper, that increasing the match ratio does not increase the probability of making a donation. The above calculations show a 1:1 match ratio when compared to a 2:1 match ratio, and a 2:1 match ratio when compared to a 3:1 match ratio. We did a two-sided t-test, which has a null hypothesis stating that 2:1 is not the same as 1:1, and 3:1 is not the same as 2:1.\n\n\n#create a variable ratio1 where if ratio column equals 1, then ratio1 equlas 1, else 0\n\nkarlan['ratio1'] = karlan['ratio'].apply(lambda x: 1 if x == 1 else 0)\n\n\n# Linear regression for match ratios and treatment\n\nlr_ratio = rsm.regress(\n    data = karlan[['gave', 'ratio1', 'ratio2', 'ratio3']],\n    evar = ['ratio1', 'ratio2', 'ratio3'],\n    rvar = 'gave'\n    )  \n\nlr_ratio.summary()\n\nLinear regression (OLS)\nData                 : Not provided\nResponse variable    : gave\nExplanatory variables: ratio1, ratio2, ratio3\nNull hyp.: the effect of x on gave is zero\nAlt. hyp.: the effect of x on gave is not zero\n\n           coefficient  std.error  t.value p.value     \nIntercept        0.018      0.001   16.225  &lt; .001  ***\nratio1           0.003      0.002    1.661   0.097    .\nratio2           0.005      0.002    2.744   0.006   **\nratio3           0.005      0.002    2.802   0.005   **\n\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nR-squared: 0.0, Adjusted R-squared: 0.0\nF-statistic: 3.665 df(3, 50079), p.value 0.012\nNr obs: 50,083\n\n\nThe results of linear regression model on match ratios of 1:1, 2:1, and 3:1 show that all but a ratio of 1:1 are statistically significant. The intercept coefficient (meaning when there is no match) is 0.018, and then for ratio 1:1 it has a coefficient of 0.003, which added to 0.018 is 0.021. For ratio 2:1 and 3:1, they both have coefficients of 0.005, creating a response rate of 0.023 for both. All these response rates match table 2A in the paper.\nThe precision of these estimates also match what is in the paper for standard error. Each ratio has a standard error of 0.002, meaning that ratio 1:1 could have a varying effect on response rate between 0.001 and 0.005, and ratios 2:1 and 3:1 could vary between 0.003 and 0.007. All however remain positive when incorporating standard error, meaning that the match ratio does have a positive effect on response rate when comparing to control.\n\n\n\n\n\nSize of Charitable Contribution\nIn this subsection, I analyze the effect of the size of matched donation on the size of the charitable contribution.\n\n\n# Linear Regression on Treatment to Predict donation amount\nlr_d_amount = rsm.regress(\n    data = karlan[['amount', 'treatment']],\n    evar = 'treatment',\n    rvar = 'amount'\n    )  \n\nlr_d_amount.summary()\n\nLinear regression (OLS)\nData                 : Not provided\nResponse variable    : amount\nExplanatory variables: treatment\nNull hyp.: the effect of x on amount is zero\nAlt. hyp.: the effect of x on amount is not zero\n\n           coefficient  std.error  t.value p.value     \nIntercept        0.813      0.067   12.063  &lt; .001  ***\ntreatment        0.154      0.083    1.861   0.063    .\n\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nR-squared: 0.0, Adjusted R-squared: 0.0\nF-statistic: 3.461 df(1, 50081), p.value 0.063\nNr obs: 50,083\n\n\nFrom our linear regression model to calculate dollars donated amount from whether the donor was in the treatment or control group, we can conclude that the control group donates $0.813 and being a part of the treatment group increases that amount by $0.154, bringing the estimated dollars donated amount to $0.967. Per a 95% confidence interval, the model is not statistically significant, however it is very close to being so.\n\n# Linear Regression on treatment to predict donation amounts above 0\n\nlr_d_amount_above0 = rsm.regress(\n    data = karlan[karlan['amount'] &gt; 0][['amount', 'treatment']],\n    evar = 'treatment',\n    rvar = 'amount'\n    )  \n\nlr_d_amount_above0.summary()\n\nLinear regression (OLS)\nData                 : Not provided\nResponse variable    : amount\nExplanatory variables: treatment\nNull hyp.: the effect of x on amount is zero\nAlt. hyp.: the effect of x on amount is not zero\n\n           coefficient  std.error  t.value p.value     \nIntercept       45.540      2.423   18.792  &lt; .001  ***\ntreatment       -1.668      2.872   -0.581   0.561     \n\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nR-squared: 0.0, Adjusted R-squared: -0.001\nF-statistic: 0.337 df(1, 1032), p.value 0.561\nNr obs: 1,034\n\n\nAfter filtering the data to only include those who donated, the linear regression model changed significantly. The intercept coefficient is now $45.54, and the treatment coefficient is now negative $1.67. Now, the treatment group has a negative affect on amount donated, meaning that the control group donates $45.54 and the treatment group donates $43.87.\nHowever, the model is not statistically significant at 95% confidence interval.\n\n\n\n# create a histogram for all the donation amounts above 0 for the control group\n\nfig, ax = plt.subplots()\nax.hist(karlan[(karlan['amount'] &gt; 0) & (karlan['treatment'] == 0)]['amount'], bins=25)\nax.axvline(43.87, color='red')\nax.text(43.87 + 1, ax.get_ylim()[1] * 0.9, f'{43.87}', color='red')\nax.set_ylabel('Frequency')\nax.set_xlabel('Donation Amount')\nax.set_title('Frequency of Donation Amounts for Control Group')\nplt.show()\n\n\n\n\n\n\n\n\n\n# create a histogram for all the donation amounts above 0 for the treatment group\n\nfig, ax = plt.subplots()\nax.hist(karlan[(karlan['amount'] &gt; 0) & (karlan['treatment'] == 1)]['amount'], bins=25)\nax.axvline(45.54, color='red')\nax.text(45.54 + 1, ax.get_ylim()[1] * 0.9, f'{45.54}', color='red')\nax.set_ylabel('Frequency')\nax.set_xlabel('Donation Amount')\nax.set_title('Frequency of Donation Amounts for Treatment Group')\nplt.show()"
  },
  {
    "objectID": "projects/Marketing_Analytics/HW1/hw1_questions.html#simulation-experiment",
    "href": "projects/Marketing_Analytics/HW1/hw1_questions.html#simulation-experiment",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Simulation Experiment",
    "text": "Simulation Experiment\nAs a reminder of how the t-statistic “works,” in this section I use simulation to demonstrate the Law of Large Numbers and the Central Limit Theorem.\nSuppose the true distribution of respondents who do not get a charitable donation match is Bernoulli with probability p=0.018 that a donation is made.\nFurther suppose that the true distribution of respondents who do get a charitable donation match of any size is Bernoulli with probability p=0.022 that a donation is made.\n\nLaw of Large Numbers\n\n\n# use a bernoulli distribution to simulate 10,000 trials for control and treatment groups using their mean percentage donated as the probabilities respectively\n\nimport numpy as np\nfrom scipy.stats import bernoulli\n\ncontrol = karlan[karlan['treatment'] == 0]['gave'].mean()\ntreatment = karlan[karlan['treatment'] == 1]['gave'].mean()\n\ncontrol_sim = bernoulli.rvs(p = control, size = 10000)\ntreatment_sim = bernoulli.rvs(p = treatment, size = 10000)\n\n# calculate cumulative average of the differences for the first 10,000 draws\n\ncum_avg = np.cumsum(treatment_sim - control_sim) / np.arange(1, 10001)\n\ncum_avg\n\narray([0.        , 0.        , 0.        , ..., 0.0045009 , 0.00460046,\n       0.0047    ])\n\n\n\n# plot the cumulative average of the differences with a line plot\n\nfig, ax = plt.subplots()\nax.plot(cum_avg)\nax.axhline(treatment - control, color='red')\nax.set_ylabel('Cumulative Average of Differences')\nax.set_xlabel('Number of Draws')\nax.set_title('Cumulative Average of Differences in Proportion of Giving Money')\nplt.show()\n\n\n\n\n\n\n\n\nThe above graph shows a simulation of the differences between simulated probabilities of an individual actually donating in the treatment group and an individual donating in the control group. The control group was given a probability of 0.018 and treatment group was given a probability of 0.022, which were both calculating from the given dataset.\nWe simulated this for the control and treatment group 10,000 times to generate the graph. The red horizontal line on the graph shows the calculated difference between the treatment and control probabilities from the dataset (0.004).\nAs shown, the cumulative difference between the treatment and control group varies greatly initially, then slowly starts to vary less and less around the average difference of 0.004. With more trials, it is expected there would be even less variability and the cumulative difference would be extremely close to 0.004.\n\n\nCentral Limit Theorem\n\n\nSimulating 50 trials\n\nn_50_c = np.random.binomial(50, 0.018, 1000)\n\nn_50_c = n_50_c / 50\n\nn_50_t = np.random.binomial(50, 0.022, 1000)\n\nn_50_t = n_50_t / 50\n\nn_50 = n_50_t - n_50_c\n\n# make a histogram of n_50 so there is no spacing between the bars\n\nfig, ax = plt.subplots()\nax.hist(n_50, bins=10, rwidth=1)\nax.axvline(n_50.mean(), color='red')\nax.text(n_50.mean(), ax.get_ylim()[1]*0.9, f'{np.round(n_50.mean(),4)}', color='red', verticalalignment='center', horizontalalignment='center')\nax.set_ylabel('Frequency')\nax.set_xlabel('Difference in Proportion of Donors (Treatment - Control) in 50 Draws')\nax.set_title('Frequency of Proportion of Control Group that Gave Money in 50 Draws')\nplt.show()\n\n\n\n\n\n\n\n\n\n\nSimulating 200 trials\n\nn_200_c = np.random.binomial(200, 0.018, 1000)/200\n\nn_200_t = np.random.binomial(200, 0.022, 1000)/200\n\n\nn_200 = n_200_t - n_200_c\n\nn_200_mean = n_200.mean()\n\nfig, ax = plt.subplots()\nax.hist(n_200, bins=10, rwidth=1)\nax.axvline(n_200_mean, color='red')\nax.text(n_200_mean, ax.get_ylim()[1]*0.9, f'{np.round(n_200_mean,4)}', color='red', verticalalignment='center', horizontalalignment='center')\nax.set_ylabel('Frequency')\nax.set_xlabel('Difference in Proportion of Donors (Treatment - Control) in 200 Draws')\nax.set_title('Frequency of Proportion of Control Group that Gave Money in 200 Draws')\nplt.show()\n\n\n\n\n\n\n\n\n\n\nSimulating 500 trials\n\nn_500_c = np.random.binomial(500, 0.018, 1000)/500\n\nn_500_t = np.random.binomial(500, 0.022, 1000)/500\n\n\nn_500 = n_500_t - n_500_c\n\nn_500_mean = n_500.mean()\n\nfig, ax = plt.subplots()\nax.hist(n_500, bins=10, rwidth=1)\nax.axvline(n_500_mean, color='red')\nax.text(n_500_mean, ax.get_ylim()[1]*0.9, f'{np.round(n_500_mean,4)}', color='red', verticalalignment='center', horizontalalignment='center')\nax.set_ylabel('Frequency')\nax.set_xlabel('Difference in Proportion of Donors (Treatment - Control) in 500 Draws')\nax.set_title('Frequency of Proportion of Control Group that Gave Money in 500 Draws')\nplt.show()\n\n\n\n\n\n\n\n\n\n\nSimulating 1000 trials\n\nn_1000_c = np.random.binomial(1000, 0.018, 1000)/1000\n\nn_1000_t = np.random.binomial(1000, 0.022, 1000)/1000\n\nn_1000 = n_1000_t - n_1000_c\n\nn_1000_mean = n_1000.mean()\n\nfig, ax = plt.subplots()\nax.hist(n_1000, bins=10, rwidth=1)\nax.axvline(n_1000_mean, color='red')\nax.text(n_1000_mean, ax.get_ylim()[1]*0.9, f'{np.round(n_1000_mean,4)}', color='red', verticalalignment='center', horizontalalignment='right')\nax.set_ylabel('Frequency')\nax.set_xlabel('Difference in Proportion of Donors (Treatment - Control) in 500 Draws')\nax.set_title('Frequency of Proportion of Control Group that Gave Money in 500 Draws')\nplt.show()\n\n\n\n\n\n\n\n\nFrom the above histograms, you can see from the red vertical line that the average difference decreases as we increase the trial size of the simulation and starts to get closer to 0.004, which is the calculated difference from the dataset between the percentage that donated from the treatment group versus the control group.\nAlthough the 0 mark on the x-axis varies because of graph sizing, you can see that the outer limits of the distribution gets smaller as we increase the number of trials. This follows the central limit theorem, since as we increase the number of trials in each simulation, we will get closer and closer to the true mean or percentage. The variation or wide distribution we see in the 50 trial simulation is no longer there in the 1000 trial distribution.\nOverall, the more trials we have in our simulation or experiment, the closer we will get to the mean."
  },
  {
    "objectID": "projects/Marketing_Analytics/HW4/hw4_questions.html",
    "href": "projects/Marketing_Analytics/HW4/hw4_questions.html",
    "title": "Key Drivers Analysis",
    "section": "",
    "text": "This post implements a few measure of variable importance, interpreted as a key drivers analysis, for certain aspects of a payment card on customer satisfaction with that payment card.\n\n1. Dataset Research\nBefore we go into any complex analysis, we first want to get familar with the data.\n\nimport pandas as pd\n\ndata = pd.read_csv('data_for_drivers_analysis.csv')\n\ndata.head()\n\n\n\n\n\n\n\n\nbrand\nid\nsatisfaction\ntrust\nbuild\ndiffers\neasy\nappealing\nrewarding\npopular\nservice\nimpact\n\n\n\n\n0\n1\n98\n3\n1\n0\n1\n1\n1\n0\n0\n1\n0\n\n\n1\n1\n179\n5\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n2\n1\n197\n3\n1\n0\n0\n1\n1\n1\n0\n1\n1\n\n\n3\n1\n317\n1\n0\n0\n0\n0\n1\n0\n1\n1\n1\n\n\n4\n1\n356\n4\n1\n1\n1\n1\n1\n1\n1\n1\n1\n\n\n\n\n\n\n\n\nprint(f'The number of rows in the dataset are {data.shape[0]}.')\nprint(f'The number of unique IDs in the dataset are {data.id.nunique()}.')\nprint(f'The number of unique brands in the dataset are {data.brand.nunique()}.')\n\nThe number of rows in the dataset are 2553.\nThe number of unique IDs in the dataset are 940.\nThe number of unique brands in the dataset are 10.\n\n\n\ndata.describe()\n\n\n\n\n\n\n\n\nbrand\nid\nsatisfaction\ntrust\nbuild\ndiffers\neasy\nappealing\nrewarding\npopular\nservice\nimpact\n\n\n\n\ncount\n2553.000000\n2553.000000\n2553.000000\n2553.000000\n2553.000000\n2553.000000\n2553.000000\n2553.000000\n2553.000000\n2553.000000\n2553.000000\n2553.000000\n\n\nmean\n4.857423\n8931.480611\n3.386604\n0.549550\n0.461810\n0.334508\n0.536232\n0.451234\n0.451234\n0.536232\n0.467293\n0.330983\n\n\nstd\n2.830096\n5114.287849\n1.172006\n0.497636\n0.498637\n0.471911\n0.498783\n0.497714\n0.497714\n0.498783\n0.499027\n0.470659\n\n\nmin\n1.000000\n88.000000\n1.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n\n\n25%\n3.000000\n4310.000000\n3.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n\n\n50%\n4.000000\n8924.000000\n4.000000\n1.000000\n0.000000\n0.000000\n1.000000\n0.000000\n0.000000\n1.000000\n0.000000\n0.000000\n\n\n75%\n6.000000\n13545.000000\n4.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n\n\nmax\n10.000000\n18088.000000\n5.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n\n\n\n\n\n\n\nAfter looking at our data, we can see that satisfaction is the dependent variable, which is scored based on a range of 1-5. There are numerous different categories which affect the satisfaction score. All those categories are binary based on what the customer thinks about that card brand.\nNow we’re ready to get into some deeper analysis\n\n\n2. Pearson Correlations\nThe analysis we’ll conduct is determining the Pearson Correlations for this dataset. That is, how each indepenent variable is correlated to satisfaction.\nWe’ll be using the corr function in python to determine our correlations in respect to satisfaction.\n\n# Calculate pearson correlations with satisfaction being the y variable and the rest being the x variables\n\nsatisfaction = data.drop(['id', 'brand'], axis=1)\n\ntable = satisfaction.corr()['satisfaction'].sort_values(ascending=False)\n\n# drop satisfaction from table\n\ntable = table.drop('satisfaction')\n\ntable = pd.DataFrame(table)\n\ntable = table.rename(columns={'satisfaction': 'Pearson_Corr'})\n\ntable['Pearson_Corr_%'] = round(table['Pearson_Corr']/table['Pearson_Corr'].sum(), 3)*100\n\ntable = table.drop('Pearson_Corr', axis = 1)\ntable\n\n\n\n\n\n\n\n\nPearson_Corr_%\n\n\n\n\ntrust\n13.3\n\n\nimpact\n13.2\n\n\nservice\n13.0\n\n\neasy\n11.1\n\n\nappealing\n10.8\n\n\nrewarding\n10.1\n\n\nbuild\n10.0\n\n\ndiffers\n9.6\n\n\npopular\n8.9\n\n\n\n\n\n\n\nFrom our calculations, we see that trust has the highest correlation to satisfaction, followed by impact and service. Popularity seems to be least correlated with satisfaction. This does not mean that trust is the most important variable when looking at satisfaction, however. It simply means it follows the most similar path (up or down) as satisfaction with respect to all the other variables.\n\n\n3. Polychoric Correlations\nPolychoric correlations are most notably used for ordinal variables, or variables that have a specific rank order, for instance military ranks (Captain, Colonel, General, etc).\nOur satisfaction variable is an ordinal variable which ranges from 1-5.\nFor this calculation, we used an R package within python. R has a much simpler package for conducting these calculations.\n\nimport rpy2.robjects as robjects\nfrom rpy2.robjects import pandas2ri\n\n# Activate the pandas2ri conversion\npandas2ri.activate()\n\n\n# Specify the columns of interest\ncolumns_of_interest = ['trust', 'build', 'differs', 'easy', 'appealing', 'rewarding', 'popular', 'service', 'impact']\n\n# Initialize a list to store the results\ncorrelation = []\n\n# Define the R code for calculating polychoric correlation\nr_code = \"\"\"\nlibrary(polycor)\npolychoric_corr &lt;- function(x, y) {\n    result &lt;- polychor(x, y)\n    return(result)\n}\n\"\"\"\n\n# Load the R code into the R environment\nrobjects.r(r_code)\n\n# Get the polychoric_corr function\npolychoric_corr = robjects.globalenv['polychoric_corr']\n\n# Calculate polychoric correlations between 'satisfaction' and each specified column\nfor col in columns_of_interest:\n    r_corr = polychoric_corr(data['satisfaction'], data[col])\n    correlation.append(r_corr[0])\n\n# Convert correlations to a pandas DataFrame\ncorrelation_df = pd.DataFrame({\n    'Variable': columns_of_interest,\n    'Polychoric_Corr': correlation\n})\n\n# Calculate the sum of the polychoric correlations\nsum_polychoric_correlations = correlation_df['Polychoric_Corr'].sum()\n\n# Calculate the percentage of each polychoric correlation\ncorrelation_df['Polychoric_Corr_%'] = (correlation_df['Polychoric_Corr'] / sum_polychoric_correlations).round(3)*100\n\n\n# make variable column the index\n\ncorrelation_df.set_index('Variable', inplace=True)\n\n# merge correlation_df with correlations\n\ntable = table.merge(correlation_df, left_index=True, right_index=True).drop('Polychoric_Corr', axis=1)\n\n\ntable\n\n\n\n\n\n\n\n\nPearson_Corr_%\nPolychoric_Corr_%\n\n\n\n\ntrust\n13.3\n12.9\n\n\nimpact\n13.2\n13.8\n\n\nservice\n13.0\n13.0\n\n\neasy\n11.1\n10.9\n\n\nappealing\n10.8\n10.6\n\n\nrewarding\n10.1\n10.1\n\n\nbuild\n10.0\n9.9\n\n\ndiffers\n9.6\n10.0\n\n\npopular\n8.9\n8.9\n\n\n\n\n\n\n\nAs we can see from the data, the groups are very similar to Pearson’s Correlations, however they are in slightly different orders. The top three correlated variables from Pearson’s (Trust, Impact, Service) are still the top 3, however the order is Impact, Service, and Trust.\nAlso, in our bottom 3 variables (Build, Differs, and Popular) from Pearson’s, Polychoric varies slightly with Differs, Build and Popular being the order from most to least. Overall, both Pearson’s and Polychoric correlations are very similar.\n\n\n4. Standardized Multiple Regression Coefficients\nStandardized regression coefficients are used for a different purpose than our last two methods. Pearson’s and Polychoric used correlations, which do not necessarily relate to importance of each variable on the dependent variable. Standardized regression coefficients measure importance for each variable in a regression analysis by scaling all the independent variables so they are now all on equal scales. The scaled indepenent variables are then fit to a regression model where their betas or standardized coefficients are generated.\nFirst, we’ll scale and fit our data to get our coefficients, then find their importance.\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LinearRegression\n\n# Initialize the StandardScaler\n\nscaler = StandardScaler()\n\n# Fit the scaler to the data\n\nscaler.fit(data.drop(['id', 'brand', 'satisfaction'], axis=1))\n\n# Transform the data\n\nscaled_data = scaler.transform(data.drop(['id', 'brand', 'satisfaction'], axis=1))\n\n# Convert the scaled data to a DataFrame\n\nscaled_data = pd.DataFrame(scaled_data, columns=data.drop(['id', 'brand', 'satisfaction'], axis=1).columns)\n\n# Add the 'satisfaction' column to the scaled data\n\nscaled_data['satisfaction'] = data['satisfaction']\n\n# Initialize the LinearRegression model\n\nmodel = LinearRegression()\n\n# Fit the model to the scaled data\n\nmodel.fit(scaled_data.drop('satisfaction', axis=1), scaled_data['satisfaction'])\n\ncoefficients = model.coef_\n\ntable['Std_Coefficients'] = coefficients\n\ntable['Std_Coef_%'] = (table['Std_Coefficients']/table['Std_Coefficients'].sum())*100\n\ntable = table.drop('Std_Coefficients', axis=1)\n\ntable\n\n\n\n\n\n\n\n\nPearson_Corr_%\nPolychoric_Corr_%\nStd_Coef_%\n\n\n\n\ntrust\n13.3\n12.9\n25.280109\n\n\nimpact\n13.2\n13.8\n4.363461\n\n\nservice\n13.0\n13.0\n6.081798\n\n\neasy\n11.1\n10.9\n4.798205\n\n\nappealing\n10.8\n10.6\n7.389479\n\n\nrewarding\n10.1\n10.1\n1.106526\n\n\nbuild\n10.0\n9.9\n3.628859\n\n\ndiffers\n9.6\n10.0\n19.304269\n\n\npopular\n8.9\n8.9\n28.047294\n\n\n\n\n\n\n\nThe ranked order for importance based on Standardized Regression Coefficients changes greatly when compared to our correlations. Popularity, which had the lowest correlational values for the first two calculations, now has the highest value. Trust is still at the top in second place, but Differs is now in the top 3 as well. Impact is also towards the bottom of the pack which was towards the top in correlations.\n\n\n5. Shapley Values for Linear Regression\nShapley Values, like the Standardized Regression Coefficients, also measure importance, but using a different method. They have been popularized in the use of machine learning, but here we will be using them with linear regression. Shapley values measure importance through the R^2 value, which measures how well the variance in the data is explained by the coefficients generated during a linear regression analysis.\nOnce the coefficients are attained through the regression analysis, they are then used to create as many unique combinations of dependent variables in our linear regression analysis. To go into this further, if our regression had 3 explanatory variables labeled a, b, and c, each one would have a coefficient. We would then measure the R2 value for every combination of variables to explain y.\nFor example, one combination would be y = beta-aa + beat-bb + beta-cc. Another combination would be y = beta-aa + beta-bb. Another one would be y = beta-aa. We would calculate the R2 values for both of these models. We would subtract the R2 value from of the equation with variable c from the R2 value from the equaion without c to get the difference. We would do this for every combination of variable, then average the differences in R2 for every variable which we got.\nLets get into the code for calculating Shapley values in python.\nFirst, we’ll make the linear regression model.\n\nfrom sklearn.ensemble import RandomForestRegressor\n\n# Load dataset\n\nX = data[columns_of_interest]\ny = data['satisfaction']\n\n# Train a model\nmodel = LinearRegression()\nmodel.fit(X, y)\n\nLinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LinearRegressionLinearRegression()\n\n\nNext, we’ll generate the Shapley Values using the SHAP package on python\n\nimport shap\n\n# Initialize SHAP explainer\nexplainer = shap.LinearExplainer(model, X)\n\n# Calculate Shapley values\nshap_values = explainer.shap_values(X)\n\n# Plot the summary\nshap.summary_plot(shap_values, X, plot_type=\"bar\")\n\n\n\n\n\n\n\n\nFinally, we’ll put our Shapley Values into our table.\n\nshap_values = pd.DataFrame(shap_values, columns=columns_of_interest)\n\n# mean of absolute values of each column\n\nshap_values_avg = shap_values.abs().mean()\n\nshap_values_avg = pd.DataFrame(shap_values_avg)\n\nshap_values_avg = shap_values_avg.rename(columns={0: 'Shapley_Value'})\n\ntable = table.merge(shap_values_avg, left_index=True, right_index=True)\n\ntable['Shapley_Value_%'] = round(table['Shapley_Value'] / table['Shapley_Value'].sum(), 3)*100\n\ntable = table.drop('Shapley_Value', axis=1)\n\ntable\n\n\n\n\n\n\n\n\nPearson_Corr_%\nPolychoric_Corr_%\nStd_Coef_%\nShapley_Value_%\n\n\n\n\ntrust\n13.3\n12.9\n25.280109\n26.7\n\n\nimpact\n13.2\n13.8\n4.363461\n25.5\n\n\nservice\n13.0\n13.0\n6.081798\n19.9\n\n\neasy\n11.1\n10.9\n4.798205\n5.1\n\n\nappealing\n10.8\n10.6\n7.389479\n7.6\n\n\nrewarding\n10.1\n10.1\n1.106526\n1.1\n\n\nbuild\n10.0\n9.9\n3.628859\n4.5\n\n\ndiffers\n9.6\n10.0\n19.304269\n5.6\n\n\npopular\n8.9\n8.9\n28.047294\n3.8\n\n\n\n\n\n\n\nThe Shapley Values produced a slightly different order than our standard coefficients. Trust, impact, and service are the top 3 most important features. The remaiining order is much different than previous calculations, with appealing, differs, and easy the following 3, and build, popular, and rewarding following them.\n\n\n6. Johnson’s Epsilon\nJohnson’s Epsilon, also known as relative weight analysis, is an approximation of the Shapley Values which uses a different calculation approach. It uses Eigenvectors and Eigenvalues to create a set of uncorrelated independent variables which can be used to calculate the partial effect of each independent variable.\nThe code is relatively simple in python using the relativeImp function as seen below.\n\nfrom relativeImp import relativeImp\n\n\ny = 'satisfaction'\nX = ['trust', 'build', 'differs', 'easy', 'appealing', 'rewarding', 'popular', 'service', 'impact']\n\n# Perform relative weights analysis\nrel_Imp = relativeImp(data, outcomeName=y, driverNames=X)\n\nrel_Imp.set_index('driver', inplace=True)\n\ntable = table.merge(rel_Imp, left_index=True, right_index=True)\n\ntable = table.drop('rawRelaImpt', axis=1)\n\ntable = table.rename(columns = {'normRelaImpt':'Johnson_Ep_%'})\n\ntable\n\n\n\n\n\n\n\n\nPearson_Corr_%\nPolychoric_Corr_%\nStd_Coef_%\nShapley_Value_%\nJohnson_Ep_%\n\n\n\n\ntrust\n13.3\n12.9\n25.280109\n26.7\n19.835524\n\n\nimpact\n13.2\n13.8\n4.363461\n25.5\n21.966601\n\n\nservice\n13.0\n13.0\n6.081798\n19.9\n16.635164\n\n\neasy\n11.1\n10.9\n4.798205\n5.1\n8.239683\n\n\nappealing\n10.8\n10.6\n7.389479\n7.6\n8.346395\n\n\nrewarding\n10.1\n10.1\n1.106526\n1.1\n5.997431\n\n\nbuild\n10.0\n9.9\n3.628859\n4.5\n6.620792\n\n\ndiffers\n9.6\n10.0\n19.304269\n5.6\n6.966081\n\n\npopular\n8.9\n8.9\n28.047294\n3.8\n5.392328\n\n\n\n\n\n\n\nFrom our results, we can see that ranked order is similar with the top, middle, and bottom 3 variables remaining the same, but values are different.\n\n\n7. Mean Decrease in RF Gini Coefficient\nThe mean decrease in Gini coefficient also measures importance, but for Random Forests and Decision trees. It does so by measuring the impurity before and after a split in the decision tree. If the split performs well, the decrease in impurity will be higher.\n\nfrom sklearn.ensemble import RandomForestClassifier\n\nmodel = RandomForestClassifier(n_estimators=100, random_state=42)\nmodel.fit(data[columns_of_interest], data['satisfaction'])\n\ngini_importance = model.feature_importances_\n\n# Create a DataFrame for better readability\nfeature_importance_df = pd.DataFrame({\n    'Feature': columns_of_interest,\n    'Gini Importance': gini_importance\n})\n\n# Sort the features by importance\nfeature_importance_df = feature_importance_df.sort_values(by='Gini Importance', ascending=False)\n\nfeature_importance_df.set_index('Feature', inplace = True)\n\nfeature_importance_df['Gini Importance'] = feature_importance_df['Gini Importance']*100\n\ntable = table.merge(feature_importance_df, left_index=True, right_index=True)\n\ntable\n\n\n\n\n\n\n\n\nPearson_Corr_%\nPolychoric_Corr_%\nStd_Coef_%\nShapley_Value_%\nJohnson_Ep_%\nGini Importance\n\n\n\n\ntrust\n13.3\n12.9\n25.280109\n26.7\n19.835524\n8.982532\n\n\nimpact\n13.2\n13.8\n4.363461\n25.5\n21.966601\n9.270374\n\n\nservice\n13.0\n13.0\n6.081798\n19.9\n16.635164\n10.588220\n\n\neasy\n11.1\n10.9\n4.798205\n5.1\n8.239683\n11.249596\n\n\nappealing\n10.8\n10.6\n7.389479\n7.6\n8.346395\n10.699951\n\n\nrewarding\n10.1\n10.1\n1.106526\n1.1\n5.997431\n11.851860\n\n\nbuild\n10.0\n9.9\n3.628859\n4.5\n6.620792\n12.371855\n\n\ndiffers\n9.6\n10.0\n19.304269\n5.6\n6.966081\n11.466956\n\n\npopular\n8.9\n8.9\n28.047294\n3.8\n5.392328\n13.518656\n\n\n\n\n\n\n\nThe Gini Coefficient showed much different results than any of our other previous calculations. Trust and impact are now at the bottom of the pack, while popularity, build, and rewarding are at the top.\nFrom all the above calculations, we can see that there are numerous ways to measure correlation, usefulness, and importance for dependent variables in a model, dependent the type of model we are looking at. They can all be useful and worthwhile depending on the specific models and information which the customer and/or data analyst ar seeking."
  },
  {
    "objectID": "projects/Marketing_Analytics/index.html",
    "href": "projects/Marketing_Analytics/index.html",
    "title": "Marketing Projects",
    "section": "",
    "text": "A Replication of Karlan and List (2007)\n\n\n\n\n\n\nNick Shuckerow\n\n\nJul 1, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nKey Drivers Analysis\n\n\n\n\n\n\nNicholas Shuckerow\n\n\nJul 1, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nPoisson\n\n\n\n\n\n\nNicholas Shuckerow\n\n\nJul 1, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nPoisson Regression Examples\n\n\n\n\n\n\nNicholas Shuckerow\n\n\nJul 1, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nSegmentation Methods\n\n\n\n\n\n\nNicholas Shuckerow\n\n\nJul 1, 2024\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "projects/Marketing_Analytics/HW2/hw2_questions.html",
    "href": "projects/Marketing_Analytics/HW2/hw2_questions.html",
    "title": "Poisson Regression Examples",
    "section": "",
    "text": "Blueprinty is a small firm that makes software for developing blueprints specifically for submitting patent applications to the US patent office. Their marketing team would like to make the claim that patent applicants using Blueprinty’s software are more successful in getting their patent applications approved. Ideal data to study such an effect might include the success rate of patent applications before using Blueprinty’s software and after using it. unfortunately, such data is not available.\nHowever, Blueprinty has collected data on 1,500 mature (non-startup) engineering firms. The data include each firm’s number of patents awarded over the last 5 years, regional location, age since incorporation, and whether or not the firm uses Blueprinty’s software. The marketing team would like to use this data to make the claim that firms using Blueprinty’s software are more successful in getting their patent applications approved.\n\n\n\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom scipy.optimize import minimize\n\n# Load the data\n\ndata = pd.read_csv('blueprinty.csv')\n\ndata.head()\n\n\n\n\n\n\n\n\nUnnamed: 0\npatents\nregion\nage\niscustomer\n\n\n\n\n0\n1\n0\nMidwest\n32.5\n0\n\n\n1\n786\n3\nSouthwest\n37.5\n0\n\n\n2\n348\n4\nNorthwest\n27.0\n1\n\n\n3\n927\n3\nNortheast\n24.5\n0\n\n\n4\n830\n3\nSouthwest\n37.0\n0\n\n\n\n\n\n\n\nAfter reading in the data, we want to confirm that all businesses are unique and are not listed twice in the data\n\n# count the number of unique values in the column 'Unnamed: 0'\n\ndata['Unnamed: 0'].nunique()\n\n1500\n\n\nEach row is fact for an business which is not repeated in the dataset.\n\n# count number of customers and non-customers\n\ncustomers = data[data['iscustomer']==1]['iscustomer'].count()\nnon_customers = data[data['iscustomer']==0]['iscustomer'].count()\n\nprint(f' There are {customers} customers and {non_customers} non-customers in the dataset.')\n\n There are 197 customers and 1303 non-customers in the dataset.\n\n\nNow we will compare histograms based on number of patents for customers and non-customers on two separate plots.\n\n# histogram of # of patents for Customers\n\n\ndata[data['iscustomer'] == 1]['patents'].hist()\n\nplt.xlabel('Number of Patents')\nplt.ylabel('Frequency')\nplt.title('Frequency of Patents for Customers')\n\nText(0.5, 1.0, 'Frequency of Patents for Customers')\n\n\n\n\n\n\n\n\n\n\n# histogram of # of patents for non-customers\n\n\ndata[data['iscustomer'] == 0]['patents'].hist()\n\nplt.xlabel('Number of Patents')\nplt.ylabel('Frequency')\nplt.title('Frequency of Patents for Customers')\n\nText(0.5, 1.0, 'Frequency of Patents for Customers')\n\n\n\n\n\n\n\n\n\nNext, we’ll look at mean number of patents for customers and non-customers so we have a baseline for the histograms\n\nmean_patents_customers = data[data['iscustomer'] == 1]['patents'].mean()\nmean_patents_noncustomers = data[data['iscustomer'] == 0]['patents'].mean()\n\nprint('Mean patents for customers:', round(mean_patents_customers,2))\nprint('Mean patents for non-customers:', round(mean_patents_noncustomers,2))\n\nMean patents for customers: 4.09\nMean patents for non-customers: 3.62\n\n\nThe number of patents for customers is slightly skewed right, however it has a more normal distribution than the number of patents for non-customers.\nBoth plots have a large drop off around 6 patents (For customers it is slightly less than 6). The number of non-customers is significantly higher than the number of customers, totaling to 1303 non-customers and 197 customers. The mean number of patents for customers was 4.09 and the mean number of patents for non-customers was 3.62. This is a difference of about 0.5 patents. The number of customers and non-customers is important to keep into account when conducting regression models, as non-customers have a higher weight due to the higher frequency of occurence in the dataset.\nBlueprinty customers are not selected at random. It may be important to account for systematic differences in the age and regional location of customers vs non-customers.\nFirst, we’ll create a table showing the counts for each region based on customers and non-customers. We’ll looking at the proportion which each region has respectively for customers and non-customers.\n\n# group by region and count number of customers and non-customers\n\nregion = data.groupby('region')['iscustomer'].value_counts().unstack()\n\n# Calculate respective proportion of non-customers and customers which make up each region\n\nregion['Prop_Non_cust'] = region[0]/(region[0].sum())\nregion['Prop_Cust'] = region[1]/(region[1].sum())\n\n# Rename columns from 0 to Non-Customers and 1 to Customers\n\nregion.rename(columns={0:'Non-Customers', 1:'Customers'}, inplace=True)\n\nregion\n\n\n\n\n\n\n\niscustomer\nNon-Customers\nCustomers\nProp_Non_cust\nProp_Cust\n\n\nregion\n\n\n\n\n\n\n\n\nMidwest\n207\n17\n0.158864\n0.086294\n\n\nNortheast\n488\n113\n0.374520\n0.573604\n\n\nNorthwest\n171\n16\n0.131236\n0.081218\n\n\nSouth\n171\n20\n0.131236\n0.101523\n\n\nSouthwest\n266\n31\n0.204144\n0.157360\n\n\n\n\n\n\n\nBelow, you’ll see a plot of the customers and non-customers by region.\n\n# create a plot of the number of customers and non-customers by region side by side\n\ndata.groupby('region')['iscustomer'].value_counts().unstack().plot(kind='bar', stacked=False)\n\nplt.xlabel('Region')\nplt.ylabel('Count')\nplt.title('Number of Customers and Non-Customers by Region')\nplt.legend(['Non-Customer', 'Customer'])\n\n\n\n\n\n\n\n\nThe Northeast has by far the most customers and non-customers, and the number of non-customers in each region clearly outweighs the number of customers. Although ranking each respective customer and non-customer base by region comes out to be nearly the same ranking, the proportions are different.\nFor the customers, nearly 60% are from the NE, while only 40% of non-customers are from the NE.\nNext, we’ll do the same as we did for regions, except with age. One variation between the two will be binning the age groups for every 5 years. In this case, 0-5 years is one group, 5-10 years is another group, etc.\n\n# group by age with bins every 5 years and count number of customers and non-customers\n\ndata['age_bins'] = pd.cut(data['age'], bins=range(0, 60, 5))\n\nage = data.groupby('age_bins')['iscustomer'].value_counts().unstack()\nage['Prop_Non_cust'] = age[0]/(age[0].sum())\nage['Prop_Cust'] = age[1]/(age[1].sum())\n\nage.rename(columns={0:'Non-Customers', 1:'Customers'}, inplace=True)\n\nage\n\n/tmp/ipykernel_63947/1316710865.py:5: FutureWarning:\n\nThe default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n\n\n\n\n\n\n\n\n\niscustomer\nNon-Customers\nCustomers\nProp_Non_cust\nProp_Cust\n\n\nage_bins\n\n\n\n\n\n\n\n\n(0, 5]\n0\n0\n0.000000\n0.000000\n\n\n(5, 10]\n3\n3\n0.002302\n0.015228\n\n\n(10, 15]\n52\n15\n0.039908\n0.076142\n\n\n(15, 20]\n213\n53\n0.163469\n0.269036\n\n\n(20, 25]\n318\n51\n0.244052\n0.258883\n\n\n(25, 30]\n301\n32\n0.231005\n0.162437\n\n\n(30, 35]\n244\n24\n0.187260\n0.121827\n\n\n(35, 40]\n135\n13\n0.103607\n0.065990\n\n\n(40, 45]\n34\n5\n0.026094\n0.025381\n\n\n(45, 50]\n3\n1\n0.002302\n0.005076\n\n\n(50, 55]\n0\n0\n0.000000\n0.000000\n\n\n\n\n\n\n\n\n# create a plot of the number of customers and non-customers by age side by side\n\ndata.groupby('age_bins')['iscustomer'].value_counts().unstack().plot(kind='bar', stacked=False)\nplt.xlabel('Age of Company')\nplt.ylabel('Count')\nplt.title('Number of Customers and Non-Customers by Age of Company')\nplt.legend(['Non-Customer', 'Customer'])\n\n/tmp/ipykernel_63947/196641801.py:3: FutureWarning:\n\nThe default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n\n\n\n\n\n\n\n\n\n\nFor age, we’ll also look at the mean age of customers and non-customers, and calculate the 95% confidence intervals.\n\n# find mean age for customers and non-customers\n\nmean_age_customers = data[data['iscustomer'] == 1]['age'].mean()\nmean_age_noncustomers = data[data['iscustomer'] == 0]['age'].mean()\n\nprint('Mean age for customers:', round(mean_age_customers,2))\nprint('Mean age for non-customers:', round(mean_age_noncustomers,2))\n\nMean age for customers: 24.15\nMean age for non-customers: 26.69\n\n\n\n# Calculate a 95% confidence interval for the mean age of customers and non-customers\n\nimport numpy as np\n\nstd_age_customers = data[data['iscustomer'] == 1]['age'].std()\nstd_age_noncustomers = data[data['iscustomer'] == 0]['age'].std()\n\nn_customers = data[data['iscustomer'] == 1]['age'].count()\nn_noncustomers = data[data['iscustomer'] == 0]['age'].count()\n\nz = 1.96\n\nci_customers = z * (std_age_customers/np.sqrt(n_customers))\nci_noncustomers = z * (std_age_noncustomers/np.sqrt(n_noncustomers))\n\nprint('95% CI for mean age of customers:', round(mean_age_customers-ci_customers,2), round(mean_age_customers+ci_customers,2))\nprint('95% CI for mean age of non-customers:', round(mean_age_noncustomers-ci_noncustomers,2), round(mean_age_noncustomers+ci_noncustomers,2))\n\n95% CI for mean age of customers: 23.09 25.21\n95% CI for mean age of non-customers: 26.3 27.08\n\n\nThe most customers come from companies which are between 15-20 years old, with the average company age being about 24 years. The most non-customers come from companies which are between 20-25 years old, with the average company age being about 27 years.\nThe distribution of company ages for customers and non-customers resembles a normal distribution, with a slight skew to the right. The largest disparity between the two distributions is at the 10-15 year mark, where the % of non-customers is 10% lower than that of customers (16%, 26%).\nThe confidence interval for the mean age of customers is (23.1, 25.2) and the confidence interval for the mean age of non-customers is (26.3, 27.1).\n\n\n\nSince our outcome variable of interest can only be small integer values per a set unit of time, we can use a Poisson density to model the number of patents awarded to each engineering firm over the last 5 years. We start by estimating a simple Poisson model via Maximum Likelihood.\nBelow is the log-likelihood function for the Poisson distribution. The likelihood function of lambda given Y is the exact same as the function of Y given lambda.\nℓ(λ∣Y)=−λ+Ylog(λ)−log(Y!)\nBelow is the code for the log-likelihood function for a poisson distribution:\n\ndef poisson_log_likelihood(lam, y):\n    \"\"\"\n    Parameters:\n    - lam (float): The rate parameter (lambda) of the Poisson distribution.\n    - y (array-like): Array of observed counts.\n\n    Returns:\n    - float: The log likelihood of observing the data given lam.\n    \"\"\"\n    y = np.array(y)\n    n = len(y)  # number of observations\n    sum_y = np.sum(y)  # sum of all observed counts\n\n    # Calculate each part of the log likelihood\n    # log(P(Y|lam)) = -n * lam + sum_y * log(lam) - log(y_i!)\n    # We use np.sum(np.log(y_factorials)) to sum log of factorials\n    log_likelihood = -n * lam + sum_y * np.log(lam) - np.sum([np.log(np.math.factorial(i)) for i in y])\n    return log_likelihood\n\nNext, we’ll plot the log-likelihoods using our observed number of lambdas as Y and then a range of values for lambda (1-10).\n\nyears = range(1,11)\nlog_likelihood_values = []\n\nfor i in years:\n    log_likelihood_value = poisson_log_likelihood(i, data['patents'])\n    log_likelihood_values.append(log_likelihood_value)\n\nlog_likelihood_values\n\n/tmp/ipykernel_63947/1539929848.py:17: DeprecationWarning:\n\n`np.math` is a deprecated alias for the standard library `math` module (Deprecated Numpy 1.25). Replace usages of `np.math` with `math`\n\n\n\n[-6548.886990069443,\n -4217.862523114625,\n -3476.856870600801,\n -3386.838056159808,\n -3653.52364804617,\n -4145.8324036459835,\n -4793.841596240727,\n -5555.81358920499,\n -6404.826751132159,\n -7322.4991810913525]\n\n\n\nplt.plot(years, log_likelihood_values)\nplt.xlabel('Lambda')\nplt.ylabel('Likelihood')\nplt.title('Likelihood of observing the data given Lambda')\nplt.show()\n\n\n\n\n\n\n\n\nWe’ll now create our negative poisson MLE function and analyze the output betas.\n\nfrom scipy.optimize import minimize\n\ndef neg_poisson_log_likelihood(lam, y):\n    \"\"\"\n    Parameters:\n    - lam (float): The rate parameter (lambda) of the Poisson distribution.\n    - y (array-like): Array of observed counts.\n\n    Returns:\n    - float: The log likelihood of observing the data given lam.\n    \"\"\"\n    y = np.array(y)\n    n = len(y)  # number of observations\n    sum_y = np.sum(y)  # sum of all observed counts\n\n    # Calculate each part of the log likelihood\n    # log(P(Y|lam)) = -n * lam + sum_y * log(lam) - log(y_i!)\n    # We use np.sum(np.log(y_factorials)) to sum log of factorials\n    return -(-n * lam + sum_y * np.log(lam) - np.sum([np.log(np.math.factorial(i)) for i in y]))\n\n\nmean = np.mean(data['patents'])\n\nresult = minimize(neg_poisson_log_likelihood, mean, args=(data['patents']), bounds = [(0, None)])\n\nresult\n\n/tmp/ipykernel_63947/2966890401.py:19: DeprecationWarning:\n\n`np.math` is a deprecated alias for the standard library `math` module (Deprecated Numpy 1.25). Replace usages of `np.math` with `math`\n\n\n\n  message: CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_&lt;=_PGTOL\n  success: True\n   status: 0\n      fun: 3367.683772235094\n        x: [ 3.685e+00]\n      nit: 0\n      jac: [ 0.000e+00]\n     nfev: 2\n     njev: 1\n hess_inv: &lt;1x1 LbfgsInvHessProduct with dtype=float64&gt;\n\n\n\n\n\nNext, we extend our simple Poisson model to a Poisson Regression Model such that \\(Y_i = \\text{Poisson}(\\lambda_i)\\) where \\(\\lambda_i = \\exp(X_i'\\beta)\\). The interpretation is that the success rate of patent awards is not constant across all firms (\\(\\lambda\\)) but rather is a function of firm characteristics \\(X_i\\). Specifically, we will use the covariates age, age squared, region, and whether the firm is a customer of Blueprinty.\nFirst, we’ll create our new poisson MLE function, incorporating our explanatory variables into the calculation.\n\nimport numpy as np\nfrom scipy.optimize import minimize\n\ndef poisson_regression_log_likelihood(beta, Y, X):\n    eta = np.dot(X, beta)\n    lambda_i = np.exp(eta)\n    log_likelihood = np.sum(Y * eta - lambda_i)\n    return -log_likelihood\n\nNext, we’ll change the format of the data to be able to be used in our functions and analysis.\n\n# convert regions column to boolean columns, dropping the first region to be the default value\n\ndata = pd.get_dummies(data, columns=['region'], drop_first=True)\n\n# creating function to convert boolean column to binary\n\ndef convert_boolean_to_binary(data, column):\n    data[column] = data[column].astype(int)\n    return data\n\n# coverting region's boolean values to binary\n\ndata = convert_boolean_to_binary(data, 'region_Northeast')\ndata = convert_boolean_to_binary(data, 'region_South')\ndata = convert_boolean_to_binary(data, 'region_Southwest')\ndata = convert_boolean_to_binary(data, 'region_Northwest')\n\n# creating an age^2 column in the dataset\n\ndata['age_squared'] = data['age']**2\n\n\n# Load and preprocess your data as before, ensuring that features are scaled\nfrom sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\n# Scaling age and age_squared to prevent precision loss from extremely large numbers\ndata['age'] = scaler.fit_transform(data[['age']])\ndata['age_squared'] = scaler.fit_transform(data[['age_squared']])\n\n\n# Defining X and Y variables\nX = np.c_[np.ones(len(data)), data['age'], data['age_squared'], data['iscustomer'], data['region_Southwest'], data['region_Northwest'],\n          data['region_Northeast'], data['region_South']]\nY = data['patents'].values\n\n# Initial guess for beta (0)\ninitial_beta = np.zeros(X.shape[1])\n\n# Minimization\nresult = minimize(poisson_regression_log_likelihood, initial_beta, args=(Y, X), method='BFGS')\n\nprint(\"Optimal beta:\", result.x)\n\nOptimal beta: [ 1.21543809  1.04645982 -1.14084528  0.11811438  0.05134699 -0.02009421\n  0.098596    0.05717198]\n\n\nNow that we calculated our betas from our self-built function, we’ll verify our betas with a built-in regression function.\n\nimport statsmodels.api as sm\n\n# Fit a Poisson regression model using statsmodels\n\npoisson_model = sm.GLM(Y, X, family=sm.families.Poisson()).fit()\n\n# Print the summary of the model\n\nprint(poisson_model.summary())\n\n                 Generalized Linear Model Regression Results                  \n==============================================================================\nDep. Variable:                      y   No. Observations:                 1500\nModel:                            GLM   Df Residuals:                     1492\nModel Family:                 Poisson   Df Model:                            7\nLink Function:                    Log   Scale:                          1.0000\nMethod:                          IRLS   Log-Likelihood:                -3275.9\nDate:                Wed, 12 Jun 2024   Deviance:                       2178.8\nTime:                        12:17:52   Pearson chi2:                 2.11e+03\nNo. Iterations:                     5   Pseudo R-squ. (CS):             0.1152\nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          z      P&gt;|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst          1.2154      0.036     33.368      0.000       1.144       1.287\nx1             1.0465      0.100     10.414      0.000       0.850       1.243\nx2            -1.1408      0.102    -11.131      0.000      -1.342      -0.940\nx3             0.1181      0.039      3.035      0.002       0.042       0.194\nx4             0.0513      0.047      1.088      0.277      -0.041       0.144\nx5            -0.0201      0.054     -0.374      0.709      -0.126       0.085\nx6             0.0986      0.042      2.347      0.019       0.016       0.181\nx7             0.0572      0.053      1.085      0.278      -0.046       0.160\n==============================================================================\n\n\nFinally, we’ll calculate our standard errors for each respective beta or coefficient and compare to our built-in regression analysis results.\n\n# Calculate Hessian at the optimal beta\nfrom scipy.linalg import inv\nhessian_inv = result.hess_inv  # Inverse Hessian is returned by BFGS\n\n# Calculating standard errors by taking the square roots of the diagonal elements of the inverse Hessian\nstd_errors = np.sqrt(np.diag(hessian_inv))\n\nprint(\"Standard Errors:\", std_errors)\n\nStandard Errors: [0.02272912 0.14333007 0.14451598 0.03888018 0.03676005 0.04672971\n 0.02526631 0.02849847]\n\n\nWe can conclude, based on our optimal beta’s through our regression model, that Blueprinty’s software has a positive effect on the number of patents awarded to a company. The coefficient or beta calculated for “iscustomer” is 0.11, meaning if they are a customer of the software, the humber of patents earned increases by 0.11.\nWe also see that p-value of “iscustomer” is 0.002, meaning there is only a 0.2% chance that the coefficient has zero affect on the number of patents given the dataset.\nOur standard errors we’re slightly off for some of the variables, which may mean our self-built function has an issue, or we are using a different optimization method (ie. BFGS, L-BFGS-B) than the built-in function."
  },
  {
    "objectID": "projects/Marketing_Analytics/HW2/hw2_questions.html#blueprinty-case-study",
    "href": "projects/Marketing_Analytics/HW2/hw2_questions.html#blueprinty-case-study",
    "title": "Poisson Regression Examples",
    "section": "",
    "text": "Blueprinty is a small firm that makes software for developing blueprints specifically for submitting patent applications to the US patent office. Their marketing team would like to make the claim that patent applicants using Blueprinty’s software are more successful in getting their patent applications approved. Ideal data to study such an effect might include the success rate of patent applications before using Blueprinty’s software and after using it. unfortunately, such data is not available.\nHowever, Blueprinty has collected data on 1,500 mature (non-startup) engineering firms. The data include each firm’s number of patents awarded over the last 5 years, regional location, age since incorporation, and whether or not the firm uses Blueprinty’s software. The marketing team would like to use this data to make the claim that firms using Blueprinty’s software are more successful in getting their patent applications approved.\n\n\n\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom scipy.optimize import minimize\n\n# Load the data\n\ndata = pd.read_csv('blueprinty.csv')\n\ndata.head()\n\n\n\n\n\n\n\n\nUnnamed: 0\npatents\nregion\nage\niscustomer\n\n\n\n\n0\n1\n0\nMidwest\n32.5\n0\n\n\n1\n786\n3\nSouthwest\n37.5\n0\n\n\n2\n348\n4\nNorthwest\n27.0\n1\n\n\n3\n927\n3\nNortheast\n24.5\n0\n\n\n4\n830\n3\nSouthwest\n37.0\n0\n\n\n\n\n\n\n\nAfter reading in the data, we want to confirm that all businesses are unique and are not listed twice in the data\n\n# count the number of unique values in the column 'Unnamed: 0'\n\ndata['Unnamed: 0'].nunique()\n\n1500\n\n\nEach row is fact for an business which is not repeated in the dataset.\n\n# count number of customers and non-customers\n\ncustomers = data[data['iscustomer']==1]['iscustomer'].count()\nnon_customers = data[data['iscustomer']==0]['iscustomer'].count()\n\nprint(f' There are {customers} customers and {non_customers} non-customers in the dataset.')\n\n There are 197 customers and 1303 non-customers in the dataset.\n\n\nNow we will compare histograms based on number of patents for customers and non-customers on two separate plots.\n\n# histogram of # of patents for Customers\n\n\ndata[data['iscustomer'] == 1]['patents'].hist()\n\nplt.xlabel('Number of Patents')\nplt.ylabel('Frequency')\nplt.title('Frequency of Patents for Customers')\n\nText(0.5, 1.0, 'Frequency of Patents for Customers')\n\n\n\n\n\n\n\n\n\n\n# histogram of # of patents for non-customers\n\n\ndata[data['iscustomer'] == 0]['patents'].hist()\n\nplt.xlabel('Number of Patents')\nplt.ylabel('Frequency')\nplt.title('Frequency of Patents for Customers')\n\nText(0.5, 1.0, 'Frequency of Patents for Customers')\n\n\n\n\n\n\n\n\n\nNext, we’ll look at mean number of patents for customers and non-customers so we have a baseline for the histograms\n\nmean_patents_customers = data[data['iscustomer'] == 1]['patents'].mean()\nmean_patents_noncustomers = data[data['iscustomer'] == 0]['patents'].mean()\n\nprint('Mean patents for customers:', round(mean_patents_customers,2))\nprint('Mean patents for non-customers:', round(mean_patents_noncustomers,2))\n\nMean patents for customers: 4.09\nMean patents for non-customers: 3.62\n\n\nThe number of patents for customers is slightly skewed right, however it has a more normal distribution than the number of patents for non-customers.\nBoth plots have a large drop off around 6 patents (For customers it is slightly less than 6). The number of non-customers is significantly higher than the number of customers, totaling to 1303 non-customers and 197 customers. The mean number of patents for customers was 4.09 and the mean number of patents for non-customers was 3.62. This is a difference of about 0.5 patents. The number of customers and non-customers is important to keep into account when conducting regression models, as non-customers have a higher weight due to the higher frequency of occurence in the dataset.\nBlueprinty customers are not selected at random. It may be important to account for systematic differences in the age and regional location of customers vs non-customers.\nFirst, we’ll create a table showing the counts for each region based on customers and non-customers. We’ll looking at the proportion which each region has respectively for customers and non-customers.\n\n# group by region and count number of customers and non-customers\n\nregion = data.groupby('region')['iscustomer'].value_counts().unstack()\n\n# Calculate respective proportion of non-customers and customers which make up each region\n\nregion['Prop_Non_cust'] = region[0]/(region[0].sum())\nregion['Prop_Cust'] = region[1]/(region[1].sum())\n\n# Rename columns from 0 to Non-Customers and 1 to Customers\n\nregion.rename(columns={0:'Non-Customers', 1:'Customers'}, inplace=True)\n\nregion\n\n\n\n\n\n\n\niscustomer\nNon-Customers\nCustomers\nProp_Non_cust\nProp_Cust\n\n\nregion\n\n\n\n\n\n\n\n\nMidwest\n207\n17\n0.158864\n0.086294\n\n\nNortheast\n488\n113\n0.374520\n0.573604\n\n\nNorthwest\n171\n16\n0.131236\n0.081218\n\n\nSouth\n171\n20\n0.131236\n0.101523\n\n\nSouthwest\n266\n31\n0.204144\n0.157360\n\n\n\n\n\n\n\nBelow, you’ll see a plot of the customers and non-customers by region.\n\n# create a plot of the number of customers and non-customers by region side by side\n\ndata.groupby('region')['iscustomer'].value_counts().unstack().plot(kind='bar', stacked=False)\n\nplt.xlabel('Region')\nplt.ylabel('Count')\nplt.title('Number of Customers and Non-Customers by Region')\nplt.legend(['Non-Customer', 'Customer'])\n\n\n\n\n\n\n\n\nThe Northeast has by far the most customers and non-customers, and the number of non-customers in each region clearly outweighs the number of customers. Although ranking each respective customer and non-customer base by region comes out to be nearly the same ranking, the proportions are different.\nFor the customers, nearly 60% are from the NE, while only 40% of non-customers are from the NE.\nNext, we’ll do the same as we did for regions, except with age. One variation between the two will be binning the age groups for every 5 years. In this case, 0-5 years is one group, 5-10 years is another group, etc.\n\n# group by age with bins every 5 years and count number of customers and non-customers\n\ndata['age_bins'] = pd.cut(data['age'], bins=range(0, 60, 5))\n\nage = data.groupby('age_bins')['iscustomer'].value_counts().unstack()\nage['Prop_Non_cust'] = age[0]/(age[0].sum())\nage['Prop_Cust'] = age[1]/(age[1].sum())\n\nage.rename(columns={0:'Non-Customers', 1:'Customers'}, inplace=True)\n\nage\n\n/tmp/ipykernel_63947/1316710865.py:5: FutureWarning:\n\nThe default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n\n\n\n\n\n\n\n\n\niscustomer\nNon-Customers\nCustomers\nProp_Non_cust\nProp_Cust\n\n\nage_bins\n\n\n\n\n\n\n\n\n(0, 5]\n0\n0\n0.000000\n0.000000\n\n\n(5, 10]\n3\n3\n0.002302\n0.015228\n\n\n(10, 15]\n52\n15\n0.039908\n0.076142\n\n\n(15, 20]\n213\n53\n0.163469\n0.269036\n\n\n(20, 25]\n318\n51\n0.244052\n0.258883\n\n\n(25, 30]\n301\n32\n0.231005\n0.162437\n\n\n(30, 35]\n244\n24\n0.187260\n0.121827\n\n\n(35, 40]\n135\n13\n0.103607\n0.065990\n\n\n(40, 45]\n34\n5\n0.026094\n0.025381\n\n\n(45, 50]\n3\n1\n0.002302\n0.005076\n\n\n(50, 55]\n0\n0\n0.000000\n0.000000\n\n\n\n\n\n\n\n\n# create a plot of the number of customers and non-customers by age side by side\n\ndata.groupby('age_bins')['iscustomer'].value_counts().unstack().plot(kind='bar', stacked=False)\nplt.xlabel('Age of Company')\nplt.ylabel('Count')\nplt.title('Number of Customers and Non-Customers by Age of Company')\nplt.legend(['Non-Customer', 'Customer'])\n\n/tmp/ipykernel_63947/196641801.py:3: FutureWarning:\n\nThe default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n\n\n\n\n\n\n\n\n\n\nFor age, we’ll also look at the mean age of customers and non-customers, and calculate the 95% confidence intervals.\n\n# find mean age for customers and non-customers\n\nmean_age_customers = data[data['iscustomer'] == 1]['age'].mean()\nmean_age_noncustomers = data[data['iscustomer'] == 0]['age'].mean()\n\nprint('Mean age for customers:', round(mean_age_customers,2))\nprint('Mean age for non-customers:', round(mean_age_noncustomers,2))\n\nMean age for customers: 24.15\nMean age for non-customers: 26.69\n\n\n\n# Calculate a 95% confidence interval for the mean age of customers and non-customers\n\nimport numpy as np\n\nstd_age_customers = data[data['iscustomer'] == 1]['age'].std()\nstd_age_noncustomers = data[data['iscustomer'] == 0]['age'].std()\n\nn_customers = data[data['iscustomer'] == 1]['age'].count()\nn_noncustomers = data[data['iscustomer'] == 0]['age'].count()\n\nz = 1.96\n\nci_customers = z * (std_age_customers/np.sqrt(n_customers))\nci_noncustomers = z * (std_age_noncustomers/np.sqrt(n_noncustomers))\n\nprint('95% CI for mean age of customers:', round(mean_age_customers-ci_customers,2), round(mean_age_customers+ci_customers,2))\nprint('95% CI for mean age of non-customers:', round(mean_age_noncustomers-ci_noncustomers,2), round(mean_age_noncustomers+ci_noncustomers,2))\n\n95% CI for mean age of customers: 23.09 25.21\n95% CI for mean age of non-customers: 26.3 27.08\n\n\nThe most customers come from companies which are between 15-20 years old, with the average company age being about 24 years. The most non-customers come from companies which are between 20-25 years old, with the average company age being about 27 years.\nThe distribution of company ages for customers and non-customers resembles a normal distribution, with a slight skew to the right. The largest disparity between the two distributions is at the 10-15 year mark, where the % of non-customers is 10% lower than that of customers (16%, 26%).\nThe confidence interval for the mean age of customers is (23.1, 25.2) and the confidence interval for the mean age of non-customers is (26.3, 27.1).\n\n\n\nSince our outcome variable of interest can only be small integer values per a set unit of time, we can use a Poisson density to model the number of patents awarded to each engineering firm over the last 5 years. We start by estimating a simple Poisson model via Maximum Likelihood.\nBelow is the log-likelihood function for the Poisson distribution. The likelihood function of lambda given Y is the exact same as the function of Y given lambda.\nℓ(λ∣Y)=−λ+Ylog(λ)−log(Y!)\nBelow is the code for the log-likelihood function for a poisson distribution:\n\ndef poisson_log_likelihood(lam, y):\n    \"\"\"\n    Parameters:\n    - lam (float): The rate parameter (lambda) of the Poisson distribution.\n    - y (array-like): Array of observed counts.\n\n    Returns:\n    - float: The log likelihood of observing the data given lam.\n    \"\"\"\n    y = np.array(y)\n    n = len(y)  # number of observations\n    sum_y = np.sum(y)  # sum of all observed counts\n\n    # Calculate each part of the log likelihood\n    # log(P(Y|lam)) = -n * lam + sum_y * log(lam) - log(y_i!)\n    # We use np.sum(np.log(y_factorials)) to sum log of factorials\n    log_likelihood = -n * lam + sum_y * np.log(lam) - np.sum([np.log(np.math.factorial(i)) for i in y])\n    return log_likelihood\n\nNext, we’ll plot the log-likelihoods using our observed number of lambdas as Y and then a range of values for lambda (1-10).\n\nyears = range(1,11)\nlog_likelihood_values = []\n\nfor i in years:\n    log_likelihood_value = poisson_log_likelihood(i, data['patents'])\n    log_likelihood_values.append(log_likelihood_value)\n\nlog_likelihood_values\n\n/tmp/ipykernel_63947/1539929848.py:17: DeprecationWarning:\n\n`np.math` is a deprecated alias for the standard library `math` module (Deprecated Numpy 1.25). Replace usages of `np.math` with `math`\n\n\n\n[-6548.886990069443,\n -4217.862523114625,\n -3476.856870600801,\n -3386.838056159808,\n -3653.52364804617,\n -4145.8324036459835,\n -4793.841596240727,\n -5555.81358920499,\n -6404.826751132159,\n -7322.4991810913525]\n\n\n\nplt.plot(years, log_likelihood_values)\nplt.xlabel('Lambda')\nplt.ylabel('Likelihood')\nplt.title('Likelihood of observing the data given Lambda')\nplt.show()\n\n\n\n\n\n\n\n\nWe’ll now create our negative poisson MLE function and analyze the output betas.\n\nfrom scipy.optimize import minimize\n\ndef neg_poisson_log_likelihood(lam, y):\n    \"\"\"\n    Parameters:\n    - lam (float): The rate parameter (lambda) of the Poisson distribution.\n    - y (array-like): Array of observed counts.\n\n    Returns:\n    - float: The log likelihood of observing the data given lam.\n    \"\"\"\n    y = np.array(y)\n    n = len(y)  # number of observations\n    sum_y = np.sum(y)  # sum of all observed counts\n\n    # Calculate each part of the log likelihood\n    # log(P(Y|lam)) = -n * lam + sum_y * log(lam) - log(y_i!)\n    # We use np.sum(np.log(y_factorials)) to sum log of factorials\n    return -(-n * lam + sum_y * np.log(lam) - np.sum([np.log(np.math.factorial(i)) for i in y]))\n\n\nmean = np.mean(data['patents'])\n\nresult = minimize(neg_poisson_log_likelihood, mean, args=(data['patents']), bounds = [(0, None)])\n\nresult\n\n/tmp/ipykernel_63947/2966890401.py:19: DeprecationWarning:\n\n`np.math` is a deprecated alias for the standard library `math` module (Deprecated Numpy 1.25). Replace usages of `np.math` with `math`\n\n\n\n  message: CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_&lt;=_PGTOL\n  success: True\n   status: 0\n      fun: 3367.683772235094\n        x: [ 3.685e+00]\n      nit: 0\n      jac: [ 0.000e+00]\n     nfev: 2\n     njev: 1\n hess_inv: &lt;1x1 LbfgsInvHessProduct with dtype=float64&gt;\n\n\n\n\n\nNext, we extend our simple Poisson model to a Poisson Regression Model such that \\(Y_i = \\text{Poisson}(\\lambda_i)\\) where \\(\\lambda_i = \\exp(X_i'\\beta)\\). The interpretation is that the success rate of patent awards is not constant across all firms (\\(\\lambda\\)) but rather is a function of firm characteristics \\(X_i\\). Specifically, we will use the covariates age, age squared, region, and whether the firm is a customer of Blueprinty.\nFirst, we’ll create our new poisson MLE function, incorporating our explanatory variables into the calculation.\n\nimport numpy as np\nfrom scipy.optimize import minimize\n\ndef poisson_regression_log_likelihood(beta, Y, X):\n    eta = np.dot(X, beta)\n    lambda_i = np.exp(eta)\n    log_likelihood = np.sum(Y * eta - lambda_i)\n    return -log_likelihood\n\nNext, we’ll change the format of the data to be able to be used in our functions and analysis.\n\n# convert regions column to boolean columns, dropping the first region to be the default value\n\ndata = pd.get_dummies(data, columns=['region'], drop_first=True)\n\n# creating function to convert boolean column to binary\n\ndef convert_boolean_to_binary(data, column):\n    data[column] = data[column].astype(int)\n    return data\n\n# coverting region's boolean values to binary\n\ndata = convert_boolean_to_binary(data, 'region_Northeast')\ndata = convert_boolean_to_binary(data, 'region_South')\ndata = convert_boolean_to_binary(data, 'region_Southwest')\ndata = convert_boolean_to_binary(data, 'region_Northwest')\n\n# creating an age^2 column in the dataset\n\ndata['age_squared'] = data['age']**2\n\n\n# Load and preprocess your data as before, ensuring that features are scaled\nfrom sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\n# Scaling age and age_squared to prevent precision loss from extremely large numbers\ndata['age'] = scaler.fit_transform(data[['age']])\ndata['age_squared'] = scaler.fit_transform(data[['age_squared']])\n\n\n# Defining X and Y variables\nX = np.c_[np.ones(len(data)), data['age'], data['age_squared'], data['iscustomer'], data['region_Southwest'], data['region_Northwest'],\n          data['region_Northeast'], data['region_South']]\nY = data['patents'].values\n\n# Initial guess for beta (0)\ninitial_beta = np.zeros(X.shape[1])\n\n# Minimization\nresult = minimize(poisson_regression_log_likelihood, initial_beta, args=(Y, X), method='BFGS')\n\nprint(\"Optimal beta:\", result.x)\n\nOptimal beta: [ 1.21543809  1.04645982 -1.14084528  0.11811438  0.05134699 -0.02009421\n  0.098596    0.05717198]\n\n\nNow that we calculated our betas from our self-built function, we’ll verify our betas with a built-in regression function.\n\nimport statsmodels.api as sm\n\n# Fit a Poisson regression model using statsmodels\n\npoisson_model = sm.GLM(Y, X, family=sm.families.Poisson()).fit()\n\n# Print the summary of the model\n\nprint(poisson_model.summary())\n\n                 Generalized Linear Model Regression Results                  \n==============================================================================\nDep. Variable:                      y   No. Observations:                 1500\nModel:                            GLM   Df Residuals:                     1492\nModel Family:                 Poisson   Df Model:                            7\nLink Function:                    Log   Scale:                          1.0000\nMethod:                          IRLS   Log-Likelihood:                -3275.9\nDate:                Wed, 12 Jun 2024   Deviance:                       2178.8\nTime:                        12:17:52   Pearson chi2:                 2.11e+03\nNo. Iterations:                     5   Pseudo R-squ. (CS):             0.1152\nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          z      P&gt;|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst          1.2154      0.036     33.368      0.000       1.144       1.287\nx1             1.0465      0.100     10.414      0.000       0.850       1.243\nx2            -1.1408      0.102    -11.131      0.000      -1.342      -0.940\nx3             0.1181      0.039      3.035      0.002       0.042       0.194\nx4             0.0513      0.047      1.088      0.277      -0.041       0.144\nx5            -0.0201      0.054     -0.374      0.709      -0.126       0.085\nx6             0.0986      0.042      2.347      0.019       0.016       0.181\nx7             0.0572      0.053      1.085      0.278      -0.046       0.160\n==============================================================================\n\n\nFinally, we’ll calculate our standard errors for each respective beta or coefficient and compare to our built-in regression analysis results.\n\n# Calculate Hessian at the optimal beta\nfrom scipy.linalg import inv\nhessian_inv = result.hess_inv  # Inverse Hessian is returned by BFGS\n\n# Calculating standard errors by taking the square roots of the diagonal elements of the inverse Hessian\nstd_errors = np.sqrt(np.diag(hessian_inv))\n\nprint(\"Standard Errors:\", std_errors)\n\nStandard Errors: [0.02272912 0.14333007 0.14451598 0.03888018 0.03676005 0.04672971\n 0.02526631 0.02849847]\n\n\nWe can conclude, based on our optimal beta’s through our regression model, that Blueprinty’s software has a positive effect on the number of patents awarded to a company. The coefficient or beta calculated for “iscustomer” is 0.11, meaning if they are a customer of the software, the humber of patents earned increases by 0.11.\nWe also see that p-value of “iscustomer” is 0.002, meaning there is only a 0.2% chance that the coefficient has zero affect on the number of patents given the dataset.\nOur standard errors we’re slightly off for some of the variables, which may mean our self-built function has an issue, or we are using a different optimization method (ie. BFGS, L-BFGS-B) than the built-in function."
  },
  {
    "objectID": "projects/Marketing_Analytics/HW2/hw2_questions.html#airbnb-case-study",
    "href": "projects/Marketing_Analytics/HW2/hw2_questions.html#airbnb-case-study",
    "title": "Poisson Regression Examples",
    "section": "AirBnB Case Study",
    "text": "AirBnB Case Study\n\nIntroduction\nAirBnB is a popular platform for booking short-term rentals. In March 2017, students Annika Awad, Evan Lebo, and Anna Linden scraped of 40,000 Airbnb listings from New York City. The data include the following variables:\n\n\n\n\n\n\nVariable Definitions\n\n\n\n\n\n- `id` = unique ID number for each unit\n- `last_scraped` = date when information scraped\n- `host_since` = date when host first listed the unit on Airbnb\n- `days` = `last_scraped` - `host_since` = number of days the unit has been listed\n- `room_type` = Entire home/apt., Private room, or Shared room\n- `bathrooms` = number of bathrooms\n- `bedrooms` = number of bedrooms\n- `price` = price per night (dollars)\n- `number_of_reviews` = number of reviews for the unit on Airbnb\n- `review_scores_cleanliness` = a cleanliness score from reviews (1-10)\n- `review_scores_location` = a \"quality of location\" score from reviews (1-10)\n- `review_scores_value` = a \"quality of value\" score from reviews (1-10)\n- `instant_bookable` = \"t\" if instantly bookable, \"f\" if not\n\n\n\nUsing the airbnb data, we will build a likelihood function for this poisson regression. However, we first need to read the data and look at its characteristics.\n\n# read airbnb csv\n\nairbnb = pd.read_csv('airbnb.csv')\n\nairbnb.head()\n\n\n\n\n\n\n\n\nUnnamed: 0\nid\ndays\nlast_scraped\nhost_since\nroom_type\nbathrooms\nbedrooms\nprice\nnumber_of_reviews\nreview_scores_cleanliness\nreview_scores_location\nreview_scores_value\ninstant_bookable\n\n\n\n\n0\n1\n2515\n3130\n4/2/2017\n9/6/2008\nPrivate room\n1.0\n1.0\n59\n150\n9.0\n9.0\n9.0\nf\n\n\n1\n2\n2595\n3127\n4/2/2017\n9/9/2008\nEntire home/apt\n1.0\n0.0\n230\n20\n9.0\n10.0\n9.0\nf\n\n\n2\n3\n3647\n3050\n4/2/2017\n11/25/2008\nPrivate room\n1.0\n1.0\n150\n0\nNaN\nNaN\nNaN\nf\n\n\n3\n4\n3831\n3038\n4/2/2017\n12/7/2008\nEntire home/apt\n1.0\n1.0\n89\n116\n9.0\n9.0\n9.0\nf\n\n\n4\n5\n4611\n3012\n4/2/2017\n1/2/2009\nPrivate room\nNaN\n1.0\n39\n93\n9.0\n8.0\n9.0\nt\n\n\n\n\n\n\n\n\n# count number of listings which have 0 reviews\nairbnb[airbnb['number_of_reviews'] == 0]['number_of_reviews'].count()\n\n9481\n\n\n\n# create subset of data which only shows bedrooms which are nan\n\nairbnb.isnull().sum()\n\nUnnamed: 0                       0\nid                               0\ndays                             0\nlast_scraped                     0\nhost_since                      35\nroom_type                        0\nbathrooms                      160\nbedrooms                        76\nprice                            0\nnumber_of_reviews                0\nreview_scores_cleanliness    10195\nreview_scores_location       10254\nreview_scores_value          10256\ninstant_bookable                 0\ndtype: int64\n\n\nLooking at the data initally, we see there are many cells having null values which we need to address. The majority of them are within the review score columns. To address this, we will drop the cells which have 0 days listed (brand new listings). For bedrooms and bathrooms in a shared or private room, we will assume bedrooms is 1 and bathrooms are 0. It is very common for rooms to not have a bathroom. For all other data which is null, we cannot say with reasonable certainty what the value would be. For example, if bedrooms or bathrooms are null for entire home or apartment, we cannot say with reasonable certainty the number of bedrooms or bathrooms.\n\n# fill bedrooms with 1 and bathrooms with 0 if nan and if room type is private or shared room\n\nairbnb.loc[(airbnb['bedrooms'].isnull()) & (airbnb['room_type'] == 'Private room'), 'bedrooms'] = 1\nairbnb.loc[(airbnb['bedrooms'].isnull()) & (airbnb['room_type'] == 'Shared room'), 'bedrooms'] = 1\nairbnb.loc[(airbnb['bathrooms'].isnull()) & (airbnb['room_type'] == 'Private room'), 'bathrooms'] = 0\nairbnb.loc[(airbnb['bathrooms'].isnull()) & (airbnb['room_type'] == 'Private room'), 'bathrooms'] = 0\n\n\n# Dropping all other rows which have null values\nairbnb = airbnb.dropna()\n\n\nairbnb.shape\n\n(30234, 14)\n\n\n\nairbnb.describe()\n\n\n\n\n\n\n\n\nUnnamed: 0\nid\ndays\nbathrooms\nbedrooms\nprice\nnumber_of_reviews\nreview_scores_cleanliness\nreview_scores_location\nreview_scores_value\n\n\n\n\ncount\n30234.000000\n3.023400e+04\n30234.000000\n30234.000000\n30234.000000\n30234.000000\n30234.000000\n30234.000000\n30234.000000\n30234.000000\n\n\nmean\n18633.661937\n8.955396e+06\n1114.714560\n1.118856\n1.150989\n140.062777\n21.245981\n9.200370\n9.414070\n9.332837\n\n\nstd\n11340.011572\n5.388450e+06\n646.007986\n0.389266\n0.698002\n188.205058\n32.137276\n1.114729\n0.843788\n0.900678\n\n\nmin\n1.000000\n2.515000e+03\n7.000000\n0.000000\n0.000000\n10.000000\n1.000000\n2.000000\n2.000000\n2.000000\n\n\n25%\n8554.500000\n4.244554e+06\n585.000000\n1.000000\n1.000000\n70.000000\n3.000000\n9.000000\n9.000000\n9.000000\n\n\n50%\n18187.000000\n9.128754e+06\n1044.000000\n1.000000\n1.000000\n103.000000\n8.000000\n10.000000\n10.000000\n10.000000\n\n\n75%\n28500.500000\n1.391076e+07\n1595.000000\n1.000000\n1.000000\n169.000000\n26.000000\n10.000000\n10.000000\n10.000000\n\n\nmax\n40504.000000\n1.797369e+07\n3317.000000\n6.000000\n10.000000\n10000.000000\n421.000000\n10.000000\n10.000000\n10.000000\n\n\n\n\n\n\n\n\nairbnb.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 30234 entries, 0 to 40503\nData columns (total 14 columns):\n #   Column                     Non-Null Count  Dtype  \n---  ------                     --------------  -----  \n 0   Unnamed: 0                 30234 non-null  int64  \n 1   id                         30234 non-null  int64  \n 2   days                       30234 non-null  int64  \n 3   last_scraped               30234 non-null  object \n 4   host_since                 30234 non-null  object \n 5   room_type                  30234 non-null  object \n 6   bathrooms                  30234 non-null  float64\n 7   bedrooms                   30234 non-null  float64\n 8   price                      30234 non-null  int64  \n 9   number_of_reviews          30234 non-null  int64  \n 10  review_scores_cleanliness  30234 non-null  float64\n 11  review_scores_location     30234 non-null  float64\n 12  review_scores_value        30234 non-null  float64\n 13  instant_bookable           30234 non-null  object \ndtypes: float64(5), int64(5), object(4)\nmemory usage: 3.5+ MB\n\n\nThe average number of reviews is 15.9 for this dataset, however the standard deviation is 29.25, meaning there is a large spread in the number of reviews, with many locations having 0 reviews (Right skewed)\nMost of the listings in the dataset rank very high in cleanliness, location, and value, with all having an average of above 9 out of 10.\nThe price of the listings is also left skewed, with the average price being $145, but the standard deviation being $211, meaning there is a large spread in the price of listings like the number of reviews. However with this spread, due to the fact that the price can’t be negative, the distribution is left skewed.\nWhile looking at the info for the columns, we will need to convert some of the variables for regression analysis. The columns need be an integer or float data type. We will be changing room type and instant bookable columns.\n\n# convert room_type to boolean\n\nairbnb = pd.get_dummies(airbnb, columns=['room_type'], drop_first=True)\n\n\nfor i in airbnb['instant_bookable']:\n    if i == 't':\n        airbnb['instant_bookable'] = 1\n    else:\n        airbnb['instant_bookable'] = 0\n\nPrior to building the regression model, we need to make sure the data is all in the same time interval since we are assuming it’s a poisson distribution.\n\n# Converting number of reviews to reviews per year\n\nairbnb['reviews_per_year'] = airbnb['number_of_reviews'] / airbnb['days'] * 365\n\nWe will also be scaling the data in order to prevent precision loss during our regression analysis.\n\nfrom sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\n# Assuming 'age' and 'age_squared' need scaling\nairbnb['days_scaled'] = scaler.fit_transform(airbnb[['days']])\nairbnb['price'] = scaler.fit_transform(airbnb[['price']])\nairbnb['review_scores_cleanliness'] = scaler.fit_transform(airbnb[['review_scores_cleanliness']])\nairbnb['review_scores_location'] = scaler.fit_transform(airbnb[['review_scores_location']])\nairbnb['review_scores_value'] = scaler.fit_transform(airbnb[['review_scores_value']])\n\nNext, we will use the previously generated MLE function to get the maximum likelihood estimators to predict our reviews per year.\n\nX = np.c_[np.ones(len(airbnb)), airbnb['days_scaled'], airbnb['bathrooms'], airbnb['bedrooms'], airbnb['price'], \n          airbnb['review_scores_cleanliness'], airbnb['review_scores_location'], airbnb['review_scores_value'], \n          airbnb['instant_bookable'], airbnb['room_type_Private room'], airbnb['room_type_Shared room']]\nY = airbnb['reviews_per_year'].values\n\n# Initial guess for beta\ninitial_beta = np.zeros(X.shape[1])\n\n# Minimization\nresult = minimize(poisson_regression_log_likelihood, initial_beta, args=(Y, X), method='BFGS')\n\nprint(\"Optimal beta:\", result.x)\n\nOptimal beta: [ 2.05080599 -0.43566627 -0.02481847  0.0721578  -0.02531794  0.13872818\n -0.05758502 -0.05235691  0.         -0.00724632 -0.07017587]\n\n\nNext, we will confirm our betas with a built-in regression function.\n\n# Fit a Poisson regression model using statsmodels\n\npoisson_model = sm.GLM(Y, X, family=sm.families.Poisson()).fit()\n\n# Print the summary of the model\n\nprint(poisson_model.summary())\n\n                 Generalized Linear Model Regression Results                  \n==============================================================================\nDep. Variable:                      y   No. Observations:                30234\nModel:                            GLM   Df Residuals:                    30224\nModel Family:                 Poisson   Df Model:                            9\nLink Function:                    Log   Scale:                          1.0000\nMethod:                          IRLS   Log-Likelihood:            -2.2032e+05\nDate:                Wed, 12 Jun 2024   Deviance:                   3.4336e+05\nTime:                        12:17:53   Pearson chi2:                 4.58e+05\nNo. Iterations:                     5   Pseudo R-squ. (CS):             0.8026\nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          z      P&gt;|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst          2.0508      0.007    313.513      0.000       2.038       2.064\nx1            -0.4357      0.002   -201.380      0.000      -0.440      -0.431\nx2            -0.0248      0.005     -4.572      0.000      -0.035      -0.014\nx3             0.0722      0.003     23.812      0.000       0.066       0.078\nx4            -0.0253      0.003     -7.690      0.000      -0.032      -0.019\nx5             0.1387      0.003     54.015      0.000       0.134       0.144\nx6            -0.0576      0.002    -27.251      0.000      -0.062      -0.053\nx7            -0.0524      0.002    -20.945      0.000      -0.057      -0.047\nx8           4.66e-18   3.65e-19     12.771      0.000    3.94e-18    5.37e-18\nx9            -0.0072      0.004     -1.663      0.096      -0.016       0.001\nx10           -0.0702      0.012     -6.036      0.000      -0.093      -0.047\n==============================================================================\n\n\nOur coefficients match, meaning per the data and our manipulation, we calculated the betas correctly.\nBelow, we are grouping the some of the data to potentially get a better explanation and generate a reasonable hypothesis for the outcome of the betas.\n\n# grouping the data based on days (binning the days) and reviews_per_year (binning reviews per year)\n\nairbnb['days_bins'] = pd.cut(airbnb['days'], bins=range(0, 400, 50))\n\nairbnb['reviews_per_year_bins'] = pd.cut(airbnb['reviews_per_year'], bins=range(0, 150, 25))\n\ndays_reviews = airbnb.groupby('days_bins')['reviews_per_year_bins'].value_counts().unstack()\n\ndays_reviews\n\n/tmp/ipykernel_63947/4142779578.py:7: FutureWarning:\n\nThe default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n\n\n\n\n\n\n\n\n\nreviews_per_year_bins\n(0, 25]\n(25, 50]\n(50, 75]\n(75, 100]\n(100, 125]\n\n\ndays_bins\n\n\n\n\n\n\n\n\n\n(0, 50]\n111\n62\n13\n10\n3\n\n\n(50, 100]\n187\n71\n18\n4\n2\n\n\n(100, 150]\n439\n117\n31\n5\n4\n\n\n(150, 200]\n438\n110\n51\n11\n4\n\n\n(200, 250]\n478\n100\n37\n10\n2\n\n\n(250, 300]\n616\n113\n32\n4\n2\n\n\n(300, 350]\n656\n133\n23\n4\n1\n\n\n\n\n\n\n\n\n# making each column in days_reviews proportional to the sum of the number in each row\n\ndays_reviews['sum'] = days_reviews.sum(axis=1)\ndays_reviews = days_reviews.div(days_reviews['sum'], axis=0)\ndays_reviews.drop(columns='sum', inplace=True)\n\ndays_reviews\n\n\n\n\n\n\n\nreviews_per_year_bins\n(0, 25]\n(25, 50]\n(50, 75]\n(75, 100]\n(100, 125]\n\n\ndays_bins\n\n\n\n\n\n\n\n\n\n(0, 50]\n0.557789\n0.311558\n0.065327\n0.050251\n0.015075\n\n\n(50, 100]\n0.663121\n0.251773\n0.063830\n0.014184\n0.007092\n\n\n(100, 150]\n0.736577\n0.196309\n0.052013\n0.008389\n0.006711\n\n\n(150, 200]\n0.713355\n0.179153\n0.083062\n0.017915\n0.006515\n\n\n(200, 250]\n0.762360\n0.159490\n0.059011\n0.015949\n0.003190\n\n\n(250, 300]\n0.803129\n0.147327\n0.041721\n0.005215\n0.002608\n\n\n(300, 350]\n0.802938\n0.162791\n0.028152\n0.004896\n0.001224\n\n\n\n\n\n\n\nLooking at the betas calculated from our Poisson Regression Likelihood model, the first explanatory variable analyzed was days listed. Days listed seems to have a negative affect on the number of reviews, with a beta of -0.91. This is significant considering the other betas. The explanation for this may be that people like to see brand new listings since as time goes on, the listings have more wear and tear, thus review scores start to decrease. However, the other explanation is there is an omitted variable we are missing that was not captured in this dataset.\nIn the days_reviews table above, it shows the proportion of reviews_per_year in comparison to each days bin. It does seem like as the number of days increases, the number of reviews per year decreases with the exception of the first bin (0-25 reviews per year).\nBathrooms also has a slightly negative affect on the number of reviews, with a beta of -0.011. This is not as significant as some of the other variables, and may be leading to omitted variable bias. However, an explanation may be that rooms are more popular than homes for the individuals in this area. If they are staying short term, they may have a bathroom outside the room.\nBedrooms do have a positive effect on the number of reviews, with a beta of 0.10. Meaning an entire house or apartment is more likely to get more reviews than a shared room. This is significant, and may be due to the fact that people are more likely to stay in a place with more bedrooms if they are traveling with a group. They also may be more likely to leave a review if traveling with a group.\nPrice has a negative effect on reviews per year, with a beta of -0.088. This may be explained by the income class of those staying and the increased prices in NYC. Most people try and spend the least amount of money as possible in order to have a satisfactory experience.\nReview scores all have a positive effect on reviews_per_year with value having the largest affect. As stated above, most people want to spend the least amount of money for a satisfactory experience, so if a place is respectively inexpensive and has a high value rating by others, than that will attract others to stay.\nInstant bookable has a significant positive effect on reviews_per_year. This aligns with the American society values. People want to be able to have control at the touch of a button. They lose interest quickly if the have to wait and want immediate feedback. Instant bookings provide that instant feedback.\nPrivate and shared rooms have a slightly negative effect, which means that entire homes or apartments are more desirable. Although price is a concern and entire homes or apartments are more expensive, they provide more privacy and space for the guests. If the guests are traveling in groups, that makes this option more affordable, which may explain why it’s positive."
  },
  {
    "objectID": "projects/Supply_Chain_Analytics/Forecasting/forecasting.html",
    "href": "projects/Supply_Chain_Analytics/Forecasting/forecasting.html",
    "title": "Forecasting Methods",
    "section": "",
    "text": "Forecasting plays a critical role in a plethora of industries. It allows businesses to plan for the future, anticipate demand, and make informed decisions. In this notebook, we will explore some of the most popular forecasting methods and apply them to the problem of predicting the future sales of a retail store.\nWe’ll be going over averages, simple exponential smoothing, and Holt’s linear trend method.\n\n\nThe natural tendency when trying to forecast a time series is to use the average of the series. This is a simple method that can be useful when the series is relatively stable and doesn’t have any trends or seasonality. However, this is the simplest of methods, and may not be maximizing your profits.\n\n\n\nSimple exponential smoothing (SES) is a technique that assigns a weight to past observations in order to make the next prediction. The weight can be between 0 and 1, with 1 meaning that it takes the full value of the previous observation and 0 meaning that it ignores the previous observation completely.\nThe formula for Simple Exponential Smoothing is:\n\\[ F_{t+1} = \\alpha Y_t + (1 - \\alpha) F_t \\]\nWhere:\n\n\\(F_{t+1}\\) is the forecast for the next period\n\\(Y_t\\) is the actual value for the current period\n\\(F_t\\) is the forecast for the current period\n\\(\\alpha\\) is the smoothing factor or level\n\nWe’ll see alpha be called the smoothing level in python packages for SES.\nA loss function can also be incorporated into the SES model in order to optimize your parameters. A loss function is a method of evaluating how well your algorithm and its parameters are performing. The most common loss function for SES is the Mean Squared Error (MSE). The MSE is calculated by taking the difference between the actual value and the forecasted value, squaring it, and then taking the average of all the squared differences.\nOnce the loss function is calculated, the goal is to minimize the loss by adjusting the parameters which in this case is the smoothing factor. There are many optimization algorithms that can be used to minimize the loss function, such as gradient descent.\n\n\n\nHolt’s Linear Trend Method is an extension of SES that incorporates a trend component. This method is useful when the data has a trend but no seasonality. The formula for Holt’s Linear Trend Method is:\n\\[ F_{t+h} = l_t + h \\cdot b_t \\]\nWhere:\n\n\\(F_{t+h}\\) is the forecast for the next period\n\\(l_t\\) is the smoothed value at the current period\n\\(b_t\\) is the trend for the current period\n\\(h\\) is the number of periods into the future you want to forecast\n\nThe formula for the level, \\(l_t\\), is:\n\\[ l_t = \\alpha Y_t + (1 - \\alpha)(l_{t-1} + b_{t-1}) \\]\nThe formula for the trend, \\(b_t\\), is:\n\\[ b_t = \\beta(l_t - l_{t-1}) + (1 - \\beta)b_{t-1} \\]\nWhere:\n\n\\(\\alpha\\) is the smoothing factor for the level\n\\(\\beta\\) is the smoothing factor for the trend\n\\(Y_t\\) is the actual value for the current period\n\\(l_{t-1}\\) is the smoothed value for the previous period\n\\(b_{t-1}\\) is the trend for the previous period\n\nJust like with SES, Holt’s Linear Trend Method can incorporate a loss function to optimize the parameters. The loss function for Holt’s Linear Trend Method is the same as the loss function for SES, which is the Mean Squared Error (MSE)."
  },
  {
    "objectID": "projects/Supply_Chain_Analytics/Forecasting/forecasting.html#average",
    "href": "projects/Supply_Chain_Analytics/Forecasting/forecasting.html#average",
    "title": "Forecasting Methods",
    "section": "",
    "text": "The natural tendency when trying to forecast a time series is to use the average of the series. This is a simple method that can be useful when the series is relatively stable and doesn’t have any trends or seasonality. However, this is the simplest of methods, and may not be maximizing your profits."
  },
  {
    "objectID": "projects/Supply_Chain_Analytics/Forecasting/forecasting.html#simple-exponential-smoothing",
    "href": "projects/Supply_Chain_Analytics/Forecasting/forecasting.html#simple-exponential-smoothing",
    "title": "Forecasting Methods",
    "section": "",
    "text": "Simple exponential smoothing (SES) is a technique that assigns a weight to past observations in order to make the next prediction. The weight can be between 0 and 1, with 1 meaning that it takes the full value of the previous observation and 0 meaning that it ignores the previous observation completely.\nThe formula for Simple Exponential Smoothing is:\n\\[ F_{t+1} = \\alpha Y_t + (1 - \\alpha) F_t \\]\nWhere:\n\n\\(F_{t+1}\\) is the forecast for the next period\n\\(Y_t\\) is the actual value for the current period\n\\(F_t\\) is the forecast for the current period\n\\(\\alpha\\) is the smoothing factor or level\n\nWe’ll see alpha be called the smoothing level in python packages for SES.\nA loss function can also be incorporated into the SES model in order to optimize your parameters. A loss function is a method of evaluating how well your algorithm and its parameters are performing. The most common loss function for SES is the Mean Squared Error (MSE). The MSE is calculated by taking the difference between the actual value and the forecasted value, squaring it, and then taking the average of all the squared differences.\nOnce the loss function is calculated, the goal is to minimize the loss by adjusting the parameters which in this case is the smoothing factor. There are many optimization algorithms that can be used to minimize the loss function, such as gradient descent."
  },
  {
    "objectID": "projects/Supply_Chain_Analytics/Forecasting/forecasting.html#holts-linear-trend-method",
    "href": "projects/Supply_Chain_Analytics/Forecasting/forecasting.html#holts-linear-trend-method",
    "title": "Forecasting Methods",
    "section": "",
    "text": "Holt’s Linear Trend Method is an extension of SES that incorporates a trend component. This method is useful when the data has a trend but no seasonality. The formula for Holt’s Linear Trend Method is:\n\\[ F_{t+h} = l_t + h \\cdot b_t \\]\nWhere:\n\n\\(F_{t+h}\\) is the forecast for the next period\n\\(l_t\\) is the smoothed value at the current period\n\\(b_t\\) is the trend for the current period\n\\(h\\) is the number of periods into the future you want to forecast\n\nThe formula for the level, \\(l_t\\), is:\n\\[ l_t = \\alpha Y_t + (1 - \\alpha)(l_{t-1} + b_{t-1}) \\]\nThe formula for the trend, \\(b_t\\), is:\n\\[ b_t = \\beta(l_t - l_{t-1}) + (1 - \\beta)b_{t-1} \\]\nWhere:\n\n\\(\\alpha\\) is the smoothing factor for the level\n\\(\\beta\\) is the smoothing factor for the trend\n\\(Y_t\\) is the actual value for the current period\n\\(l_{t-1}\\) is the smoothed value for the previous period\n\\(b_{t-1}\\) is the trend for the previous period\n\nJust like with SES, Holt’s Linear Trend Method can incorporate a loss function to optimize the parameters. The loss function for Holt’s Linear Trend Method is the same as the loss function for SES, which is the Mean Squared Error (MSE)."
  },
  {
    "objectID": "projects/Supply_Chain_Analytics/Forecasting/forecasting.html#initial-setup",
    "href": "projects/Supply_Chain_Analytics/Forecasting/forecasting.html#initial-setup",
    "title": "Forecasting Methods",
    "section": "Initial Setup",
    "text": "Initial Setup\nLet’s start by loading the data and taking a look at the first few rows and some basic analysis.\n\n\n\n\n\n\n\n\n\nt\ndemand\n\n\n\n\n0\n1\n1974\n\n\n1\n2\n1919\n\n\n2\n3\n1731\n\n\n3\n4\n1668\n\n\n4\n5\n1895\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nt\ndemand\n\n\n\n\ncount\n500.000000\n500.000000\n\n\nmean\n250.500000\n3012.316000\n\n\nstd\n144.481833\n678.488964\n\n\nmin\n1.000000\n1606.000000\n\n\n25%\n125.750000\n2477.000000\n\n\n50%\n250.500000\n3045.000000\n\n\n75%\n375.250000\n3528.250000\n\n\nmax\n500.000000\n4410.000000\n\n\n\n\n\n\n\nWe can see that the mean demand among the dataset is 3012 cupcakes with a standard deviation of 678. We’ve confirmed there are 500 data points in the dataset through the “counts” variable. We’ll take a look at the plot of the data.\n\nplt.plot(demand['t'], demand['demand'])\nplt.xlabel('Weeks')\nplt.ylabel('Demand')\nplt.title('Demand vs Weeks')\n\nText(0.5, 1.0, 'Demand vs Weeks')\n\n\n\n\n\n\n\n\n\nThere is a clear upward trend, with a decent amount of variation. That should give us a hint at which forecasting method to use. However, we will continue with all the methods to show the differences."
  },
  {
    "objectID": "projects/Supply_Chain_Analytics/Forecasting/forecasting.html#average-1",
    "href": "projects/Supply_Chain_Analytics/Forecasting/forecasting.html#average-1",
    "title": "Forecasting Methods",
    "section": "Average",
    "text": "Average\nAs stated previously, averaging is the most common and simple method for forecasting. It could be moving average, or a cumulative average. Lets see what our average is for the first 300 weeks.\n\n\nThe average demand for cupcakes in the first 300 weeks is 2600.0 cupcakes\n\n\nWith our average demand, lets determine how much profit we would make in the last 200 weeks.\n\nretail = 4\nunit_cost = 0.8\n\ndemand['profit'] = np.where(demand['demand'] &gt; avg_demand, retail*avg_demand - unit_cost*avg_demand, retail*demand['demand'] - unit_cost*avg_demand)\n\navg_profit = np.round(demand.loc[300:, 'profit'].mean(), 2)\n\nprint(f'The average profit for the last 200 weeks when using only the average demand for the past 300 weeks is ${avg_profit}')\n\nThe average profit for the last 200 weeks when using only the average demand for the past 300 weeks is $8320.0\n\n\n\nprofits = pd.DataFrame({'Profit ($)':[avg_profit]}, index=['Avg'])\n\nprofits\n\n\n\n\n\n\n\n\nProfit ($)\n\n\n\n\nAvg\n8320.0\n\n\n\n\n\n\n\nUsing just the mean as the way to forecast for the next 200 weeks makes an average weekly profit of $7508.40. Not bad, but lets see how the cumulative average works out.\n\nCumulative Average\nNow, it doesn’t matter about the past 300 weeks, since we are calculating a new average after every week, but we’ll still only calculate the average weekly profit for the last 200 weeks for comparison.\n\ndemand['cum_avg_demand'] = demand['demand'].expanding().mean()\n\nLet’s see what the cumulative average demand looks like when plotted.\n\nplt.plot(demand['t'], demand['cum_avg_demand'])\nplt.xlabel('Weeks')\nplt.ylabel('Cumulative Demand')\nplt.title('Cumulative Demand vs Weeks')\nplt.show()\n\n\n\n\n\n\n\n\nAs expected, the cumulative average has much less variation as the weeks go on, and takes a very linear trend line.\n\ndemand['cum_profit'] = np.where(demand['demand'] &gt; demand['cum_avg_demand'], retail*demand['cum_avg_demand'] - unit_cost*demand['cum_avg_demand'], retail*demand['demand'] - unit_cost*demand['cum_avg_demand'])\n\navg_cum_profit = np.round(demand.loc[300:, 'cum_profit'].mean(), 2)\n\nprint(f'The average profit for the last 200 weeks when using the cumulative average demand is ${avg_cum_profit}')\n\nThe average profit for the last 200 weeks when using the cumulative average demand is $8968.06\n\n\n\nprofits.loc['Avg Cumul.', 'Profit ($)'] = avg_cum_profit\n\nprofits\n\n\n\n\n\n\n\n\nProfit ($)\n\n\n\n\nAvg\n8320.00\n\n\nAvg Cumul.\n8968.06\n\n\n\n\n\n\n\nThe average profit decreased for cumulative profit as compared to just using the average for the first 300 weeks."
  },
  {
    "objectID": "projects/Supply_Chain_Analytics/Forecasting/forecasting.html#simple-exponential-smoothing-1",
    "href": "projects/Supply_Chain_Analytics/Forecasting/forecasting.html#simple-exponential-smoothing-1",
    "title": "Forecasting Methods",
    "section": "Simple Exponential Smoothing",
    "text": "Simple Exponential Smoothing\nAs discussed previously, Simple Exponential Smoothing (SES) assigns a weight to the current and previous level. If the weight (alpha) is high (closer to 1), that means the most recent data will weigh more heavily into the forecast than the past data points. On the opposite end, if alpha is low, it means the past data is used more to make the next forecast.\nLet’s implement the SES method into our data. We’ll use a smoothing level of 0.2 initially and take a look at the plot.\n\nSES_model = SimpleExpSmoothing(demand.loc[:300, 'demand']).fit(smoothing_level=0.2, optimized=False)\n\ndemand['SES_forecast'] = SES_model.fittedvalues\ndemand.loc[300:, 'SES_forecast'] = SES_model.forecast(200)\n\n\n\n\n\n\n\n\n\n\nThe SES method produces a singular value for the last 200 weeks. This is correct, and is why SES method is not always used. As seen in the formula, it uses keeps using the last actual value to make the next predicition. Once we want to forecast, it produces a singular value for all the future forecasts until you want to recalculate.\nLets calculate the profit for SES non-optimized version.\n\ndemand['SES_profit'] = np.where(demand['demand'] &gt; demand['SES_forecast'], retail*demand['SES_forecast'] - unit_cost*demand['SES_forecast'], retail*demand['demand'] - unit_cost*demand['SES_forecast'])\n\navg_SES_profit = np.round(demand.loc[300:, 'SES_profit'].mean(), 2)\n\nprofits.loc['SES', 'Profit ($)'] = avg_SES_profit\n\nprofits\n\n\n\n\n\n\n\n\nProfit ($)\n\n\n\n\nAvg\n8320.00\n\n\nAvg Cumul.\n8968.06\n\n\nSES\n10224.50\n\n\n\n\n\n\n\nImplementing SES method increased profits by nearly $3000 weekly!\n\nSES Optimized\nNow we’ll use and optimized alpha with the SES method. Keep in mind, alpha is being optimized based on forecast, not on profit. We’ll see if the optimized alpha translates to increased profits.\n\nSES_Opt_model = SimpleExpSmoothing(demand.loc[:300, 'demand']).fit(optimized=True)\n\ndemand['SES_Opt_forecast'] = SES_Opt_model.fittedvalues\ndemand.loc[300:, 'SES_Opt_forecast'] = SES_Opt_model.forecast(200)\n\ndemand['SES_Opt_profit'] = np.where(demand['demand'] &gt; demand['SES_Opt_forecast'], retail*demand['SES_Opt_forecast'] - unit_cost*demand['SES_Opt_forecast'], retail*demand['demand'] - unit_cost*demand['SES_Opt_forecast'])\n\navg_SES_Opt_profit = np.round(demand.loc[300:, 'SES_Opt_profit'].mean(), 2)\n\nprofits.loc['SES Optim.', 'Profit ($)'] = avg_SES_Opt_profit\n\nprofits\n\n\n\n\n\n\n\n\nProfit ($)\n\n\n\n\nAvg\n8320.00\n\n\nAvg Cumul.\n8968.06\n\n\nSES\n10224.50\n\n\nSES Optim.\n10249.05\n\n\n\n\n\n\n\nThe optimized version did increase profits by about $25 weekly!\nAnother method to ensure you have enough inventory to meet your demand is implementing a safety stock. A saftey stock allows you to be ready for the fluctuations in your demand. It is calculated by determing your cost of understocking and overstocking. Once you have those metrics, you determine your target service level, which the percentile of safety stock you want to keep on hand based on your error in your forecast to the actual demand. The Cost of understocking and overstocking are:\n\\[ Cu = retailprice - unitcost \\] \\[ Cu = unitcost - salvagevalue \\]\nThe formula for Target Service Level is:\n\\[ TSL = Cu/(Cu-Co) \\]\nLet’s find the profit for our optimized SES model when incorporating a safety stock. First we’ll find the Target service level.\n\nCu = retail - unit_cost\nCo = unit_cost\nTSL = Cu/(Cu-Co)\nTSL\n\n1.3333333333333333\n\n\nOur TSL for this product is 0.8, or 80% service level. We’ll use that to find our safety stock.\n\ndef cumulative_quantile(x):\n    return x.expanding().quantile(0.8)\ndemand['error_op'] = demand['demand'] - demand['SES_Opt_forecast']\ndemand['cum_error_optim'] = demand['error_op'].transform(cumulative_quantile)\ndemand['stockQ_op'] = demand['SES_Opt_forecast'] + demand['cum_error_optim']\ndemand['profit_op'] = np.where(demand['demand'] &gt; demand['stockQ_op'], retail*demand['stockQ_op'] - unit_cost*demand['stockQ_op'], retail*demand['demand'] - unit_cost*demand['stockQ_op'])\n\navg_SES_Opt_safety_profit = np.round(demand.loc[300:, 'profit_op'].mean(), 2)\n\nprofits.loc['SES Optim. Safety', 'Profit ($)'] = avg_SES_Opt_safety_profit\n\nprofits\n\n\n\n\n\n\n\n\nProfit ($)\n\n\n\n\nAvg\n8320.00\n\n\nAvg Cumul.\n8968.06\n\n\nSES\n10224.50\n\n\nSES Optim.\n10249.05\n\n\nSES Optim. Safety\n11039.99\n\n\n\n\n\n\n\nProfits increased by over $350 weekly!\nAs discussed previously, the optimization is happening based on the forecasts, and not profits. Let’s see if we can find a better alpha which translates to higher profits.\n\nSES_opt_alpha = SES_Opt_model.params['smoothing_level']\n\nSES_opt_alpha\n\n0.08742752183220932\n\n\n\nalpha_list = []\nSES_profit_list = []\n\nfor alpha in np.arange(0,1,0.01):\n    SES_Opt_model = SimpleExpSmoothing(demand.loc[:300, 'demand']).fit(smoothing_level = alpha, optimized=False)\n    demand['SES_Opt_forecast'] = SES_Opt_model.fittedvalues\n    demand.loc[300:, 'SES_Opt_forecast'] = SES_Opt_model.forecast(200)\n    demand['SES_Opt_profit'] = np.where(demand['demand'] &gt; demand['SES_Opt_forecast'], retail*demand['SES_Opt_forecast'] - unit_cost*demand['SES_Opt_forecast'], retail*demand['demand'] - unit_cost*demand['SES_Opt_forecast'])\n    avg_SES_Opt_profit = np.round(demand.loc[300:, 'SES_Opt_profit'].mean(), 2)\n    alpha_list.append(alpha)\n    SES_profit_list.append(avg_SES_Opt_profit)\n\n\n\n The maximum profit capable through SES method is $10260.53 which resulted from an alpha value of 0.12.\n\n\n\n\n\n\n\n\n\nProfit ($)\n\n\n\n\nAvg\n8320.00\n\n\nAvg Cumul.\n8968.06\n\n\nSES\n10224.50\n\n\nSES Optim.\n10249.05\n\n\nSES Optim. Safety\n11039.99\n\n\nSES Best Profit\n10260.53\n\n\n\n\n\n\n\nThe previous optimized model was fairly close with an alpha of 0.08 when compared to the alpha which produces the best profit for the next 200 weeks. This is not always the case where the optimized alpha is close to the alpha which produces the best profit. The optimzied model based on profit was less than when we incorporated safety stock, but still more than using the optimized parameters based on forecast.\nThis also depends on your strategy, whether its profit or hedging because you don’t want to potentially overstock."
  },
  {
    "objectID": "projects/Supply_Chain_Analytics/Forecasting/forecasting.html#holts-linear-trend-method-1",
    "href": "projects/Supply_Chain_Analytics/Forecasting/forecasting.html#holts-linear-trend-method-1",
    "title": "Forecasting Methods",
    "section": "Holt’s Linear Trend Method",
    "text": "Holt’s Linear Trend Method\nNow that we’ve seen the SES method in work, it’s time for Holt’s Linear Trend Method. As the name says, Holt’s adds on to SES by introducing a trend in addition to the level.\nWe can clearly see a trend in the demand, so forecasts and this profits should increase.\nBefore we use Holt’s, we’ll first run a linear regression model to get an initial level and trend for our model by using the intercept and coefficient. This method can lead to better forecasting by having relatively accurate starting point.\n\nlr_model = LinearRegression().fit(demand.loc[:300, 't'].values.reshape(-1, 1), demand.loc[:300,'demand'].values)\n\ninit_trend = lr_model.coef_[0] \n\ninit_level = lr_model.intercept_\n\ninit_level, init_trend\n\n(1923.0031229235879, 4.483137004686365)\n\n\nNow that we have our initial trend and level, we’ll implement those in Holt’s model. We’ll use 0.2 for alpha and beta for our initial calculations. We will plot the data as well to see how the forecasted demand follows the actual demand.\n\nHolt_model = Holt(demand.loc[:300,'demand'], initialization_method=\"known\", initial_level=init_level, initial_trend=init_trend).fit(smoothing_level=0.2, smoothing_trend=0.2, optimized=False)\ndemand['forecast_holt'] = Holt_model.fittedvalues\ndemand.loc[300:, 'forecast_holt'] = Holt_model.forecast(200)\n\n\n\n\n\n\n\n\n\n\nThe forecasted results are way off! The forecasts are decreasing rather than increasing. This is likely due to alpha and beta values combined with the large amount of variation in the data.\nA lower smoothing level means it has less weight in the most recent data and more weight in the past data. Same goes for a low smoothing trend. A low smoothing trend would mean it is less volatile to short term fluctuations, which there is plenty in this data. Maybe the alpha and beta values are too large, even though they’re at the lower end of the range?\nLet’s see what the optimized model comes up with.\n\nOptimized Holts\n\nHolt_opt_model = Holt(demand.loc[:300,'demand'], initialization_method=\"known\", initial_level=init_level, initial_trend=init_trend).fit(optimized=True)\ndemand['forecast_holt_opt'] = Holt_opt_model.fittedvalues\ndemand.loc[300:, 'forecast_holt_opt'] = Holt_opt_model.forecast(200)\n\noptimal_alpha = Holt_opt_model.model.params['smoothing_level']\noptimal_beta = Holt_opt_model.model.params['smoothing_trend']\n\noptimal_alpha, optimal_beta\n\n(1.4901161193847656e-08, 0.0)\n\n\nThe optimized holt parameters are even lower than 0.2! Alpha and beta are both close to 0, meaning that forecastingrelies on the long term trend and levels, and should not be misguided by short term fluctuations.\nHere’s a plot of the optimized data.\n\n\n\n\n\n\n\n\n\nThe fitted values we used to train our optimized holt value have almost no fluctuation. It is nearly a perfect linear line, which it should be with a beta of 0.\nNow lets check our profits for both.\n\n\n\n\n\n\n\n\n\nProfit ($)\n\n\n\n\nAvg\n8320.00\n\n\nAvg Cumul.\n8968.06\n\n\nSES\n10224.50\n\n\nSES Optim.\n10249.05\n\n\nSES Optim. Safety\n11039.99\n\n\nSES Best Profit\n10260.53\n\n\nHolt\n8627.90\n\n\nHolt Optim.\n11241.21\n\n\n\n\n\n\n\nProfits are still better than just using the average or the cumulative average when alpha and beta are 0.2, but less than all profits with SES.\nHowever, our optimized Holt model did significantly better than any of the other models!\nBefore optimizing on profit rather than forecast, lets see what implementing a safety stock does for Holts.\n\ndemand['error_op'] = demand['demand'] - demand['forecast_holt_opt']\ndemand['cum_error_optim'] = demand['error_op'].transform(cumulative_quantile)\ndemand['stockQ_op'] = demand['forecast_holt_opt'] + demand['cum_error_optim']\ndemand['profit_op'] = np.where(demand['demand'] &gt; demand['stockQ_op'], retail*demand['stockQ_op'] - unit_cost*demand['stockQ_op'], retail*demand['demand'] - unit_cost*demand['stockQ_op'])\n\navg_Holt_Opt_safety_profit = np.round(demand.loc[300:, 'profit_op'].mean(), 2)\n\nprofits.loc['Holt Optim. Safety', 'Profit ($)'] = avg_Holt_Opt_safety_profit\n\nprofits\n\n\n\n\n\n\n\n\nProfit ($)\n\n\n\n\nAvg\n8320.00\n\n\nAvg Cumul.\n8968.06\n\n\nSES\n10224.50\n\n\nSES Optim.\n10249.05\n\n\nSES Optim. Safety\n11039.99\n\n\nSES Best Profit\n10260.53\n\n\nHolt\n8627.90\n\n\nHolt Optim.\n11241.21\n\n\nHolt Optim. Safety\n11318.08\n\n\n\n\n\n\n\nProfit’s increased by about $80 a week!\nNow lets see if we can find the best parameters with Holt’s model for optimizing profit.\n\nalpha_list = []\nbeta_list = []\nHolt_profit_list = []\n\nfor alpha in np.arange(0,1,0.01):\n    for beta in np.arange(0,1,0.01):\n        Holt_profit_model = Holt(demand.loc[:300, 'demand'], initialization_method=\"known\", initial_level=init_level, initial_trend=init_trend).fit(smoothing_level = alpha, smoothing_trend = beta, optimized=False)\n        demand['Holt_demand'] = Holt_profit_model.fittedvalues\n        demand.loc[300:, 'Holt_demand'] = Holt_profit_model.forecast(200)\n        demand['Holt_Opt_profit'] = np.where(demand['demand'] &gt; demand['Holt_demand'], retail*demand['Holt_demand'] - unit_cost*demand['Holt_demand'], retail*demand['demand'] - unit_cost*demand['Holt_demand'])\n        avg_Holt_profit = np.round(demand.loc[300:, 'Holt_Opt_profit'].mean(), 2)\n        alpha_list.append(alpha)\n        beta_list.append(beta)\n        Holt_profit_list.append(avg_Holt_profit)\n\n\nprint(f'The optimal parameters in regards to profit were an alpha of {alpha_list[np.argmax(Holt_profit_list)]} and a beta of {np.round(beta_list[np.argmax(Holt_profit_list)], 2)}. This produced an average weekly profit of ${np.max(Holt_profit_list)}.')\n\nprofits.loc['Holt Best Profit', 'Profit ($)'] = np.max(Holt_profit_list)\n\nprofits\n\nThe optimal parameters in regards to profit were an alpha of 0.07 and a beta of 0.95. This produced an average weekly profit of $11332.89.\n\n\n\n\n\n\n\n\n\nProfit ($)\n\n\n\n\nAvg\n8320.00\n\n\nAvg Cumul.\n8968.06\n\n\nSES\n10224.50\n\n\nSES Optim.\n10249.05\n\n\nSES Optim. Safety\n11039.99\n\n\nSES Best Profit\n10260.53\n\n\nHolt\n8627.90\n\n\nHolt Optim.\n11241.21\n\n\nHolt Optim. Safety\n11318.08\n\n\nHolt Best Profit\n11332.89\n\n\n\n\n\n\n\nIn contradiction, the optimal parameters in regards to profit had a higher alpha, but still low, meaning it using past data created better results than current data in regards to smoothing level. However, the smoothing trend was very high (close to 1), meaning it was best to follow the most recent changes in trend than historical trend.\nLet’s take a look at the plot to confirm.\n\n\n\n\n\n\n\n\n\nThe forecasted trend was still positive, but had a slightly higher slope than the optimized model. This caused the forecasts to be at the higher end of the actual demand in most weeks. Since the cost of understocking is much higher than overstocking, that was the difference in having the best possible profit using holt’s linear trend method."
  },
  {
    "objectID": "Notebooks/HW5.html",
    "href": "Notebooks/HW5.html",
    "title": "Nick_Shuckerow",
    "section": "",
    "text": "todo: write your own code to implement the k-means algorithm. Make plots of the various steps the algorithm takes so you can “see” the algorithm working. Test your algorithm on either the Iris or PalmerPenguins datasets. Compare your results to the built-in kmeans function in R or Python.\n\nimport numpy as np\n\n\n\ndef initialize_centroids(X, k):\n    \"\"\"\n    Randomly initialize k centroids from the dataset X.\n    \"\"\"\n    np.random.seed(42)\n\n    # Randomly choose k data points from the dataset as initial centroids\n    random_indices = np.random.choice(X.shape[0], size=k, replace=False)\n\n    # Creates array of data points that are the initial centroids\n    centroids = X[random_indices, :]\n    return centroids\n\n\ndef compute_distances(X, centroids):\n    \"\"\"\n    Compute the distance from each point in X to each centroid.\n    \"\"\"\n    # Create a matrix of distances between each data point and each centroid\n    distances = np.zeros((X.shape[0], len(centroids)))\n    for i, centroid in enumerate(centroids):\n        distances[:, i] = np.linalg.norm(X - centroid, axis=1)\n    return distances\n\n\ndef assign_clusters(distances):\n    \"\"\"\n    Assign each point to the nearest centroid.\n    \"\"\"\n    return np.argmin(distances, axis=1)\n\n\ndef update_centroids(X, labels, k):\n    \"\"\"\n    Update the centroids by calculating the mean of the points assigned to each centroid.\n    \"\"\"\n    new_centroids = np.zeros((k, X.shape[1]))\n    for i in range(k):\n        new_centroids[i, :] = X[labels == i].mean(axis=0)\n    return new_centroids\n\n\ndef k_means(X, k, max_iters=100, tol=1e-4):\n    \"\"\"\n    The main function to run the k-means algorithm.\n    \"\"\"\n    # Step 1: Initialize centroids\n    centroids = initialize_centroids(X, k)\n    for _ in range(max_iters):\n        # Step 2: Compute distances and assign clusters\n        distances = compute_distances(X, centroids)\n        labels = assign_clusters(distances)\n        \n        # Step 3: Update centroids\n        new_centroids = update_centroids(X, labels, k)\n        \n        # Step 4: Check for convergence\n        if np.all(np.abs(new_centroids - centroids) &lt; tol):\n            break\n        centroids = new_centroids\n    \n    return centroids, labels\n\n\n# load iris dataset\nimport pandas as pd\nfrom sklearn.datasets import load_iris\n\niris = load_iris()\n\nX = iris.data\n\ndf = pd.DataFrame(iris.data, columns=iris.feature_names)\n\n# Run k-means algorithm\n\nk_means(X, k=3)\n\n(array([[5.9016129 , 2.7483871 , 4.39354839, 1.43387097],\n        [5.006     , 3.428     , 1.462     , 0.246     ],\n        [6.85      , 3.07368421, 5.74210526, 2.07105263]]),\n array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n        1, 1, 1, 1, 1, 1, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 2, 2, 2, 2, 0, 2, 2, 2,\n        2, 2, 2, 0, 0, 2, 2, 2, 2, 0, 2, 0, 2, 0, 2, 2, 0, 0, 2, 2, 2, 2,\n        2, 0, 2, 2, 2, 2, 0, 2, 2, 2, 0, 2, 2, 2, 0, 2, 2, 0]))\n\n\n\n# plot k-means results with centroids and color coded clusters\n\nimport matplotlib.pyplot as plt\n\ncentroids, labels = k_means(X, k=3)\n\nplt.scatter(X[labels == 0, 0], X[labels == 0, 1], color='red', label='Cluster 1')\nplt.scatter(X[labels == 1, 0], X[labels == 1, 1], color='blue', label='Cluster 2')\nplt.scatter(X[labels == 2, 0], X[labels == 2, 1], color='green', label='Cluster 3')\nplt.scatter(centroids[:, 0], centroids[:, 1], color='black', marker='x', label='Centroids')\nplt.xlabel(iris.feature_names[0])\nplt.ylabel(iris.feature_names[1])\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\n# run k means python function\n\nfrom sklearn.cluster import KMeans\n\nkmeans = KMeans(n_clusters=3, random_state=42).fit(X)\n\nkmeans_centroids = kmeans.cluster_centers_\n\nkmeans_labels = kmeans.labels_\n\n/opt/conda/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\n\n\n\n# plot k-means results with centroids and color coded clusters\n\nplt.scatter(X[kmeans_labels == 0, 0], X[kmeans_labels == 0, 1], color='red', label='Cluster 1')\nplt.scatter(X[kmeans_labels == 1, 0], X[kmeans_labels == 1, 1], color='blue', label='Cluster 2')\nplt.scatter(X[kmeans_labels == 2, 0], X[kmeans_labels == 2, 1], color='green', label='Cluster 3')\nplt.scatter(kmeans_centroids[:, 0], kmeans_centroids[:, 1], color='black', marker='x', label='Centroids')\nplt.xlabel(iris.feature_names[0])\nplt.ylabel(iris.feature_names[1])\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\n# Compare results from self-made k-means and sklearn's k-means\n\n\nnp.allclose(k_means(X, k=3)[0], kmeans.cluster_centers_)\n\nnp.allclose(k_means(X, k=3)[1], kmeans.labels_)\n\n\nTrue\n\n\n\n# Initialize centroid\n\ninit_cent = initialize_centroids(X[:,[0,1]], k=2)[:,[0,1]]\n\ninit_cent\n\narray([[6.1, 2.8],\n       [5.7, 3.8]])\n\n\n\n# create scatter plot of X with column 1 on x-axis and column 2 on y-axis\n\n# add init_cent to the plot and color them differently and make them larger than the normal points\n\nplt.scatter(X[:, 0], X[:, 1])\nplt.scatter(init_cent[:, 0], init_cent[:, 1], s=100, c='red')\nplt.xlabel(iris.feature_names[0])\nplt.ylabel(iris.feature_names[1])\nplt.show()\n\n\n\n\n\n\n\n\n\ncomp_dist = compute_distances(X[:,[0,1]], init_cent)\n\ncomp_dist = pd.DataFrame(comp_dist, columns=['centroid_1', 'centroid_2'])\n\n\n\n\n\n\n# move the text up so its not on the line itself\n\nplt.scatter(init_cent[:, 0], init_cent[:, 1], s=100, c='red')\nplt.scatter(X[0, 0], X[0, 1], s=100, c='green')\nplt.plot([init_cent[0, 0], X[0, 0]], [init_cent[0, 1], X[0, 1]], c='black')\nplt.plot([init_cent[1, 0], X[0, 0]], [init_cent[1, 1], X[0, 1]], c='black')\nplt.text((init_cent[0, 0] + X[0, 0]) / 2, (init_cent[0, 1] + X[0, 1]) / 2 + 0.05, f'{comp_dist[0,0]:.2f}', c='black')\nplt.text((init_cent[1, 0] + X[0, 0]) / 2, (init_cent[1, 1] + X[0, 1]) / 2 + 0.1, f'{comp_dist[0,1]:.2f}', c='black')\nplt.xlabel(iris.feature_names[0])\nplt.ylabel(iris.feature_names[1])\nplt.legend(['Centroids','Data Point 1'])\nplt.show()\n\nKeyError: (0, 0)\n\n\n\n\n\n\n\n\n\n\ncluster = assign_clusters(comp_dist)\n\n# merge cluster and comp_dist into one dataframe\n\ncomp_dist['cluster'] = cluster\n\ncomp_dist\n\n\n\n\n\n\n\n\ncentroid_1\ncentroid_2\ncluster\n\n\n\n\n0\n1.220656\n0.670820\n1\n\n\n1\n1.216553\n1.131371\n1\n\n\n2\n1.456022\n1.166190\n1\n\n\n3\n1.529706\n1.303840\n1\n\n\n4\n1.360147\n0.728011\n1\n\n\n...\n...\n...\n...\n\n\n145\n0.632456\n1.280625\n0\n\n\n146\n0.360555\n1.431782\n0\n\n\n147\n0.447214\n1.131371\n0\n\n\n148\n0.608276\n0.640312\n0\n\n\n149\n0.282843\n0.824621\n0\n\n\n\n\n150 rows × 3 columns\n\n\n\n\n# plot the data points and the centroids, color the data points according to the cluster they belong to\n\nplt.scatter(X[:, 0], X[:, 1], c=cluster)\nplt.scatter(init_cent[:, 0], init_cent[:, 1], s=100, c='red')\nplt.xlabel(iris.feature_names[0])\nplt.ylabel(iris.feature_names[1])\nplt.show()\n\n\n\n\n\n\n\n\n\nupdate_centroids(X[:,[0,1]], cluster, k=2)\n\narray([[6.247, 2.861],\n       [5.036, 3.45 ]])\n\n\n\n# Plot the data points and the updated centroids\n\nplt.scatter(X[:, 0], X[:, 1], c=cluster)\nplt.scatter(update_centroids(X[:,[0,1,2]], cluster, k=3)[:, 0], update_centroids(X[:,[0,1]], cluster, k=2)[:, 1], s=100, c='red')\nplt.xlabel(iris.feature_names[0])\nplt.ylabel(iris.feature_names[1])\nplt.show()\n\nNameError: name 'cluster' is not defined\n\n\n\nX\n\narray([[5.1, 3.5, 1.4, 0.2],\n       [4.9, 3. , 1.4, 0.2],\n       [4.7, 3.2, 1.3, 0.2],\n       [4.6, 3.1, 1.5, 0.2],\n       [5. , 3.6, 1.4, 0.2],\n       [5.4, 3.9, 1.7, 0.4],\n       [4.6, 3.4, 1.4, 0.3],\n       [5. , 3.4, 1.5, 0.2],\n       [4.4, 2.9, 1.4, 0.2],\n       [4.9, 3.1, 1.5, 0.1],\n       [5.4, 3.7, 1.5, 0.2],\n       [4.8, 3.4, 1.6, 0.2],\n       [4.8, 3. , 1.4, 0.1],\n       [4.3, 3. , 1.1, 0.1],\n       [5.8, 4. , 1.2, 0.2],\n       [5.7, 4.4, 1.5, 0.4],\n       [5.4, 3.9, 1.3, 0.4],\n       [5.1, 3.5, 1.4, 0.3],\n       [5.7, 3.8, 1.7, 0.3],\n       [5.1, 3.8, 1.5, 0.3],\n       [5.4, 3.4, 1.7, 0.2],\n       [5.1, 3.7, 1.5, 0.4],\n       [4.6, 3.6, 1. , 0.2],\n       [5.1, 3.3, 1.7, 0.5],\n       [4.8, 3.4, 1.9, 0.2],\n       [5. , 3. , 1.6, 0.2],\n       [5. , 3.4, 1.6, 0.4],\n       [5.2, 3.5, 1.5, 0.2],\n       [5.2, 3.4, 1.4, 0.2],\n       [4.7, 3.2, 1.6, 0.2],\n       [4.8, 3.1, 1.6, 0.2],\n       [5.4, 3.4, 1.5, 0.4],\n       [5.2, 4.1, 1.5, 0.1],\n       [5.5, 4.2, 1.4, 0.2],\n       [4.9, 3.1, 1.5, 0.2],\n       [5. , 3.2, 1.2, 0.2],\n       [5.5, 3.5, 1.3, 0.2],\n       [4.9, 3.6, 1.4, 0.1],\n       [4.4, 3. , 1.3, 0.2],\n       [5.1, 3.4, 1.5, 0.2],\n       [5. , 3.5, 1.3, 0.3],\n       [4.5, 2.3, 1.3, 0.3],\n       [4.4, 3.2, 1.3, 0.2],\n       [5. , 3.5, 1.6, 0.6],\n       [5.1, 3.8, 1.9, 0.4],\n       [4.8, 3. , 1.4, 0.3],\n       [5.1, 3.8, 1.6, 0.2],\n       [4.6, 3.2, 1.4, 0.2],\n       [5.3, 3.7, 1.5, 0.2],\n       [5. , 3.3, 1.4, 0.2],\n       [7. , 3.2, 4.7, 1.4],\n       [6.4, 3.2, 4.5, 1.5],\n       [6.9, 3.1, 4.9, 1.5],\n       [5.5, 2.3, 4. , 1.3],\n       [6.5, 2.8, 4.6, 1.5],\n       [5.7, 2.8, 4.5, 1.3],\n       [6.3, 3.3, 4.7, 1.6],\n       [4.9, 2.4, 3.3, 1. ],\n       [6.6, 2.9, 4.6, 1.3],\n       [5.2, 2.7, 3.9, 1.4],\n       [5. , 2. , 3.5, 1. ],\n       [5.9, 3. , 4.2, 1.5],\n       [6. , 2.2, 4. , 1. ],\n       [6.1, 2.9, 4.7, 1.4],\n       [5.6, 2.9, 3.6, 1.3],\n       [6.7, 3.1, 4.4, 1.4],\n       [5.6, 3. , 4.5, 1.5],\n       [5.8, 2.7, 4.1, 1. ],\n       [6.2, 2.2, 4.5, 1.5],\n       [5.6, 2.5, 3.9, 1.1],\n       [5.9, 3.2, 4.8, 1.8],\n       [6.1, 2.8, 4. , 1.3],\n       [6.3, 2.5, 4.9, 1.5],\n       [6.1, 2.8, 4.7, 1.2],\n       [6.4, 2.9, 4.3, 1.3],\n       [6.6, 3. , 4.4, 1.4],\n       [6.8, 2.8, 4.8, 1.4],\n       [6.7, 3. , 5. , 1.7],\n       [6. , 2.9, 4.5, 1.5],\n       [5.7, 2.6, 3.5, 1. ],\n       [5.5, 2.4, 3.8, 1.1],\n       [5.5, 2.4, 3.7, 1. ],\n       [5.8, 2.7, 3.9, 1.2],\n       [6. , 2.7, 5.1, 1.6],\n       [5.4, 3. , 4.5, 1.5],\n       [6. , 3.4, 4.5, 1.6],\n       [6.7, 3.1, 4.7, 1.5],\n       [6.3, 2.3, 4.4, 1.3],\n       [5.6, 3. , 4.1, 1.3],\n       [5.5, 2.5, 4. , 1.3],\n       [5.5, 2.6, 4.4, 1.2],\n       [6.1, 3. , 4.6, 1.4],\n       [5.8, 2.6, 4. , 1.2],\n       [5. , 2.3, 3.3, 1. ],\n       [5.6, 2.7, 4.2, 1.3],\n       [5.7, 3. , 4.2, 1.2],\n       [5.7, 2.9, 4.2, 1.3],\n       [6.2, 2.9, 4.3, 1.3],\n       [5.1, 2.5, 3. , 1.1],\n       [5.7, 2.8, 4.1, 1.3],\n       [6.3, 3.3, 6. , 2.5],\n       [5.8, 2.7, 5.1, 1.9],\n       [7.1, 3. , 5.9, 2.1],\n       [6.3, 2.9, 5.6, 1.8],\n       [6.5, 3. , 5.8, 2.2],\n       [7.6, 3. , 6.6, 2.1],\n       [4.9, 2.5, 4.5, 1.7],\n       [7.3, 2.9, 6.3, 1.8],\n       [6.7, 2.5, 5.8, 1.8],\n       [7.2, 3.6, 6.1, 2.5],\n       [6.5, 3.2, 5.1, 2. ],\n       [6.4, 2.7, 5.3, 1.9],\n       [6.8, 3. , 5.5, 2.1],\n       [5.7, 2.5, 5. , 2. ],\n       [5.8, 2.8, 5.1, 2.4],\n       [6.4, 3.2, 5.3, 2.3],\n       [6.5, 3. , 5.5, 1.8],\n       [7.7, 3.8, 6.7, 2.2],\n       [7.7, 2.6, 6.9, 2.3],\n       [6. , 2.2, 5. , 1.5],\n       [6.9, 3.2, 5.7, 2.3],\n       [5.6, 2.8, 4.9, 2. ],\n       [7.7, 2.8, 6.7, 2. ],\n       [6.3, 2.7, 4.9, 1.8],\n       [6.7, 3.3, 5.7, 2.1],\n       [7.2, 3.2, 6. , 1.8],\n       [6.2, 2.8, 4.8, 1.8],\n       [6.1, 3. , 4.9, 1.8],\n       [6.4, 2.8, 5.6, 2.1],\n       [7.2, 3. , 5.8, 1.6],\n       [7.4, 2.8, 6.1, 1.9],\n       [7.9, 3.8, 6.4, 2. ],\n       [6.4, 2.8, 5.6, 2.2],\n       [6.3, 2.8, 5.1, 1.5],\n       [6.1, 2.6, 5.6, 1.4],\n       [7.7, 3. , 6.1, 2.3],\n       [6.3, 3.4, 5.6, 2.4],\n       [6.4, 3.1, 5.5, 1.8],\n       [6. , 3. , 4.8, 1.8],\n       [6.9, 3.1, 5.4, 2.1],\n       [6.7, 3.1, 5.6, 2.4],\n       [6.9, 3.1, 5.1, 2.3],\n       [5.8, 2.7, 5.1, 1.9],\n       [6.8, 3.2, 5.9, 2.3],\n       [6.7, 3.3, 5.7, 2.5],\n       [6.7, 3. , 5.2, 2.3],\n       [6.3, 2.5, 5. , 1.9],\n       [6.5, 3. , 5.2, 2. ],\n       [6.2, 3.4, 5.4, 2.3],\n       [5.9, 3. , 5.1, 1.8]])\n\n\n\nwcss = []\nk_values = range(1, 11)\nfor k in k_values:\n    kmeans = KMeans(n_clusters=k, random_state=0)\n    kmeans.fit(X)\n    wcss.append(kmeans.inertia_)\n\n/opt/conda/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\n/opt/conda/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\n/opt/conda/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\n/opt/conda/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\n/opt/conda/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\n/opt/conda/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\n/opt/conda/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\n/opt/conda/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\n/opt/conda/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\n/opt/conda/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\n\n\n\nplt.plot(k_values, wcss, 'bo-')\nplt.xlabel('Number of clusters (k)')\nplt.ylabel('Within-cluster sum of squares (WCSS)')\nplt.title('Elbow Method for Optimal k')\nplt.show()\n\n\n\n\n\n\n\n\n\nfrom sklearn.metrics import silhouette_score\n\nsilhouette_scores = []\nk_values = range(2, 11)  # Silhouette score is not defined for k=1\n\nfor k in k_values:\n    kmeans = KMeans(n_clusters=k, random_state=0)\n    kmeans.fit(X)\n    score = silhouette_score(X, kmeans.labels_)\n    silhouette_scores.append(score)\n\n/opt/conda/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\n/opt/conda/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\n/opt/conda/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\n/opt/conda/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\n/opt/conda/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\n/opt/conda/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\n/opt/conda/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\n/opt/conda/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\n/opt/conda/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\n\n\n\nplt.plot(k_values, silhouette_scores, 'bo-')\nplt.xlabel('Number of clusters (k)')\nplt.ylabel('Silhouette Score')\nplt.title('Silhouette Method for Optimal k')\nplt.show()\n\n\n\n\n\n\n\n\n\nfrom yellowbrick.cluster import SilhouetteVisualizer\n\nkmeans = KMeans(n_clusters=2, random_state=0)\nvisualizer = SilhouetteVisualizer(kmeans, colors='yellowbrick')\n\nvisualizer.fit(X)        # Fit the data to the visualizer\nvisualizer.show()        # Finalize and render the figure\n\n/opt/conda/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\n\n\n\n\n\n\n\n\n\n\nfrom yellowbrick.cluster import SilhouetteVisualizer\n\nkmeans = KMeans(n_clusters=3, random_state=0)\nvisualizer = SilhouetteVisualizer(kmeans, colors='yellowbrick')\n\nvisualizer.fit(X)        # Fit the data to the visualizer\nvisualizer.show()        # Finalize and render the figure\n\n/opt/conda/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\n\n\n\n\n\n\n\n\n\ntodo: Use the Yogurt dataset from HW3 to estimate a latent-class MNL model. This model was formally introduced in the paper by Kamakura & Russell (1989), which you may want to read or reference. Compare the results to the standard (aggregate) MNL model from HW3. What are the differences in the parameter estimates?\n\nyogurt = pd.read_csv('yogurt_data.csv')\n\n\n\nyogurt['yogurt_type'] = yogurt[['y1', 'y2', 'y3', 'y4']].idxmax(axis=1).str[1].astype(int)\n\n\nyogurt.head()\n\n\n\n\n\n\n\n\nid\ny1\ny2\ny3\ny4\nf1\nf2\nf3\nf4\np1\np2\np3\np4\nyogurt_type\n\n\n\n\n0\n1\n0\n0\n0\n1\n0\n0\n0\n0\n0.108\n0.081\n0.061\n0.079\n4\n\n\n1\n2\n0\n1\n0\n0\n0\n0\n0\n0\n0.108\n0.098\n0.064\n0.075\n2\n\n\n2\n3\n0\n1\n0\n0\n0\n0\n0\n0\n0.108\n0.098\n0.061\n0.086\n2\n\n\n3\n4\n0\n1\n0\n0\n0\n0\n0\n0\n0.108\n0.098\n0.061\n0.086\n2\n\n\n4\n5\n0\n1\n0\n0\n0\n0\n0\n0\n0.125\n0.098\n0.049\n0.079\n2\n\n\n\n\n\n\n\n\ny1 = yogurt['y1'].sum()\ny2 = yogurt['y2'].sum()\ny3 = yogurt['y3'].sum()\ny4 = yogurt['y4'].sum()\n\ny1_share = y1 / (y1 + y2 + y3 + y4)\ny2_share = y2 / (y1 + y2 + y3 + y4)\ny3_share = y3 / (y1 + y2 + y3 + y4)\ny4_share = y4 / (y1 + y2 + y3 + y4)\n\ny1_share, y2_share, y3_share, y4_share\n\n(0.3419753086419753,\n 0.4012345679012346,\n 0.029218106995884775,\n 0.22757201646090536)\n\n\n\nimport rpy2.robjects as ro\nfrom rpy2.robjects import pandas2ri\nfrom rpy2.robjects.packages import importr\n\n\npandas2ri.activate()\n\n# Load poLCA package\nbase = importr('base')\nutils = importr('utils')\npoLCA = importr('poLCA')\n# Convert the pandas DataFrame to an R data.frame\nr_df = pandas2ri.py2rpy(yogurt)\n\nmodel_formula = ro.Formula('cbind(f1, f2, f3, f4, p1, p2, p3, p4) ~ 1')\n\n# Run poLCA with 2 latent classes\nn_classes = 2\npoLCA_result = poLCA.poLCA(model_formula, r_df, nclass=n_classes)\n\npoLCA_result_py = ro.conversion.rpy2py(poLCA_result)\n\nclass_coefficients = poLCA_result_py.rx2('coeff')\n\nclass_coefficients\n\n\n ALERT: some manifest variables contain values that are not\n    positive integers. For poLCA to run, please recode categorical\n    outcome variables to increment from 1 to the maximum number of\n    outcome categories for each variable. \n\n\n\n&lt;rpy2.rinterface_lib.sexp.NULLType object at 0x7f4a4b0cc7d0&gt; [0]\n\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Create some example data\nnp.random.seed(0)\nX = np.random.randn(100, 2)\ny = (X[:, 0] + 0.5 * X[:, 1] &gt; 0.5).astype(int)\n\n# Plot the data\nplt.scatter(X[:, 0], X[:, 1], c=y, cmap='bwr', alpha=0.7)\nplt.xlabel('x1')\nplt.ylabel('x2')\nplt.title('Example Data')\nplt.show()\n\n\n\n\n\n\n\n\n\n# Linear classifier without bias\nw = np.array([1, 1])\nb = 0\n\n# Plot data points\nplt.scatter(X[:, 0], X[:, 1], c=y, cmap='bwr', alpha=0.7)\n# Plot decision boundary\nx1 = np.linspace(-3, 3, 100)\nx2 = -w[0] / w[1] * x1\nplt.plot(x1, x2, label='Without Bias (Passes through origin)')\nplt.xlabel('x1')\nplt.ylabel('x2')\nplt.legend()\nplt.title('Decision Boundary without Bias')\nplt.show()\n\n\n\n\n\n\n\n\n\n# Linear classifier with bias\nw = np.array([1, 1])\nb = -1\n\n# Plot data points\nplt.scatter(X[:, 0], X[:, 1], c=y, cmap='bwr', alpha=0.7)\n# Plot decision boundary\nx1 = np.linspace(-3, 3, 100)\nx2 = -w[0] / w[1] * x1 - b / w[1]\nplt.plot(x1, x2, label='With Bias (Shifted)')\nplt.xlabel('x1')\nplt.ylabel('x2')\nplt.legend()\nplt.title('Decision Boundary with Bias')\nplt.show()"
  }
]