[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Nick Shuckerow",
    "section": "",
    "text": "Welcome to my website!\nI am a Systems Engineer for Raytheon"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "resume.html",
    "href": "resume.html",
    "title": "Nick Shuckerow’s Resume",
    "section": "",
    "text": "Download PDF file."
  },
  {
    "objectID": "projects/project1/index.html",
    "href": "projects/project1/index.html",
    "title": "Analysis of Cars",
    "section": "",
    "text": "Let’s investigate the relationship between fuel efficiency (mpg) and engine displacement (disp) from the mtcars dataset. Those variables have a correlation of -0.85.\n\n\nHere is a plot:\n\nlibrary(tidyverse)\ndata(mtcars)\nmtcars |&gt;\n  ggplot(aes(mpg, disp)) + \n  geom_point(color=\"dodgerblue4\", size=2)"
  },
  {
    "objectID": "projects/project1/index.html#sub-header",
    "href": "projects/project1/index.html#sub-header",
    "title": "Analysis of Cars",
    "section": "",
    "text": "Here is a plot:\n\nlibrary(tidyverse)\ndata(mtcars)\nmtcars |&gt;\n  ggplot(aes(mpg, disp)) + \n  geom_point(color=\"dodgerblue4\", size=2)"
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "My Projects",
    "section": "",
    "text": "A Replication of Karlan and List (2007)\n\n\n\n\n\n\nNick Shuckerow\n\n\nMay 1, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nAnalysis of Cars\n\n\n\n\n\n\nYour Name\n\n\nMay 1, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nPoisson Regression Examples\n\n\n\n\n\n\nNicholas Shuckerow\n\n\nMay 1, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nVariable Definitions\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "projects/HW1/hw1_questions.html",
    "href": "projects/HW1/hw1_questions.html",
    "title": "A Replication of Karlan and List (2007)",
    "section": "",
    "text": "Dean Karlan at Yale and John List at the University of Chicago conducted a field experiment to test the effectiveness of different fundraising letters. They sent out 50,000 fundraising letters to potential donors, randomly assigning each letter to one of three treatments: a standard letter, a matching grant letter, or a challenge grant letter. They published the results of this experiment in the American Economic Review in 2007. The article and supporting data are available from the AEA website and from Innovations for Poverty Action as part of Harvard’s Dataverse.\nThe experiment was conducted through a direct mail campaign for a liberal nonprofit organization. The authors assigned 67% of the 50,000 to the treatment group and 33% to the control group.\nThe letters sent to the treatment and control group were the same except in the following regards:\n\nThe treatment group letter had an additional paragraph saying that their donation will be matched.\nThe reply card for the treatment group had details on the matching grant.\n\nThe letters were randomized through 3 different dimensions.\n\nmatching price ratio\nmaximum match amount\nsuggested donation amount\n\nThe experiment found that using matching donations increases the revenue per solicitation and the probability that someone donates.\nIt also found that larger match ratios relative to smaller match ratios have no additional impact. This project seeks to replicate their results.\nOverall, the authors found a unique way to measure the scope of a public good (donations) in a real world setting, rather than hypothetical scenarios. This is a valuable contribution to the literature on charitable giving and public goods."
  },
  {
    "objectID": "projects/HW1/hw1_questions.html#introduction",
    "href": "projects/HW1/hw1_questions.html#introduction",
    "title": "A Replication of Karlan and List (2007)",
    "section": "",
    "text": "Dean Karlan at Yale and John List at the University of Chicago conducted a field experiment to test the effectiveness of different fundraising letters. They sent out 50,000 fundraising letters to potential donors, randomly assigning each letter to one of three treatments: a standard letter, a matching grant letter, or a challenge grant letter. They published the results of this experiment in the American Economic Review in 2007. The article and supporting data are available from the AEA website and from Innovations for Poverty Action as part of Harvard’s Dataverse.\nThe experiment was conducted through a direct mail campaign for a liberal nonprofit organization. The authors assigned 67% of the 50,000 to the treatment group and 33% to the control group.\nThe letters sent to the treatment and control group were the same except in the following regards:\n\nThe treatment group letter had an additional paragraph saying that their donation will be matched.\nThe reply card for the treatment group had details on the matching grant.\n\nThe letters were randomized through 3 different dimensions.\n\nmatching price ratio\nmaximum match amount\nsuggested donation amount\n\nThe experiment found that using matching donations increases the revenue per solicitation and the probability that someone donates.\nIt also found that larger match ratios relative to smaller match ratios have no additional impact. This project seeks to replicate their results.\nOverall, the authors found a unique way to measure the scope of a public good (donations) in a real world setting, rather than hypothetical scenarios. This is a valuable contribution to the literature on charitable giving and public goods."
  },
  {
    "objectID": "projects/HW1/hw1_questions.html#data",
    "href": "projects/HW1/hw1_questions.html#data",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Data",
    "text": "Data\n\nDescription\nThe dataset seen below which Karlan and List gathered on the 50,083 subjects has over 52 columns or variables. The primary variables we are concerned with for our research are ‘gave’ (if the individual donated any amount), ‘amount’ (what amount they donated), ‘treatment’ (if they were a part of the treatment or control group), and ‘ratio’ (matching ratio given to that subject).\nMany of the columns are redundant (treatmeant vs control columns, ratio columns vs ratio1, ratio2, ratio3 columns), however they allow us to more easily filter the data and make calculations since they are in binary format, rather than categorical (text).\nIn the second cell, you can see the description of variables, including the first two variables, ‘treatment’ and ‘control’. Through this, it is shown that 66% of the participants were a part of the treatment group, and 33% were a part of the control group.\nSince many of these variables are binary, a 1 or 0 is used to describe whether the participant had that treatment, variables, or attribute, which allows analysts to easily calculate proportions or percentages like with treatment and control.\n\nimport pandas as pd\nkarlan = pd.read_stata(\"karlan_list_2007.dta\")\nkarlan\n\n\n\n\n\n\n\n\ntreatment\ncontrol\nratio\nratio2\nratio3\nsize\nsize25\nsize50\nsize100\nsizeno\n...\nredcty\nbluecty\npwhite\npblack\npage18_39\nave_hh_sz\nmedian_hhincome\npowner\npsch_atlstba\npop_propurban\n\n\n\n\n0\n0\n1\nControl\n0\n0\nControl\n0\n0\n0\n0\n...\n0.0\n1.0\n0.446493\n0.527769\n0.317591\n2.10\n28517.0\n0.499807\n0.324528\n1.000000\n\n\n1\n0\n1\nControl\n0\n0\nControl\n0\n0\n0\n0\n...\n1.0\n0.0\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n2\n1\n0\n1\n0\n0\n$100,000\n0\n0\n1\n0\n...\n0.0\n1.0\n0.935706\n0.011948\n0.276128\n2.48\n51175.0\n0.721941\n0.192668\n1.000000\n\n\n3\n1\n0\n1\n0\n0\nUnstated\n0\n0\n0\n1\n...\n1.0\n0.0\n0.888331\n0.010760\n0.279412\n2.65\n79269.0\n0.920431\n0.412142\n1.000000\n\n\n4\n1\n0\n1\n0\n0\n$50,000\n0\n1\n0\n0\n...\n0.0\n1.0\n0.759014\n0.127421\n0.442389\n1.85\n40908.0\n0.416072\n0.439965\n1.000000\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n50078\n1\n0\n1\n0\n0\n$25,000\n1\n0\n0\n0\n...\n0.0\n1.0\n0.872797\n0.089959\n0.257265\n2.13\n45047.0\n0.771316\n0.263744\n1.000000\n\n\n50079\n0\n1\nControl\n0\n0\nControl\n0\n0\n0\n0\n...\n0.0\n1.0\n0.688262\n0.108889\n0.288792\n2.67\n74655.0\n0.741931\n0.586466\n1.000000\n\n\n50080\n0\n1\nControl\n0\n0\nControl\n0\n0\n0\n0\n...\n1.0\n0.0\n0.900000\n0.021311\n0.178689\n2.36\n26667.0\n0.778689\n0.107930\n0.000000\n\n\n50081\n1\n0\n3\n0\n1\nUnstated\n0\n0\n0\n1\n...\n1.0\n0.0\n0.917206\n0.008257\n0.225619\n2.57\n39530.0\n0.733988\n0.184768\n0.634903\n\n\n50082\n1\n0\n3\n0\n1\n$25,000\n1\n0\n0\n0\n...\n0.0\n1.0\n0.530023\n0.074112\n0.340698\n3.70\n48744.0\n0.717843\n0.127941\n0.994181\n\n\n\n\n50083 rows × 51 columns\n\n\n\n\n\n\n\n\n\n\nkarlan.describe()\n\n\n\n\n\n\n\n\ntreatment\ncontrol\nratio2\nratio3\nsize25\nsize50\nsize100\nsizeno\naskd1\naskd2\n...\nredcty\nbluecty\npwhite\npblack\npage18_39\nave_hh_sz\nmedian_hhincome\npowner\npsch_atlstba\npop_propurban\n\n\n\n\ncount\n50083.000000\n50083.000000\n50083.000000\n50083.000000\n50083.000000\n50083.000000\n50083.000000\n50083.000000\n50083.000000\n50083.000000\n...\n49978.000000\n49978.000000\n48217.000000\n48047.000000\n48217.000000\n48221.000000\n48209.000000\n48214.000000\n48215.000000\n48217.000000\n\n\nmean\n0.666813\n0.333187\n0.222311\n0.222211\n0.166723\n0.166623\n0.166723\n0.166743\n0.222311\n0.222291\n...\n0.510245\n0.488715\n0.819599\n0.086710\n0.321694\n2.429012\n54815.700533\n0.669418\n0.391661\n0.871968\n\n\nstd\n0.471357\n0.471357\n0.415803\n0.415736\n0.372732\n0.372643\n0.372732\n0.372750\n0.415803\n0.415790\n...\n0.499900\n0.499878\n0.168560\n0.135868\n0.103039\n0.378105\n22027.316665\n0.193405\n0.186599\n0.258633\n\n\nmin\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n...\n0.000000\n0.000000\n0.009418\n0.000000\n0.000000\n0.000000\n5000.000000\n0.000000\n0.000000\n0.000000\n\n\n25%\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n...\n0.000000\n0.000000\n0.755845\n0.014729\n0.258311\n2.210000\n39181.000000\n0.560222\n0.235647\n0.884929\n\n\n50%\n1.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n...\n1.000000\n0.000000\n0.872797\n0.036554\n0.305534\n2.440000\n50673.000000\n0.712296\n0.373744\n1.000000\n\n\n75%\n1.000000\n1.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n...\n1.000000\n1.000000\n0.938827\n0.090882\n0.369132\n2.660000\n66005.000000\n0.816798\n0.530036\n1.000000\n\n\nmax\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n...\n1.000000\n1.000000\n1.000000\n0.989622\n0.997544\n5.270000\n200001.000000\n1.000000\n1.000000\n1.000000\n\n\n\n\n8 rows × 48 columns\n\n\n\n\nVariable Definitions\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\ntreatment\nTreatment\n\n\ncontrol\nControl\n\n\nratio\nMatch ratio\n\n\nratio2\n2:1 match ratio\n\n\nratio3\n3:1 match ratio\n\n\nsize\nMatch threshold\n\n\nsize25\n$25,000 match threshold\n\n\nsize50\n$50,000 match threshold\n\n\nsize100\n$100,000 match threshold\n\n\nsizeno\nUnstated match threshold\n\n\nask\nSuggested donation amount\n\n\naskd1\nSuggested donation was highest previous contribution\n\n\naskd2\nSuggested donation was 1.25 x highest previous contribution\n\n\naskd3\nSuggested donation was 1.50 x highest previous contribution\n\n\nask1\nHighest previous contribution (for suggestion)\n\n\nask2\n1.25 x highest previous contribution (for suggestion)\n\n\nask3\n1.50 x highest previous contribution (for suggestion)\n\n\namount\nDollars given\n\n\ngave\nGave anything\n\n\namountchange\nChange in amount given\n\n\nhpa\nHighest previous contribution\n\n\nltmedmra\nSmall prior donor: last gift was less than median $35\n\n\nfreq\nNumber of prior donations\n\n\nyears\nNumber of years since initial donation\n\n\nyear5\nAt least 5 years since initial donation\n\n\nmrm2\nNumber of months since last donation\n\n\ndormant\nAlready donated in 2005\n\n\nfemale\nFemale\n\n\ncouple\nCouple\n\n\nstate50one\nState tag: 1 for one observation of each of 50 states; 0 otherwise\n\n\nnonlit\nNonlitigation\n\n\ncases\nCourt cases from state in 2004-5 in which organization was involved\n\n\nstatecnt\nPercent of sample from state\n\n\nstateresponse\nProportion of sample from the state who gave\n\n\nstateresponset\nProportion of treated sample from the state who gave\n\n\nstateresponsec\nProportion of control sample from the state who gave\n\n\nstateresponsetminc\nstateresponset - stateresponsec\n\n\nperbush\nState vote share for Bush\n\n\nclose25\nState vote share for Bush between 47.5% and 52.5%\n\n\nred0\nRed state\n\n\nblue0\nBlue state\n\n\nredcty\nRed county\n\n\nbluecty\nBlue county\n\n\npwhite\nProportion white within zip code\n\n\npblack\nProportion black within zip code\n\n\npage18_39\nProportion age 18-39 within zip code\n\n\nave_hh_sz\nAverage household size within zip code\n\n\nmedian_hhincome\nMedian household income within zip code\n\n\npowner\nProportion house owner within zip code\n\n\npsch_atlstba\nProportion who finished college within zip code\n\n\npop_propurban\nProportion of population urban within zip code\n\n\n\n\n\n\n\n\n\nBalance Test\nAs an ad hoc test of the randomization mechanism, I provide a series of tests that compare aspects of the treatment and control groups to assess whether they are statistically significantly different from one another.\n\n\nMonths since last Donation Variable\n\n# Months since last donation t-test\n\nmrm2_treat = karlan[karlan['treatment'] == 1]['mrm2']\nmrm2_control = karlan[karlan['treatment'] == 0]['mrm2']\n\nmrm2_t_mean = mrm2_treat.mean()\nmrm2_c_mean = mrm2_control.mean()\n\nmrm2_t_std = mrm2_treat.std()\nmrm2_c_std = mrm2_control.std()\n\nmrm2_t_n = mrm2_treat.count()\nmrm2_c_n = mrm2_control.count()\n\nt_mrm2 = (mrm2_t_mean - mrm2_c_mean) / ((mrm2_t_std**2/mrm2_t_n) + (mrm2_c_std**2/mrm2_c_n))**0.5\n\nprint(t_mrm2)\n\n0.11953155228176905\n\n\n\n# calculate p-value\nfrom scipy import stats\n\np_mrm2 = stats.t.sf(abs(t_mrm2), mrm2_t_n + mrm2_c_n - 2) * 2\nprint(p_mrm2)\n\n# Not statistically significant\n\n0.9048547235822526\n\n\n\n# Linear Regression - mrm2\n\nimport pyrsm as rsm\n\nlr_mrm2 = rsm.regress(\n    data = karlan[['treatment', 'mrm2']],\n    evar = \"treatment\",\n    rvar = \"mrm2\"\n    )\n\nlr_mrm2.summary()\n\nLinear regression (OLS)\nData                 : Not provided\nResponse variable    : mrm2\nExplanatory variables: treatment\nNull hyp.: the effect of x on mrm2 is zero\nAlt. hyp.: the effect of x on mrm2 is not zero\n\n           coefficient  std.error  t.value p.value     \nIntercept       12.998      0.094  138.979  &lt; .001  ***\ntreatment        0.014      0.115    0.119   0.905     \n\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nR-squared: 0.0, Adjusted R-squared: -0.0\nF-statistic: 0.014 df(1, 50080), p.value 0.905\nNr obs: 50,082\n\n\n\n\nCouple Variable\n\ncouple_treat = karlan[karlan['treatment'] == 1]['couple']\ncouple_control = karlan[karlan['treatment'] == 0]['couple']\n\ncouple_t_mean = couple_treat.mean()\ncouple_c_mean = couple_control.mean()\n\ncouple_t_std = couple_treat.std()\ncouple_c_std = couple_control.std()\n\ncouple_t_n = couple_treat.count()\ncouple_c_n = couple_control.count()\n\nt_couple = (couple_t_mean - couple_c_mean) / ((couple_t_std**2/couple_t_n) + (couple_c_std**2/couple_c_n))**0.5\n\nprint(t_couple)\n\n-0.5822577486768489\n\n\n\n# calculate p-value\n\np_couple = stats.t.sf(abs(t_couple), couple_t_n + couple_c_n - 2) * 2\nprint(p_couple)\n\n# Not statistically significant\n\n0.5603957630249871\n\n\n\nlr_couple = rsm.regress(\n    data = karlan[['treatment', 'couple']],\n    evar = \"treatment\",\n    rvar = \"couple\"\n    )\n\nlr_couple.summary()\n\nLinear regression (OLS)\nData                 : Not provided\nResponse variable    : couple\nExplanatory variables: treatment\nNull hyp.: the effect of x on couple is zero\nAlt. hyp.: the effect of x on couple is not zero\n\n           coefficient  std.error  t.value p.value     \nIntercept        0.093      0.002   41.124  &lt; .001  ***\ntreatment       -0.002      0.003   -0.584   0.559     \n\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nR-squared: 0.0, Adjusted R-squared: -0.0\nF-statistic: 0.341 df(1, 48933), p.value 0.559\nNr obs: 48,935\n\n\n\n\nBalance Test Results\nThe intercept coefficients calculated for mrm2 and couple variables are exactly the same as in table 1 of the paper for the control group. Also, when incorporating the coefficients for treatment (treatment = 1), they also equal the mean values in table 1 for the treatment group.\nFor mrm2 (months since last donation), the control group mean was 12.998 and that increases to 13.012 if they were a part of the treatment group. This is not a large disparity and as such did not prove to be statistically significant on a 95% confidence interval.\nFor couple (whether the donor was a couple), the control group mean was 0.093 (interpreted as 9.3% of donors in the control group were couples) and that decreases to 0.091 if they were a part of the treatment group. This is also not a large disparity and as such did not prove to be statistically significant on a 95% confidence interval."
  },
  {
    "objectID": "projects/HW1/hw1_questions.html#experimental-results",
    "href": "projects/HW1/hw1_questions.html#experimental-results",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Experimental Results",
    "text": "Experimental Results\n\nCharitable Contribution Made\nFirst, I analyze whether matched donations lead to an increased response rate of making a donation.\n\n\n# find proportion of treatment group that gave money\n\ntreat_gave = karlan[karlan['treatment'] == 1]['gave'].mean()\ncontrol_gave = karlan[karlan['treatment'] == 0]['gave'].mean()\n\nprint(treat_gave, control_gave)\n\n0.02203856749311295 0.017858212980164198\n\n\n\n# create bar graph for proportion of treatment/control group that gave money\n\nimport matplotlib.pyplot as plt\n\nfig, ax = plt.subplots()\nax.bar(['Treatment', 'Control'], [treat_gave, control_gave])\nax.set_ylabel('Proportion of Group that Gave Money')\nax.set_title('Proportion of Treatment and Control Group that Gave Money')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n# Calculate t-stat for those who donated if they were a part of treatment or control group\n\ngave_treat = karlan[karlan['treatment'] == 1]['gave']\ngave_control = karlan[karlan['treatment'] == 0]['gave']\n\ngave_t_mean = gave_treat.mean()\ngave_c_mean = gave_control.mean()\n\ngave_t_std = gave_treat.std()\ngave_c_std = gave_control.std()\n\ngave_t_n = gave_treat.count()\ngave_c_n = gave_control.count()\n\nt_gave = (gave_t_mean - gave_c_mean) / ((gave_t_std**2/gave_t_n) + (gave_c_std**2/gave_c_n))**0.5\n\nprint(t_gave)\n\n3.2094621908279835\n\n\n\n# Calculate p-value on 95% confidence interval\n\np_gave = stats.t.sf(abs(t_gave), gave_t_n + gave_c_n - 2) * 2\np_gave\n\n# Statistically significant\n\n0.0013306730060655475\n\n\n\n# Run a linear regression which 'gave' is the response variable and 'treatment' is the explanatory variable\n\nlr_gave = rsm.regress(\n    data = karlan[['treatment', 'gave']],\n    evar = \"treatment\",\n    rvar = \"gave\"\n    )\n\nlr_gave.summary()\n\nLinear regression (OLS)\nData                 : Not provided\nResponse variable    : gave\nExplanatory variables: treatment\nNull hyp.: the effect of x on gave is zero\nAlt. hyp.: the effect of x on gave is not zero\n\n           coefficient  std.error  t.value p.value     \nIntercept        0.018      0.001   16.225  &lt; .001  ***\ntreatment        0.004      0.001    3.101   0.002   **\n\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nR-squared: 0.0, Adjusted R-squared: 0.0\nF-statistic: 9.618 df(1, 50081), p.value 0.002\nNr obs: 50,083\n\n\nThe outcome of our t-test and linear regression were both similar in that it was found at a 95% confidence interval that the treatment did cause an increase in the number of donations. The linear regression did not explain any variance in the data, however that is not as important because we are using what should be a logistic regression (binary outcome of 1 or 0 if they donated or not) with a linear regression model.\n\n\n# Run probit regression on gave and treatment variables\n\nimport statsmodels.formula.api as smf\n\nmod = smf.probit('gave ~ treatment', data=karlan)\nres = mod.fit()\nres.summary()\n\nOptimization terminated successfully.\n         Current function value: 0.100443\n         Iterations 7\n\n\n\nProbit Regression Results\n\n\nDep. Variable:\ngave\nNo. Observations:\n50083\n\n\nModel:\nProbit\nDf Residuals:\n50081\n\n\nMethod:\nMLE\nDf Model:\n1\n\n\nDate:\nMon, 15 Apr 2024\nPseudo R-squ.:\n0.0009783\n\n\nTime:\n20:30:57\nLog-Likelihood:\n-5030.5\n\n\nconverged:\nTrue\nLL-Null:\n-5035.4\n\n\nCovariance Type:\nnonrobust\nLLR p-value:\n0.001696\n\n\n\n\n\n\n\ncoef\nstd err\nz\nP&gt;|z|\n[0.025\n0.975]\n\n\nIntercept\n-2.1001\n0.023\n-90.073\n0.000\n-2.146\n-2.054\n\n\ntreatment\n0.0868\n0.028\n3.113\n0.002\n0.032\n0.141\n\n\n\n\n\n\n\nDifferences between Match Rates\nNext, I assess the effectiveness of different sizes of matched donations on the response rate.\n\nRatio of 1:1 vs Ratio of 2:1\n\n# Calculate t-stat for Ratio 1:1 vs Ratio 2:1\n\ngave_ratio1 = karlan[karlan['ratio'] == 1]['gave']\ngave_ratio2 = karlan[karlan['ratio'] == 2]['gave']\n\ngave_1_mean = gave_ratio1.mean()\ngave_2_mean = gave_ratio2.mean()\n\ngave_1_std = gave_ratio1.std()\ngave_2_std = gave_ratio2.std()\n\ngave_1_n = gave_ratio1.count()\ngave_2_n = gave_ratio2.count()\n\nt_gave_ratio = (gave_2_mean - gave_1_mean) / ((gave_2_std**2/gave_2_n) + (gave_1_std**2/gave_1_n))**0.5\n\nprint(t_gave_ratio)\n\n0.965048975142932\n\n\n\np_gave_ratio = stats.t.sf(abs(t_gave_ratio), gave_2_n + gave_1_n - 2) * 2\np_gave_ratio\n\n# Not statistically significant\n\n0.3345307635444439\n\n\n\n\nRatio of 2:1 vs Ratio of 3:1\n\ngave_ratio3 = karlan[karlan['ratio'] == 3]['gave']\ngave_ratio2 = karlan[karlan['ratio'] == 2]['gave']\n\ngave_3_mean = gave_ratio3.mean()\ngave_2_mean = gave_ratio2.mean()\n\ngave_3_std = gave_ratio3.std()\ngave_2_std = gave_ratio2.std()\n\ngave_3_n = gave_ratio3.count()\ngave_2_n = gave_ratio2.count()\n\nt_gave_ratio_23 = (gave_2_mean - gave_3_mean) / ((gave_2_std**2/gave_2_n) + (gave_3_std**2/gave_3_n))**0.5\n\nprint(t_gave_ratio_23)\n\n-0.05011581369764474\n\n\n\np_gave_ratio_23 = stats.t.sf(abs(t_gave_ratio_23), gave_2_n + gave_3_n - 2) * 2\np_gave_ratio_23\n\n# Not statistically significant\n\n0.9600305476910405\n\n\nThe above calculations matches what Karlan suggests in his paper, that increasing the match ratio does not increase the probability of making a donation. The above calculations show a 1:1 match ratio when compared to a 2:1 match ratio, and a 2:1 match ratio when compared to a 3:1 match ratio. We did a two-sided t-test, which has a null hypothesis stating that 2:1 is not the same as 1:1, and 3:1 is not the same as 2:1.\n\n\n#create a variable ratio1 where if ratio column equals 1, then ratio1 equlas 1, else 0\n\nkarlan['ratio1'] = karlan['ratio'].apply(lambda x: 1 if x == 1 else 0)\n\n\n# Linear regression for match ratios and treatment\n\nlr_ratio = rsm.regress(\n    data = karlan[['gave', 'ratio1', 'ratio2', 'ratio3']],\n    evar = ['ratio1', 'ratio2', 'ratio3'],\n    rvar = 'gave'\n    )  \n\nlr_ratio.summary()\n\nLinear regression (OLS)\nData                 : Not provided\nResponse variable    : gave\nExplanatory variables: ratio1, ratio2, ratio3\nNull hyp.: the effect of x on gave is zero\nAlt. hyp.: the effect of x on gave is not zero\n\n           coefficient  std.error  t.value p.value     \nIntercept        0.018      0.001   16.225  &lt; .001  ***\nratio1           0.003      0.002    1.661   0.097    .\nratio2           0.005      0.002    2.744   0.006   **\nratio3           0.005      0.002    2.802   0.005   **\n\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nR-squared: 0.0, Adjusted R-squared: 0.0\nF-statistic: 3.665 df(3, 50079), p.value 0.012\nNr obs: 50,083\n\n\nThe results of linear regression model on match ratios of 1:1, 2:1, and 3:1 show that all but a ratio of 1:1 are statistically significant. The intercept coefficient (meaning when there is no match) is 0.018, and then for ratio 1:1 it has a coefficient of 0.003, which added to 0.018 is 0.021. For ratio 2:1 and 3:1, they both have coefficients of 0.005, creating a response rate of 0.023 for both. All these response rates match table 2A in the paper.\nThe precision of these estimates also match what is in the paper for standard error. Each ratio has a standard error of 0.002, meaning that ratio 1:1 could have a varying effect on response rate between 0.001 and 0.005, and ratios 2:1 and 3:1 could vary between 0.003 and 0.007. All however remain positive when incorporating standard error, meaning that the match ratio does have a positive effect on response rate when comparing to control.\n\n\n\n\n\nSize of Charitable Contribution\nIn this subsection, I analyze the effect of the size of matched donation on the size of the charitable contribution.\n\n\n# Linear Regression on Treatment to Predict donation amount\nlr_d_amount = rsm.regress(\n    data = karlan[['amount', 'treatment']],\n    evar = 'treatment',\n    rvar = 'amount'\n    )  \n\nlr_d_amount.summary()\n\nLinear regression (OLS)\nData                 : Not provided\nResponse variable    : amount\nExplanatory variables: treatment\nNull hyp.: the effect of x on amount is zero\nAlt. hyp.: the effect of x on amount is not zero\n\n           coefficient  std.error  t.value p.value     \nIntercept        0.813      0.067   12.063  &lt; .001  ***\ntreatment        0.154      0.083    1.861   0.063    .\n\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nR-squared: 0.0, Adjusted R-squared: 0.0\nF-statistic: 3.461 df(1, 50081), p.value 0.063\nNr obs: 50,083\n\n\nFrom our linear regression model to calculate dollars donated amount from whether the donor was in the treatment or control group, we can conclude that the control group donates $0.813 and being a part of the treatment group increases that amount by $0.154, bringing the estimated dollars donated amount to $0.967. Per a 95% confidence interval, the model is not statistically significant, however it is very close to being so.\n\n# Linear Regression on treatment to predict donation amounts above 0\n\nlr_d_amount_above0 = rsm.regress(\n    data = karlan[karlan['amount'] &gt; 0][['amount', 'treatment']],\n    evar = 'treatment',\n    rvar = 'amount'\n    )  \n\nlr_d_amount_above0.summary()\n\nLinear regression (OLS)\nData                 : Not provided\nResponse variable    : amount\nExplanatory variables: treatment\nNull hyp.: the effect of x on amount is zero\nAlt. hyp.: the effect of x on amount is not zero\n\n           coefficient  std.error  t.value p.value     \nIntercept       45.540      2.423   18.792  &lt; .001  ***\ntreatment       -1.668      2.872   -0.581   0.561     \n\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nR-squared: 0.0, Adjusted R-squared: -0.001\nF-statistic: 0.337 df(1, 1032), p.value 0.561\nNr obs: 1,034\n\n\nAfter filtering the data to only include those who donated, the linear regression model changed significantly. The intercept coefficient is now $45.54, and the treatment coefficient is now negative $1.67. Now, the treatment group has a negative affect on amount donated, meaning that the control group donates $45.54 and the treatment group donates $43.87.\nHowever, the model is not statistically significant at 95% confidence interval.\n\n\n\n# create a histogram for all the donation amounts above 0 for the control group\n\nfig, ax = plt.subplots()\nax.hist(karlan[(karlan['amount'] &gt; 0) & (karlan['treatment'] == 0)]['amount'], bins=25)\nax.axvline(43.87, color='red')\nax.text(43.87 + 1, ax.get_ylim()[1] * 0.9, f'{43.87}', color='red')\nax.set_ylabel('Frequency')\nax.set_xlabel('Donation Amount')\nax.set_title('Frequency of Donation Amounts for Control Group')\nplt.show()\n\n\n\n\n\n\n\n\n\n# create a histogram for all the donation amounts above 0 for the treatment group\n\nfig, ax = plt.subplots()\nax.hist(karlan[(karlan['amount'] &gt; 0) & (karlan['treatment'] == 1)]['amount'], bins=25)\nax.axvline(45.54, color='red')\nax.text(45.54 + 1, ax.get_ylim()[1] * 0.9, f'{45.54}', color='red')\nax.set_ylabel('Frequency')\nax.set_xlabel('Donation Amount')\nax.set_title('Frequency of Donation Amounts for Treatment Group')\nplt.show()"
  },
  {
    "objectID": "projects/HW1/hw1_questions.html#simulation-experiment",
    "href": "projects/HW1/hw1_questions.html#simulation-experiment",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Simulation Experiment",
    "text": "Simulation Experiment\nAs a reminder of how the t-statistic “works,” in this section I use simulation to demonstrate the Law of Large Numbers and the Central Limit Theorem.\nSuppose the true distribution of respondents who do not get a charitable donation match is Bernoulli with probability p=0.018 that a donation is made.\nFurther suppose that the true distribution of respondents who do get a charitable donation match of any size is Bernoulli with probability p=0.022 that a donation is made.\n\nLaw of Large Numbers\n\n\n# use a bernoulli distribution to simulate 10,000 trials for control and treatment groups using their mean percentage donated as the probabilities respectively\n\nimport numpy as np\nfrom scipy.stats import bernoulli\n\ncontrol = karlan[karlan['treatment'] == 0]['gave'].mean()\ntreatment = karlan[karlan['treatment'] == 1]['gave'].mean()\n\ncontrol_sim = bernoulli.rvs(p = control, size = 10000)\ntreatment_sim = bernoulli.rvs(p = treatment, size = 10000)\n\n# calculate cumulative average of the differences for the first 10,000 draws\n\ncum_avg = np.cumsum(treatment_sim - control_sim) / np.arange(1, 10001)\n\ncum_avg\n\narray([0.        , 0.        , 0.        , ..., 0.00830166, 0.00830083,\n       0.0083    ])\n\n\n\n# plot the cumulative average of the differences with a line plot\n\nfig, ax = plt.subplots()\nax.plot(cum_avg)\nax.axhline(treatment - control, color='red')\nax.set_ylabel('Cumulative Average of Differences')\nax.set_xlabel('Number of Draws')\nax.set_title('Cumulative Average of Differences in Proportion of Giving Money')\nplt.show()\n\n\n\n\n\n\n\n\nThe above graph shows a simulation of the differences between simulated probabilities of an individual actually donating in the treatment group and an individual donating in the control group. The control group was given a probability of 0.018 and treatment group was given a probability of 0.022, which were both calculating from the given dataset.\nWe simulated this for the control and treatment group 10,000 times to generate the graph. The red horizontal line on the graph shows the calculated difference between the treatment and control probabilities from the dataset (0.004).\nAs shown, the cumulative difference between the treatment and control group varies greatly initially, then slowly starts to vary less and less around the average difference of 0.004. With more trials, it is expected there would be even less variability and the cumulative difference would be extremely close to 0.004.\n\n\nCentral Limit Theorem\n\n\nSimulating 50 trials\n\nn_50_c = np.random.binomial(50, 0.018, 1000)\n\nn_50_c = n_50_c / 50\n\nn_50_t = np.random.binomial(50, 0.022, 1000)\n\nn_50_t = n_50_t / 50\n\nn_50 = n_50_t - n_50_c\n\n# make a histogram of n_50 so there is no spacing between the bars\n\nfig, ax = plt.subplots()\nax.hist(n_50, bins=10, rwidth=1)\nax.axvline(n_50.mean(), color='red')\nax.text(n_50.mean(), ax.get_ylim()[1]*0.9, f'{np.round(n_50.mean(),4)}', color='red', verticalalignment='center', horizontalalignment='center')\nax.set_ylabel('Frequency')\nax.set_xlabel('Difference in Proportion of Donors (Treatment - Control) in 50 Draws')\nax.set_title('Frequency of Proportion of Control Group that Gave Money in 50 Draws')\nplt.show()\n\n\n\n\n\n\n\n\n\n\nSimulating 200 trials\n\nn_200_c = np.random.binomial(200, 0.018, 1000)/200\n\nn_200_t = np.random.binomial(200, 0.022, 1000)/200\n\n\nn_200 = n_200_t - n_200_c\n\nn_200_mean = n_200.mean()\n\nfig, ax = plt.subplots()\nax.hist(n_200, bins=10, rwidth=1)\nax.axvline(n_200_mean, color='red')\nax.text(n_200_mean, ax.get_ylim()[1]*0.9, f'{np.round(n_200_mean,4)}', color='red', verticalalignment='center', horizontalalignment='center')\nax.set_ylabel('Frequency')\nax.set_xlabel('Difference in Proportion of Donors (Treatment - Control) in 200 Draws')\nax.set_title('Frequency of Proportion of Control Group that Gave Money in 200 Draws')\nplt.show()\n\n\n\n\n\n\n\n\n\n\nSimulating 500 trials\n\nn_500_c = np.random.binomial(500, 0.018, 1000)/500\n\nn_500_t = np.random.binomial(500, 0.022, 1000)/500\n\n\nn_500 = n_500_t - n_500_c\n\nn_500_mean = n_500.mean()\n\nfig, ax = plt.subplots()\nax.hist(n_500, bins=10, rwidth=1)\nax.axvline(n_500_mean, color='red')\nax.text(n_500_mean, ax.get_ylim()[1]*0.9, f'{np.round(n_500_mean,4)}', color='red', verticalalignment='center', horizontalalignment='center')\nax.set_ylabel('Frequency')\nax.set_xlabel('Difference in Proportion of Donors (Treatment - Control) in 500 Draws')\nax.set_title('Frequency of Proportion of Control Group that Gave Money in 500 Draws')\nplt.show()\n\n\n\n\n\n\n\n\n\n\nSimulating 1000 trials\n\nn_1000_c = np.random.binomial(1000, 0.018, 1000)/1000\n\nn_1000_t = np.random.binomial(1000, 0.022, 1000)/1000\n\nn_1000 = n_1000_t - n_1000_c\n\nn_1000_mean = n_1000.mean()\n\nfig, ax = plt.subplots()\nax.hist(n_1000, bins=10, rwidth=1)\nax.axvline(n_1000_mean, color='red')\nax.text(n_1000_mean, ax.get_ylim()[1]*0.9, f'{np.round(n_1000_mean,4)}', color='red', verticalalignment='center', horizontalalignment='right')\nax.set_ylabel('Frequency')\nax.set_xlabel('Difference in Proportion of Donors (Treatment - Control) in 500 Draws')\nax.set_title('Frequency of Proportion of Control Group that Gave Money in 500 Draws')\nplt.show()\n\n\n\n\n\n\n\n\nFrom the above histograms, you can see from the red vertical line that the average difference decreases as we increase the trial size of the simulation and starts to get closer to 0.004, which is the calculated difference from the dataset between the percentage that donated from the treatment group versus the control group.\nAlthough the 0 mark on the x-axis varies because of graph sizing, you can see that the outer limits of the distribution gets smaller as we increase the number of trials. This follows the central limit theorem, since as we increase the number of trials in each simulation, we will get closer and closer to the true mean or percentage. The variation or wide distribution we see in the 50 trial simulation is no longer there in the 1000 trial distribution.\nOverall, the more trials we have in our simulation or experiment, the closer we will get to the mean."
  },
  {
    "objectID": "projects/HW1/HW1.html",
    "href": "projects/HW1/HW1.html",
    "title": "Nick_Shuckerow",
    "section": "",
    "text": "import pandas as pd\nfrom scipy import stats\nkarlan = pd.read_stata(\"karlan_list_2007.dta\")\nkarlan\n\n\n\n\n\n\n\n\ntreatment\ncontrol\nratio\nratio2\nratio3\nsize\nsize25\nsize50\nsize100\nsizeno\n...\nredcty\nbluecty\npwhite\npblack\npage18_39\nave_hh_sz\nmedian_hhincome\npowner\npsch_atlstba\npop_propurban\n\n\n\n\n0\n0\n1\nControl\n0\n0\nControl\n0\n0\n0\n0\n...\n0.0\n1.0\n0.446493\n0.527769\n0.317591\n2.10\n28517.0\n0.499807\n0.324528\n1.000000\n\n\n1\n0\n1\nControl\n0\n0\nControl\n0\n0\n0\n0\n...\n1.0\n0.0\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n2\n1\n0\n1\n0\n0\n$100,000\n0\n0\n1\n0\n...\n0.0\n1.0\n0.935706\n0.011948\n0.276128\n2.48\n51175.0\n0.721941\n0.192668\n1.000000\n\n\n3\n1\n0\n1\n0\n0\nUnstated\n0\n0\n0\n1\n...\n1.0\n0.0\n0.888331\n0.010760\n0.279412\n2.65\n79269.0\n0.920431\n0.412142\n1.000000\n\n\n4\n1\n0\n1\n0\n0\n$50,000\n0\n1\n0\n0\n...\n0.0\n1.0\n0.759014\n0.127421\n0.442389\n1.85\n40908.0\n0.416072\n0.439965\n1.000000\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n50078\n1\n0\n1\n0\n0\n$25,000\n1\n0\n0\n0\n...\n0.0\n1.0\n0.872797\n0.089959\n0.257265\n2.13\n45047.0\n0.771316\n0.263744\n1.000000\n\n\n50079\n0\n1\nControl\n0\n0\nControl\n0\n0\n0\n0\n...\n0.0\n1.0\n0.688262\n0.108889\n0.288792\n2.67\n74655.0\n0.741931\n0.586466\n1.000000\n\n\n50080\n0\n1\nControl\n0\n0\nControl\n0\n0\n0\n0\n...\n1.0\n0.0\n0.900000\n0.021311\n0.178689\n2.36\n26667.0\n0.778689\n0.107930\n0.000000\n\n\n50081\n1\n0\n3\n0\n1\nUnstated\n0\n0\n0\n1\n...\n1.0\n0.0\n0.917206\n0.008257\n0.225619\n2.57\n39530.0\n0.733988\n0.184768\n0.634903\n\n\n50082\n1\n0\n3\n0\n1\n$25,000\n1\n0\n0\n0\n...\n0.0\n1.0\n0.530023\n0.074112\n0.340698\n3.70\n48744.0\n0.717843\n0.127941\n0.994181\n\n\n\n\n50083 rows × 51 columns\n\n\n\n\nkarlan.describe()\n\n\n\n\n\n\n\n\ntreatment\ncontrol\nratio2\nratio3\nsize25\nsize50\nsize100\nsizeno\naskd1\naskd2\n...\nredcty\nbluecty\npwhite\npblack\npage18_39\nave_hh_sz\nmedian_hhincome\npowner\npsch_atlstba\npop_propurban\n\n\n\n\ncount\n50083.000000\n50083.000000\n50083.000000\n50083.000000\n50083.000000\n50083.000000\n50083.000000\n50083.000000\n50083.000000\n50083.000000\n...\n49978.000000\n49978.000000\n48217.000000\n48047.000000\n48217.000000\n48221.000000\n48209.000000\n48214.000000\n48215.000000\n48217.000000\n\n\nmean\n0.666813\n0.333187\n0.222311\n0.222211\n0.166723\n0.166623\n0.166723\n0.166743\n0.222311\n0.222291\n...\n0.510245\n0.488715\n0.819599\n0.086710\n0.321694\n2.429012\n54815.700533\n0.669418\n0.391661\n0.871968\n\n\nstd\n0.471357\n0.471357\n0.415803\n0.415736\n0.372732\n0.372643\n0.372732\n0.372750\n0.415803\n0.415790\n...\n0.499900\n0.499878\n0.168560\n0.135868\n0.103039\n0.378105\n22027.316665\n0.193405\n0.186599\n0.258633\n\n\nmin\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n...\n0.000000\n0.000000\n0.009418\n0.000000\n0.000000\n0.000000\n5000.000000\n0.000000\n0.000000\n0.000000\n\n\n25%\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n...\n0.000000\n0.000000\n0.755845\n0.014729\n0.258311\n2.210000\n39181.000000\n0.560222\n0.235647\n0.884929\n\n\n50%\n1.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n...\n1.000000\n0.000000\n0.872797\n0.036554\n0.305534\n2.440000\n50673.000000\n0.712296\n0.373744\n1.000000\n\n\n75%\n1.000000\n1.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n...\n1.000000\n1.000000\n0.938827\n0.090882\n0.369132\n2.660000\n66005.000000\n0.816798\n0.530036\n1.000000\n\n\nmax\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n...\n1.000000\n1.000000\n1.000000\n0.989622\n0.997544\n5.270000\n200001.000000\n1.000000\n1.000000\n1.000000\n\n\n\n\n8 rows × 48 columns\n\n\n\n\nkarlan.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 50083 entries, 0 to 50082\nData columns (total 51 columns):\n #   Column              Non-Null Count  Dtype   \n---  ------              --------------  -----   \n 0   treatment           50083 non-null  int8    \n 1   control             50083 non-null  int8    \n 2   ratio               50083 non-null  category\n 3   ratio2              50083 non-null  int8    \n 4   ratio3              50083 non-null  int8    \n 5   size                50083 non-null  category\n 6   size25              50083 non-null  int8    \n 7   size50              50083 non-null  int8    \n 8   size100             50083 non-null  int8    \n 9   sizeno              50083 non-null  int8    \n 10  ask                 50083 non-null  category\n 11  askd1               50083 non-null  int8    \n 12  askd2               50083 non-null  int8    \n 13  askd3               50083 non-null  int8    \n 14  ask1                50083 non-null  int16   \n 15  ask2                50083 non-null  int16   \n 16  ask3                50083 non-null  int16   \n 17  amount              50083 non-null  float32 \n 18  gave                50083 non-null  int8    \n 19  amountchange        50083 non-null  float32 \n 20  hpa                 50083 non-null  float32 \n 21  ltmedmra            50083 non-null  int8    \n 22  freq                50083 non-null  int16   \n 23  years               50082 non-null  float64 \n 24  year5               50083 non-null  int8    \n 25  mrm2                50082 non-null  float64 \n 26  dormant             50083 non-null  int8    \n 27  female              48972 non-null  float64 \n 28  couple              48935 non-null  float64 \n 29  state50one          50083 non-null  int8    \n 30  nonlit              49631 non-null  float64 \n 31  cases               49631 non-null  float64 \n 32  statecnt            50083 non-null  float32 \n 33  stateresponse       50083 non-null  float32 \n 34  stateresponset      50083 non-null  float32 \n 35  stateresponsec      50080 non-null  float32 \n 36  stateresponsetminc  50080 non-null  float32 \n 37  perbush             50048 non-null  float32 \n 38  close25             50048 non-null  float64 \n 39  red0                50048 non-null  float64 \n 40  blue0               50048 non-null  float64 \n 41  redcty              49978 non-null  float64 \n 42  bluecty             49978 non-null  float64 \n 43  pwhite              48217 non-null  float32 \n 44  pblack              48047 non-null  float32 \n 45  page18_39           48217 non-null  float32 \n 46  ave_hh_sz           48221 non-null  float32 \n 47  median_hhincome     48209 non-null  float64 \n 48  powner              48214 non-null  float32 \n 49  psch_atlstba        48215 non-null  float32 \n 50  pop_propurban       48217 non-null  float32 \ndtypes: category(3), float32(16), float64(12), int16(4), int8(16)\nmemory usage: 8.9 MB\n\n\n\n# write karlan to csv\n\nkarlan.to_csv(\"karlan.csv\")\n\n\n# Months since last donation t-test\n\nmrm2_treat = karlan[karlan['treatment'] == 1]['mrm2']\nmrm2_control = karlan[karlan['treatment'] == 0]['mrm2']\n\nmrm2_t_mean = mrm2_treat.mean()\nmrm2_c_mean = mrm2_control.mean()\n\nmrm2_t_std = mrm2_treat.std()\nmrm2_c_std = mrm2_control.std()\n\nmrm2_t_n = mrm2_treat.count()\nmrm2_c_n = mrm2_control.count()\n\nt_mrm2 = (mrm2_t_mean - mrm2_c_mean) / ((mrm2_t_std**2/mrm2_t_n) + (mrm2_c_std**2/mrm2_c_n))**0.5\n\nprint(t_mrm2)\n\n\n0.11953155228176905\n\n\n\n# calculate p-value\n\np_mrm2 = stats.t.sf(abs(t_mrm2), mrm2_t_n + mrm2_c_n - 2) * 2\np_mrm2\n\n# Not statistically significant\n\n0.9048547235822526\n\n\n\ncouple_treat = karlan[karlan['treatment'] == 1]['couple']\ncouple_control = karlan[karlan['treatment'] == 0]['couple']\n\ncouple_t_mean = couple_treat.mean()\ncouple_c_mean = couple_control.mean()\n\ncouple_t_std = couple_treat.std()\ncouple_c_std = couple_control.std()\n\ncouple_t_n = couple_treat.count()\ncouple_c_n = couple_control.count()\n\nt_couple = (couple_t_mean - couple_c_mean) / ((couple_t_std**2/couple_t_n) + (couple_c_std**2/couple_c_n))**0.5\n\nt_couple\n\n-0.5822577486768489\n\n\n\n# calculate p-value\n\np_couple = stats.t.sf(abs(t_couple), couple_t_n + couple_c_n - 2) * 2\np_couple\n\n# Not statistically significant\n\n0.5603957630249871\n\n\n\n# Linear Regression - mrm2\n\nimport pyrsm as rsm\n\nlr_mrm2 = rsm.regress(\n    data = karlan[['treatment', 'mrm2']],\n    evar = \"treatment\",\n    rvar = \"mrm2\"\n    )\n\nlr_mrm2.summary()\n\n\n\n\nLinear regression (OLS)\nData                 : Not provided\nResponse variable    : mrm2\nExplanatory variables: treatment\nNull hyp.: the effect of x on mrm2 is zero\nAlt. hyp.: the effect of x on mrm2 is not zero\n\n           coefficient  std.error  t.value p.value     \nIntercept       12.998      0.094  138.979  &lt; .001  ***\ntreatment        0.014      0.115    0.119   0.905     \n\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nR-squared: 0.0, Adjusted R-squared: -0.0\nF-statistic: 0.014 df(1, 50080), p.value 0.905\nNr obs: 50,082\n\n\n\nlr_couple = rsm.regress(\n    data = karlan[['treatment', 'couple']],\n    evar = \"treatment\",\n    rvar = \"couple\"\n    )\n\nlr_couple.summary()\n\nLinear regression (OLS)\nData                 : Not provided\nResponse variable    : couple\nExplanatory variables: treatment\nNull hyp.: the effect of x on couple is zero\nAlt. hyp.: the effect of x on couple is not zero\n\n           coefficient  std.error  t.value p.value     \nIntercept        0.093      0.002   41.124  &lt; .001  ***\ntreatment       -0.002      0.003   -0.584   0.559     \n\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nR-squared: 0.0, Adjusted R-squared: -0.0\nF-statistic: 0.341 df(1, 48933), p.value 0.559\nNr obs: 48,935\n\n\nThe intercept coefficients calculated for mrm2 and couple variables are exactly the same as in table 1 of the paper for the control group. Also, when incorporating the coefficients for treatment (treatment = 1), they also equal the mean values in table 1 for the treatment group.\nFor mrm2 (months since last donation), the control group mean was 12.998 and that increases to 13.012 if they were a part of the treatment group. This is not a large disparity and as such did not prove to be statistically significant on a 95% confidence interval.\nFor couple (whether the donor was a couple), the control group mean was 0.093 (interpreted as 9.3% of donors in the control group were couples) and that decreases to 0.091 if they were a part of the treatment group. This is also not a large disparity and as such did not prove to be statistically significant on a 95% confidence interval.\n\n# find proportion of treatment group that gave money\n\ntreat_gave = karlan[karlan['treatment'] == 1]['gave'].mean()\ncontrol_gave = karlan[karlan['treatment'] == 0]['gave'].mean()\n\ntreat_gave, control_gave\n\n(0.02203856749311295, 0.017858212980164198)\n\n\n\n# create bar graph for proportion of treatment/control group that gave money\n\nimport matplotlib.pyplot as plt\n\nfig, ax = plt.subplots()\nax.bar(['Treatment', 'Control'], [treat_gave, control_gave])\nax.set_ylabel('Proportion of Group that Gave Money')\nax.set_title('Proportion of Treatment and Control Group that Gave Money')\nplt.show()\n\n\n\n\n\n\n\n\n\ngave_treat = karlan[karlan['treatment'] == 1]['gave']\ngave_control = karlan[karlan['treatment'] == 0]['gave']\n\ngave_t_mean = gave_treat.mean()\ngave_c_mean = gave_control.mean()\n\ngave_t_std = gave_treat.std()\ngave_c_std = gave_control.std()\n\ngave_t_n = gave_treat.count()\ngave_c_n = gave_control.count()\n\nt_gave = (gave_t_mean - gave_c_mean) / ((gave_t_std**2/gave_t_n) + (gave_c_std**2/gave_c_n))**0.5\n\nt_gave\n\n3.2094621908279835\n\n\n\np_gave = stats.t.sf(abs(t_gave), gave_t_n + gave_c_n - 2) * 2\np_gave\n\n# Statistically significant\n\n0.0013306730060655475\n\n\n\nlr_gave = rsm.regress(\n    data = karlan[['treatment', 'gave']],\n    evar = \"treatment\",\n    rvar = \"gave\"\n    )\n\nlr_gave.summary()\n\nLinear regression (OLS)\nData                 : Not provided\nResponse variable    : gave\nExplanatory variables: treatment\nNull hyp.: the effect of x on gave is zero\nAlt. hyp.: the effect of x on gave is not zero\n\n           coefficient  std.error  t.value p.value     \nIntercept        0.018      0.001   16.225  &lt; .001  ***\ntreatment        0.004      0.001    3.101   0.002   **\n\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nR-squared: 0.0, Adjusted R-squared: 0.0\nF-statistic: 9.618 df(1, 50081), p.value 0.002\nNr obs: 50,083\n\n\nThe outcome of our t-test and linear regression were both similar in that it was found at a 95% confidence interval that the treatment did cause an increase in the number of donations. The linear regression did not explain any variance in the data, however that is not as important because we are using what should be a logistic regression (binary outcome of 1 or 0 for if they donated or not) with a linear regression model.\n\nimport statsmodels.formula.api as smf\n\n\nmod = smf.probit('gave ~ treatment', data=karlan)\n\n\nres = mod.fit()\n\nOptimization terminated successfully.\n         Current function value: 0.100443\n         Iterations 7\n\n\n\nres.summary()\n\n\nProbit Regression Results\n\n\nDep. Variable:\ngave\nNo. Observations:\n50083\n\n\nModel:\nProbit\nDf Residuals:\n50081\n\n\nMethod:\nMLE\nDf Model:\n1\n\n\nDate:\nSun, 14 Apr 2024\nPseudo R-squ.:\n0.0009783\n\n\nTime:\n21:43:24\nLog-Likelihood:\n-5030.5\n\n\nconverged:\nTrue\nLL-Null:\n-5035.4\n\n\nCovariance Type:\nnonrobust\nLLR p-value:\n0.001696\n\n\n\n\n\n\n\ncoef\nstd err\nz\nP&gt;|z|\n[0.025\n0.975]\n\n\nIntercept\n-2.1001\n0.023\n-90.073\n0.000\n-2.146\n-2.054\n\n\ntreatment\n0.0868\n0.028\n3.113\n0.002\n0.032\n0.141\n\n\n\n\n\n\ngave_ratio1 = karlan[karlan['ratio'] == 1]['gave']\ngave_ratio2 = karlan[karlan['ratio'] == 2]['gave']\n\ngave_1_mean = gave_ratio1.mean()\ngave_2_mean = gave_ratio2.mean()\n\ngave_1_std = gave_ratio1.std()\ngave_2_std = gave_ratio2.std()\n\ngave_1_n = gave_ratio1.count()\ngave_2_n = gave_ratio2.count()\n\nt_gave_ratio = (gave_2_mean - gave_1_mean) / ((gave_2_std**2/gave_2_n) + (gave_1_std**2/gave_1_n))**0.5\n\nt_gave_ratio\n\n0.965048975142932\n\n\n\ngave_1_mean, gave_2_mean\n\n(0.020749124225276205, 0.0226333752469912)\n\n\n\np_gave_ratio = stats.t.sf(abs(t_gave_ratio), gave_2_n + gave_1_n - 2) * 2\np_gave_ratio\n\n# Not statistically significant\n\n0.3345307635444439\n\n\n\ngave_ratio3 = karlan[karlan['ratio'] == 3]['gave']\ngave_ratio2 = karlan[karlan['ratio'] == 2]['gave']\n\ngave_3_mean = gave_ratio3.mean()\ngave_2_mean = gave_ratio2.mean()\n\ngave_3_std = gave_ratio3.std()\ngave_2_std = gave_ratio2.std()\n\ngave_3_n = gave_ratio3.count()\ngave_2_n = gave_ratio2.count()\n\nt_gave_ratio_23 = (gave_2_mean - gave_3_mean) / ((gave_2_std**2/gave_2_n) + (gave_3_std**2/gave_3_n))**0.5\n\nt_gave_ratio_23\n\n-0.05011581369764474\n\n\n\np_gave_ratio_23 = stats.t.sf(abs(t_gave_ratio_23), gave_2_n + gave_3_n - 2) * 2\np_gave_ratio_23\n\n# Not statistically significant\n\n0.9600305476910405\n\n\nThe above calculations matches what Karlan suggests in his paper, that increasing the match ratio does not increase the probability of making a donation. The above calculations show a 1:1 match ratio when compared to a 2:1 match ratio, and a 2:1 match ratio when compared to a 3:1 match ratio. We did a two-sided t-test, which has a null hypothesis stating that 2:1 is not the same as 1:1, and 3:1 is not the same as 2:1.\n\n#create a variable ratio1 where if ratio column equals 1, then ratio1 equlas 1, else 0\n\nkarlan['ratio1'] = karlan['ratio'].apply(lambda x: 1 if x == 1 else 0)\n\n\n# Linear regression for match ratios and treatment\n\nlr_ratio = rsm.regress(\n    data = karlan[['gave', 'ratio1', 'ratio2', 'ratio3']],\n    evar = ['ratio1', 'ratio2', 'ratio3'],\n    rvar = 'gave'\n    )  \n\nlr_ratio.summary()\n\nLinear regression (OLS)\nData                 : Not provided\nResponse variable    : gave\nExplanatory variables: ratio1, ratio2, ratio3\nNull hyp.: the effect of x on gave is zero\nAlt. hyp.: the effect of x on gave is not zero\n\n           coefficient  std.error  t.value p.value     \nIntercept        0.018      0.001   16.225  &lt; .001  ***\nratio1           0.003      0.002    1.661   0.097    .\nratio2           0.005      0.002    2.744   0.006   **\nratio3           0.005      0.002    2.802   0.005   **\n\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nR-squared: 0.0, Adjusted R-squared: 0.0\nF-statistic: 3.665 df(3, 50079), p.value 0.012\nNr obs: 50,083\n\n\nThe results of linear regression model on match ratios of 1:1, 2:1, and 3:1 show that all but a ratio of 1:1 are statistically significant. The intercept coefficient (meaning when there is no match) is 0.018, and then for ratio 1:1 it has a coefficient of 0.003, which added to 0.018 is 0.021. For ratio 2:1 and 3:1, they both have coefficients of 0.005, creating a response rate of 0.023 for both. All these response rates match table 2A in the paper.\nThe precision of these estimates also match what is in the paper for standard error. Each ratio has a standard error of 0.002, meaning that ratio 1:1 could have a varying effect on response rate between 0.001 and 0.005, and ratios 2:1 and 3:1 could vary between 0.003 and 0.007. All however remain positive when incorporating standard error, meaning that the match ratio does have a positive effect on response rate when comparing to control.\nThe difference in responsive rate between 1:1 and 2:1 match ratios is 0.002, and between 2:1 and 3:1 match ratios is 0. From this, we can conclude that increasing makes a very small difference from 1:1 to 2:1 but no difference between 2:1 and 3:1. Overall, increasing the match donation above 1:1 does not make a large difference in response rate.\n\nlr_d_amount = rsm.regress(\n    data = karlan[['amount', 'treatment']],\n    evar = 'treatment',\n    rvar = 'amount'\n    )  \n\nlr_d_amount.summary()\n\nLinear regression (OLS)\nData                 : Not provided\nResponse variable    : amount\nExplanatory variables: treatment\nNull hyp.: the effect of x on amount is zero\nAlt. hyp.: the effect of x on amount is not zero\n\n           coefficient  std.error  t.value p.value     \nIntercept        0.813      0.067   12.063  &lt; .001  ***\ntreatment        0.154      0.083    1.861   0.063    .\n\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nR-squared: 0.0, Adjusted R-squared: 0.0\nF-statistic: 3.461 df(1, 50081), p.value 0.063\nNr obs: 50,083\n\n\nFrom our linear regression model to calculate dollars donated amount from whether the donor was in the treatment or control group, we can conclude that the control group donates $0.813 and being a part of the treatment group increases that amount by $0.154, bringing the estimated dollars donated amount to $0.967. Per a 95% confidence interval, the model is not statistically significant, however it is very close to being so.\n\nlr_d_amount_above0 = rsm.regress(\n    data = karlan[karlan['amount'] &gt; 0][['amount', 'treatment']],\n    evar = 'treatment',\n    rvar = 'amount'\n    )  \n\nlr_d_amount_above0.summary()\n\nLinear regression (OLS)\nData                 : Not provided\nResponse variable    : amount\nExplanatory variables: treatment\nNull hyp.: the effect of x on amount is zero\nAlt. hyp.: the effect of x on amount is not zero\n\n           coefficient  std.error  t.value p.value     \nIntercept       45.540      2.423   18.792  &lt; .001  ***\ntreatment       -1.668      2.872   -0.581   0.561     \n\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nR-squared: 0.0, Adjusted R-squared: -0.001\nF-statistic: 0.337 df(1, 1032), p.value 0.561\nNr obs: 1,034\n\n\nAfter filtering the data to only include those who donated, the linear regression model changed significantly. The intercept coefficient is now $45.54, and the treatment coefficient is now negative $1.67. Now, the treatment group has a negative affect on amount donated, meaning that the control group donates $45.54 and the treatment group donates $43.87.\nHowever, the model is not statistically significant at 95% confidence interval.\n\n# create a histogram for all the donation amounts above 0 for the control group\n# have bin intervals be 25\n\nfig, ax = plt.subplots()\nax.hist(karlan[(karlan['amount'] &gt; 0) & (karlan['treatment'] == 0)]['amount'], bins=25)\nax.axvline(43.87, color='red')\nax.text(43.87 + 1, ax.get_ylim()[1] * 0.9, f'{43.87}', color='red')\nax.set_ylabel('Frequency')\nax.set_xlabel('Donation Amount')\nax.set_title('Frequency of Donation Amounts for Control Group')\nplt.show()\n\n\n\n\n\n\n\n\n\n\nfig, ax = plt.subplots()\nax.hist(karlan[(karlan['amount'] &gt; 0) & (karlan['treatment'] == 1)]['amount'], bins=25)\nax.axvline(45.54, color='red')\nax.text(45.54 + 1, ax.get_ylim()[1] * 0.9, f'{45.54}', color='red')\nax.set_ylabel('Frequency')\nax.set_xlabel('Donation Amount')\nax.set_title('Frequency of Donation Amounts for Treatment Group')\nplt.show()\n\n# add a line to the histogram at 43.87 and display the number 43.87\n\n\n\n\n\n\n\n\n\n\n\n\n\n# simulate 10,000 draws from the control distribution and 10,000 draws from the treatment distribution. You'll then calculate a vector of 10,000 differences, and then you'll plot the cumulative average of that vector of differences. Comment on whether the cumulative average approaches the true difference in means.\n\nimport numpy as np\nfrom scipy.stats import bernoulli\n\ncontrol = karlan[karlan['treatment'] == 0]['gave'].mean()\ntreatment = karlan[karlan['treatment'] == 1]['gave'].mean()\n\nprint(control, treatment)\n\n(0.017858212980164198, 0.02203856749311295)\n\n\n\ncontrol_sim = bernoulli.rvs(p = control, size = 10000)\ntreatment_sim = bernoulli.rvs(p = treatment, size = 10000)\n\n\ncontrol_sim.mean(), treatment_sim.mean()\n\n(0.0163, 0.0213)\n\n\n\n# calculate cumulative average of the differences for the first 10,000 draws\n\ncum_avg = np.cumsum(treatment_sim - control_sim) / np.arange(1, 10001)\n\ncum_avg\n\narray([0.       , 0.       , 0.       , ..., 0.005001 , 0.0050005,\n       0.005    ])\n\n\n\n# plot the cumulative average of the differences with a line plot\n\nfig, ax = plt.subplots()\nax.plot(cum_avg)\nax.axhline(treatment - control, color='red')\nax.set_ylabel('Cumulative Average of Differences')\nax.set_xlabel('Number of Draws')\nax.set_title('Cumulative Average of Differences in Proportion of Giving Money')\nplt.show()\n\n\n\n\n\n\n\n\nThe above graph shows a simulation of the differences between simulated probabilities of an individual actually donating in the treatment group and an individual donating in the control group. The control group was given a probability of 0.018 and treatment group was given a probability of 0.022, which were both calculating from the given dataset.\nWe simulated this for the control and treatment group 10,000 times to generate the graph. The red horizontal line on the graph shows the calculated difference between the treatment and control probabilities from the dataset (0.004).\nAs shown, the cumulative difference between the treatment and control group varies greatly initially, then slowly starts to vary less and less around the average difference of 0.004. With more trials, it is expected there would be even less variability and the cumulative difference would be extremely close to 0.004.\n\nn_50_c = np.random.binomial(50, 0.018, 1000)\n\nn_50_c = n_50_c / 50\n\nn_50_t = np.random.binomial(50, 0.022, 1000)\n\nn_50_t = n_50_t / 50\n\nn_50 = n_50_t - n_50_c\n\n\n# make a histogram of n_50 so there is no spacing between the bars\n\nfig, ax = plt.subplots()\nax.hist(n_50, bins=10, rwidth=1)\nax.axvline(n_50.mean(), color='red')\nax.text(n_50.mean(), ax.get_ylim()[1], f'{np.round(n_50.mean(),4)}', color='red', verticalalignment='center', horizontalalignment='center')\nax.set_ylabel('Frequency')\nax.set_xlabel('Difference in Proportion of Donors (Treatment - Control) in 50 Draws')\nax.set_title('Frequency of Proportion of Control Group that Gave Money in 50 Draws')\nplt.show()\n\n\n\n\n\n\n\n\n\nn_200_c = np.random.binomial(200, 0.018, 1000)/200\n\nn_200_t = np.random.binomial(200, 0.022, 1000)/200\n\n\nn_200 = n_200_t - n_200_c\n\nn_200_mean = n_200.mean()\n\nn_200_mean\n\n0.00452\n\n\n\nfig, ax = plt.subplots()\nax.hist(n_200, bins=10, rwidth=1)\nax.axvline(n_200_mean, color='red')\nax.text(n_200_mean, ax.get_ylim()[1], f'{np.round(n_200_mean,4)}', color='red', verticalalignment='center', horizontalalignment='center')\nax.set_ylabel('Frequency')\nax.set_xlabel('Difference in Proportion of Donors (Treatment - Control) in 200 Draws')\nax.set_title('Frequency of Proportion of Control Group that Gave Money in 200 Draws')\nplt.show()\n\n\n\n\n\n\n\n\n\nn_500_c = np.random.binomial(500, 0.018, 1000)/500\n\nn_500_t = np.random.binomial(500, 0.022, 1000)/500\n\n\nn_500 = n_500_t - n_500_c\n\nn_500_mean = n_500.mean()\n\nn_500_mean\n\n0.0040739999999999995\n\n\n\nfig, ax = plt.subplots()\nax.hist(n_500, bins=10, rwidth=1)\nax.axvline(n_500_mean, color='red')\nax.text(n_500_mean, ax.get_ylim()[1], f'{np.round(n_500_mean,4)}', color='red', verticalalignment='center', horizontalalignment='center')\nax.set_ylabel('Frequency')\nax.set_xlabel('Difference in Proportion of Donors (Treatment - Control) in 500 Draws')\nax.set_title('Frequency of Proportion of Control Group that Gave Money in 500 Draws')\nplt.show()\n\n\n\n\n\n\n\n\n\nn_1000_c = np.random.binomial(1000, 0.018, 1000)/1000\n\nn_1000_t = np.random.binomial(1000, 0.022, 1000)/1000\n\nn_1000 = n_1000_t - n_1000_c\n\nn_1000_mean = n_1000.mean()\n\nn_1000_mean\n\n0.004085\n\n\n\nfig, ax = plt.subplots()\nax.hist(n_1000, bins=10, rwidth=1)\nax.axvline(n_1000_mean, color='red')\nax.text(n_1000_mean, ax.get_ylim()[1]*0.9, f'{np.round(n_1000_mean,4)}', color='red', verticalalignment='center', horizontalalignment='right')\nax.set_ylabel('Frequency')\nax.set_xlabel('Difference in Proportion of Donors (Treatment - Control) in 500 Draws')\nax.set_title('Frequency of Proportion of Control Group that Gave Money in 500 Draws')\nplt.show()"
  },
  {
    "objectID": "projects/HW2/hw2_questions.html",
    "href": "projects/HW2/hw2_questions.html",
    "title": "Poisson Regression Examples",
    "section": "",
    "text": "Blueprinty is a small firm that makes software for developing blueprints specifically for submitting patent applications to the US patent office. Their marketing team would like to make the claim that patent applicants using Blueprinty’s software are more successful in getting their patent applications approved. Ideal data to study such an effect might include the success rate of patent applications before using Blueprinty’s software and after using it. unfortunately, such data is not available.\nHowever, Blueprinty has collected data on 1,500 mature (non-startup) engineering firms. The data include each firm’s number of patents awarded over the last 5 years, regional location, age since incorporation, and whether or not the firm uses Blueprinty’s software. The marketing team would like to use this data to make the claim that firms using Blueprinty’s software are more successful in getting their patent applications approved.\n\n\n\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom scipy.optimize import minimize\n\n# Load the data\n\ndata = pd.read_csv('blueprinty.csv')\n\ndata.head()\n\n\n\n\n\n\n\n\nUnnamed: 0\npatents\nregion\nage\niscustomer\n\n\n\n\n0\n1\n0\nMidwest\n32.5\n0\n\n\n1\n786\n3\nSouthwest\n37.5\n0\n\n\n2\n348\n4\nNorthwest\n27.0\n1\n\n\n3\n927\n3\nNortheast\n24.5\n0\n\n\n4\n830\n3\nSouthwest\n37.0\n0\n\n\n\n\n\n\n\nAfter reading in the data, we want to confirm that all businesses are unique and are not listed twice in the data\n\n# count the number of unique values in the column 'Unnamed: 0'\n\ndata['Unnamed: 0'].nunique()\n\n1500\n\n\nEach row is fact for an business which is not repeated in the dataset.\n\n# count number of customers and non-customers\n\ncustomers = data[data['iscustomer']==1]['iscustomer'].count()\nnon_customers = data[data['iscustomer']==0]['iscustomer'].count()\n\nprint(f' There are {customers} customers and {non_customers} non-customers in the dataset.')\n\n There are 197 customers and 1303 non-customers in the dataset.\n\n\nNow we will compare histograms based on number of patents for customers and non-customers on two separate plots.\n\n# histogram of # of patents for Customers\n\n\ndata[data['iscustomer'] == 1]['patents'].hist()\n\nplt.xlabel('Number of Patents')\nplt.ylabel('Frequency')\nplt.title('Frequency of Patents for Customers')\n\nText(0.5, 1.0, 'Frequency of Patents for Customers')\n\n\n\n\n\n\n\n\n\n\n# histogram of # of patents for non-customers\n\n\ndata[data['iscustomer'] == 0]['patents'].hist()\n\nplt.xlabel('Number of Patents')\nplt.ylabel('Frequency')\nplt.title('Frequency of Patents for Customers')\n\nText(0.5, 1.0, 'Frequency of Patents for Customers')\n\n\n\n\n\n\n\n\n\nNext, we’ll look at mean number of patents for customers and non-customers so we have a baseline for the histograms\n\nmean_patents_customers = data[data['iscustomer'] == 1]['patents'].mean()\nmean_patents_noncustomers = data[data['iscustomer'] == 0]['patents'].mean()\n\nprint('Mean patents for customers:', round(mean_patents_customers,2))\nprint('Mean patents for non-customers:', round(mean_patents_noncustomers,2))\n\nMean patents for customers: 4.09\nMean patents for non-customers: 3.62\n\n\nThe number of patents for customers is slightly skewed right, however it has a more normal distribution than the number of patents for non-customers.\nBoth plots have a large drop off around 6 patents (For customers it is slightly less than 6). The number of non-customers is significantly higher than the number of customers, totaling to 1303 non-customers and 197 customers. The mean number of patents for customers was 4.09 and the mean number of patents for non-customers was 3.62. This is a difference of about 0.5 patents. The number of customers and non-customers is important to keep into account when conducting regression models, as non-customers have a higher weight due to the higher frequency of occurence in the dataset.\nBlueprinty customers are not selected at random. It may be important to account for systematic differences in the age and regional location of customers vs non-customers.\nFirst, we’ll create a table showing the counts for each region based on customers and non-customers. We’ll looking at the proportion which each region has respectively for customers and non-customers.\n\n# group by region and count number of customers and non-customers\n\nregion = data.groupby('region')['iscustomer'].value_counts().unstack()\n\n# Calculate respective proportion of non-customers and customers which make up each region\n\nregion['Prop_Non_cust'] = region[0]/(region[0].sum())\nregion['Prop_Cust'] = region[1]/(region[1].sum())\n\n# Rename columns from 0 to Non-Customers and 1 to Customers\n\nregion.rename(columns={0:'Non-Customers', 1:'Customers'}, inplace=True)\n\nregion\n\n\n\n\n\n\n\niscustomer\nNon-Customers\nCustomers\nProp_Non_cust\nProp_Cust\n\n\nregion\n\n\n\n\n\n\n\n\nMidwest\n207\n17\n0.158864\n0.086294\n\n\nNortheast\n488\n113\n0.374520\n0.573604\n\n\nNorthwest\n171\n16\n0.131236\n0.081218\n\n\nSouth\n171\n20\n0.131236\n0.101523\n\n\nSouthwest\n266\n31\n0.204144\n0.157360\n\n\n\n\n\n\n\nBelow, you’ll see a plot of the customers and non-customers by region.\n\n# create a plot of the number of customers and non-customers by region side by side\n\ndata.groupby('region')['iscustomer'].value_counts().unstack().plot(kind='bar', stacked=False)\n\nplt.xlabel('Region')\nplt.ylabel('Count')\nplt.title('Number of Customers and Non-Customers by Region')\nplt.legend(['Non-Customer', 'Customer'])\n\n\n\n\n\n\n\n\nThe Northeast has by far the most customers and non-customers, and the number of non-customers in each region clearly outweighs the number of customers. Although ranking each respective customer and non-customer base by region comes out to be nearly the same ranking, the proportions are different.\nFor the customers, nearly 60% are from the NE, while only 40% of non-customers are from the NE.\nNext, we’ll do the same as we did for regions, except with age. One variation between the two will be binning the age groups for every 5 years. In this case, 0-5 years is one group, 5-10 years is another group, etc.\n\n# group by age with bins every 5 years and count number of customers and non-customers\n\ndata['age_bins'] = pd.cut(data['age'], bins=range(0, 60, 5))\n\nage = data.groupby('age_bins')['iscustomer'].value_counts().unstack()\nage['Prop_Non_cust'] = age[0]/(age[0].sum())\nage['Prop_Cust'] = age[1]/(age[1].sum())\n\nage.rename(columns={0:'Non-Customers', 1:'Customers'}, inplace=True)\n\nage\n\n/tmp/ipykernel_14747/1316710865.py:5: FutureWarning:\n\nThe default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n\n\n\n\n\n\n\n\n\niscustomer\nNon-Customers\nCustomers\nProp_Non_cust\nProp_Cust\n\n\nage_bins\n\n\n\n\n\n\n\n\n(0, 5]\n0\n0\n0.000000\n0.000000\n\n\n(5, 10]\n3\n3\n0.002302\n0.015228\n\n\n(10, 15]\n52\n15\n0.039908\n0.076142\n\n\n(15, 20]\n213\n53\n0.163469\n0.269036\n\n\n(20, 25]\n318\n51\n0.244052\n0.258883\n\n\n(25, 30]\n301\n32\n0.231005\n0.162437\n\n\n(30, 35]\n244\n24\n0.187260\n0.121827\n\n\n(35, 40]\n135\n13\n0.103607\n0.065990\n\n\n(40, 45]\n34\n5\n0.026094\n0.025381\n\n\n(45, 50]\n3\n1\n0.002302\n0.005076\n\n\n(50, 55]\n0\n0\n0.000000\n0.000000\n\n\n\n\n\n\n\n\n# create a plot of the number of customers and non-customers by age side by side\n\ndata.groupby('age_bins')['iscustomer'].value_counts().unstack().plot(kind='bar', stacked=False)\nplt.xlabel('Age of Company')\nplt.ylabel('Count')\nplt.title('Number of Customers and Non-Customers by Age of Company')\nplt.legend(['Non-Customer', 'Customer'])\n\n/tmp/ipykernel_14747/196641801.py:3: FutureWarning:\n\nThe default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n\n\n\n\n\n\n\n\n\n\nFor age, we’ll also look at the mean age of customers and non-customers, and calculate the 95% confidence intervals.\n\n# find mean age for customers and non-customers\n\nmean_age_customers = data[data['iscustomer'] == 1]['age'].mean()\nmean_age_noncustomers = data[data['iscustomer'] == 0]['age'].mean()\n\nprint('Mean age for customers:', round(mean_age_customers,2))\nprint('Mean age for non-customers:', round(mean_age_noncustomers,2))\n\nMean age for customers: 24.15\nMean age for non-customers: 26.69\n\n\n\n# Calculate a 95% confidence interval for the mean age of customers and non-customers\n\nimport numpy as np\n\nstd_age_customers = data[data['iscustomer'] == 1]['age'].std()\nstd_age_noncustomers = data[data['iscustomer'] == 0]['age'].std()\n\nn_customers = data[data['iscustomer'] == 1]['age'].count()\nn_noncustomers = data[data['iscustomer'] == 0]['age'].count()\n\nz = 1.96\n\nci_customers = z * (std_age_customers/np.sqrt(n_customers))\nci_noncustomers = z * (std_age_noncustomers/np.sqrt(n_noncustomers))\n\nprint('95% CI for mean age of customers:', round(mean_age_customers-ci_customers,2), round(mean_age_customers+ci_customers,2))\nprint('95% CI for mean age of non-customers:', round(mean_age_noncustomers-ci_noncustomers,2), round(mean_age_noncustomers+ci_noncustomers,2))\n\n95% CI for mean age of customers: 23.09 25.21\n95% CI for mean age of non-customers: 26.3 27.08\n\n\nThe most customers come from companies which are between 15-20 years old, with the average company age being about 24 years. The most non-customers come from companies which are between 20-25 years old, with the average company age being about 27 years.\nThe distribution of company ages for customers and non-customers resembles a normal distribution, with a slight skew to the right. The largest disparity between the two distributions is at the 10-15 year mark, where the % of non-customers is 10% lower than that of customers (16%, 26%).\nThe confidence interval for the mean age of customers is (23.1, 25.2) and the confidence interval for the mean age of non-customers is (26.3, 27.1).\n\n\n\nSince our outcome variable of interest can only be small integer values per a set unit of time, we can use a Poisson density to model the number of patents awarded to each engineering firm over the last 5 years. We start by estimating a simple Poisson model via Maximum Likelihood.\nBelow is the log-likelihood function for the Poisson distribution. The likelihood function of lambda given Y is the exact same as the function of Y given lambda.\nℓ(λ∣Y)=−λ+Ylog(λ)−log(Y!)\nBelow is the code for the log-likelihood function for a poisson distribution:\n\ndef poisson_log_likelihood(lam, y):\n    \"\"\"\n    Parameters:\n    - lam (float): The rate parameter (lambda) of the Poisson distribution.\n    - y (array-like): Array of observed counts.\n\n    Returns:\n    - float: The log likelihood of observing the data given lam.\n    \"\"\"\n    y = np.array(y)\n    n = len(y)  # number of observations\n    sum_y = np.sum(y)  # sum of all observed counts\n\n    # Calculate each part of the log likelihood\n    # log(P(Y|lam)) = -n * lam + sum_y * log(lam) - log(y_i!)\n    # We use np.sum(np.log(y_factorials)) to sum log of factorials\n    log_likelihood = -n * lam + sum_y * np.log(lam) - np.sum([np.log(np.math.factorial(i)) for i in y])\n    return log_likelihood\n\nNext, we’ll plot the log-likelihoods using our observed number of lambdas as Y and then a range of values for lambda (1-10).\n\nyears = range(1,11)\nlog_likelihood_values = []\n\nfor i in years:\n    log_likelihood_value = poisson_log_likelihood(i, data['patents'])\n    log_likelihood_values.append(log_likelihood_value)\n\nlog_likelihood_values\n\n/tmp/ipykernel_14747/1539929848.py:17: DeprecationWarning:\n\n`np.math` is a deprecated alias for the standard library `math` module (Deprecated Numpy 1.25). Replace usages of `np.math` with `math`\n\n\n\n[-6548.886990069443,\n -4217.862523114625,\n -3476.856870600801,\n -3386.838056159808,\n -3653.52364804617,\n -4145.8324036459835,\n -4793.841596240727,\n -5555.81358920499,\n -6404.826751132159,\n -7322.4991810913525]\n\n\n\nplt.plot(years, log_likelihood_values)\nplt.xlabel('Lambda')\nplt.ylabel('Likelihood')\nplt.title('Likelihood of observing the data given Lambda')\nplt.show()\n\n\n\n\n\n\n\n\nWe’ll now create our negative poisson MLE function and analyze the output betas.\n\nfrom scipy.optimize import minimize\n\ndef neg_poisson_log_likelihood(lam, y):\n    \"\"\"\n    Parameters:\n    - lam (float): The rate parameter (lambda) of the Poisson distribution.\n    - y (array-like): Array of observed counts.\n\n    Returns:\n    - float: The log likelihood of observing the data given lam.\n    \"\"\"\n    y = np.array(y)\n    n = len(y)  # number of observations\n    sum_y = np.sum(y)  # sum of all observed counts\n\n    # Calculate each part of the log likelihood\n    # log(P(Y|lam)) = -n * lam + sum_y * log(lam) - log(y_i!)\n    # We use np.sum(np.log(y_factorials)) to sum log of factorials\n    return -(-n * lam + sum_y * np.log(lam) - np.sum([np.log(np.math.factorial(i)) for i in y]))\n\n\nmean = np.mean(data['patents'])\n\nresult = minimize(neg_poisson_log_likelihood, mean, args=(data['patents']), bounds = [(0, None)])\n\nresult\n\n/tmp/ipykernel_14747/2966890401.py:19: DeprecationWarning:\n\n`np.math` is a deprecated alias for the standard library `math` module (Deprecated Numpy 1.25). Replace usages of `np.math` with `math`\n\n\n\n  message: CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_&lt;=_PGTOL\n  success: True\n   status: 0\n      fun: 3367.683772235094\n        x: [ 3.685e+00]\n      nit: 0\n      jac: [ 0.000e+00]\n     nfev: 2\n     njev: 1\n hess_inv: &lt;1x1 LbfgsInvHessProduct with dtype=float64&gt;\n\n\n\n\n\nNext, we extend our simple Poisson model to a Poisson Regression Model such that \\(Y_i = \\text{Poisson}(\\lambda_i)\\) where \\(\\lambda_i = \\exp(X_i'\\beta)\\). The interpretation is that the success rate of patent awards is not constant across all firms (\\(\\lambda\\)) but rather is a function of firm characteristics \\(X_i\\). Specifically, we will use the covariates age, age squared, region, and whether the firm is a customer of Blueprinty.\nFirst, we’ll create our new poisson MLE function, incorporating our explanatory variables into the calculation.\n\nimport numpy as np\nfrom scipy.optimize import minimize\n\ndef poisson_regression_log_likelihood(beta, Y, X):\n    eta = np.dot(X, beta)\n    lambda_i = np.exp(eta)\n    log_likelihood = np.sum(Y * eta - lambda_i)\n    return -log_likelihood\n\nNext, we’ll change the format of the data to be able to be used in our functions and analysis.\n\n# convert regions column to boolean columns, dropping the first region to be the default value\n\ndata = pd.get_dummies(data, columns=['region'], drop_first=True)\n\n# creating function to convert boolean column to binary\n\ndef convert_boolean_to_binary(data, column):\n    data[column] = data[column].astype(int)\n    return data\n\n# coverting region's boolean values to binary\n\ndata = convert_boolean_to_binary(data, 'region_Northeast')\ndata = convert_boolean_to_binary(data, 'region_South')\ndata = convert_boolean_to_binary(data, 'region_Southwest')\ndata = convert_boolean_to_binary(data, 'region_Northwest')\n\n# creating an age^2 column in the dataset\n\ndata['age_squared'] = data['age']**2\n\n\n# Load and preprocess your data as before, ensuring that features are scaled\nfrom sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\n# Scaling age and age_squared to prevent precision loss from extremely large numbers\ndata['age'] = scaler.fit_transform(data[['age']])\ndata['age_squared'] = scaler.fit_transform(data[['age_squared']])\n\n\n# Defining X and Y variables\nX = np.c_[np.ones(len(data)), data['age'], data['age_squared'], data['iscustomer'], data['region_Southwest'], data['region_Northwest'],\n          data['region_Northeast'], data['region_South']]\nY = data['patents'].values\n\n# Initial guess for beta (0)\ninitial_beta = np.zeros(X.shape[1])\n\n# Minimization\nresult = minimize(poisson_regression_log_likelihood, initial_beta, args=(Y, X), method='BFGS')\n\nprint(\"Optimal beta:\", result.x)\n\nOptimal beta: [ 1.21543809  1.04645982 -1.14084528  0.11811438  0.05134699 -0.02009421\n  0.098596    0.05717198]\n\n\nNow that we calculated our betas from our self-built function, we’ll verify our betas with a built-in regression function.\n\nimport statsmodels.api as sm\n\n# Fit a Poisson regression model using statsmodels\n\npoisson_model = sm.GLM(Y, X, family=sm.families.Poisson()).fit()\n\n# Print the summary of the model\n\nprint(poisson_model.summary())\n\n                 Generalized Linear Model Regression Results                  \n==============================================================================\nDep. Variable:                      y   No. Observations:                 1500\nModel:                            GLM   Df Residuals:                     1492\nModel Family:                 Poisson   Df Model:                            7\nLink Function:                    Log   Scale:                          1.0000\nMethod:                          IRLS   Log-Likelihood:                -3275.9\nDate:                Wed, 01 May 2024   Deviance:                       2178.8\nTime:                        18:08:07   Pearson chi2:                 2.11e+03\nNo. Iterations:                     5   Pseudo R-squ. (CS):             0.1152\nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          z      P&gt;|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst          1.2154      0.036     33.368      0.000       1.144       1.287\nx1             1.0465      0.100     10.414      0.000       0.850       1.243\nx2            -1.1408      0.102    -11.131      0.000      -1.342      -0.940\nx3             0.1181      0.039      3.035      0.002       0.042       0.194\nx4             0.0513      0.047      1.088      0.277      -0.041       0.144\nx5            -0.0201      0.054     -0.374      0.709      -0.126       0.085\nx6             0.0986      0.042      2.347      0.019       0.016       0.181\nx7             0.0572      0.053      1.085      0.278      -0.046       0.160\n==============================================================================\n\n\nFinally, we’ll calculate our standard errors for each respective beta or coefficient and compare to our built-in regression analysis results.\n\n# Calculate Hessian at the optimal beta\nfrom scipy.linalg import inv\nhessian_inv = result.hess_inv  # Inverse Hessian is returned by BFGS\n\n# Calculating standard errors by taking the square roots of the diagonal elements of the inverse Hessian\nstd_errors = np.sqrt(np.diag(hessian_inv))\n\nprint(\"Standard Errors:\", std_errors)\n\nStandard Errors: [0.02272912 0.14333007 0.14451598 0.03888018 0.03676005 0.04672971\n 0.02526631 0.02849847]\n\n\nWe can conclude, based on our optimal beta’s through our regression model, that Blueprinty’s software has a positive effect on the number of patents awarded to a company. The coefficient or beta calculated for “iscustomer” is 0.11, meaning if they are a customer of the software, the humber of patents earned increases by 0.11.\nWe also see that p-value of “iscustomer” is 0.002, meaning there is only a 0.2% chance that the coefficient has zero affect on the number of patents given the dataset.\nOur standard errors we’re slightly off for some of the variables, which may mean our self-built function has an issue, or we are using a different optimization method (ie. BFGS, L-BFGS-B) than the built-in function."
  },
  {
    "objectID": "projects/HW2/hw2_questions.html#blueprinty-case-study",
    "href": "projects/HW2/hw2_questions.html#blueprinty-case-study",
    "title": "Poisson Regression Examples",
    "section": "",
    "text": "Blueprinty is a small firm that makes software for developing blueprints specifically for submitting patent applications to the US patent office. Their marketing team would like to make the claim that patent applicants using Blueprinty’s software are more successful in getting their patent applications approved. Ideal data to study such an effect might include the success rate of patent applications before using Blueprinty’s software and after using it. unfortunately, such data is not available.\nHowever, Blueprinty has collected data on 1,500 mature (non-startup) engineering firms. The data include each firm’s number of patents awarded over the last 5 years, regional location, age since incorporation, and whether or not the firm uses Blueprinty’s software. The marketing team would like to use this data to make the claim that firms using Blueprinty’s software are more successful in getting their patent applications approved.\n\n\n\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom scipy.optimize import minimize\n\n# Load the data\n\ndata = pd.read_csv('blueprinty.csv')\n\ndata.head()\n\n\n\n\n\n\n\n\nUnnamed: 0\npatents\nregion\nage\niscustomer\n\n\n\n\n0\n1\n0\nMidwest\n32.5\n0\n\n\n1\n786\n3\nSouthwest\n37.5\n0\n\n\n2\n348\n4\nNorthwest\n27.0\n1\n\n\n3\n927\n3\nNortheast\n24.5\n0\n\n\n4\n830\n3\nSouthwest\n37.0\n0\n\n\n\n\n\n\n\nAfter reading in the data, we want to confirm that all businesses are unique and are not listed twice in the data\n\n# count the number of unique values in the column 'Unnamed: 0'\n\ndata['Unnamed: 0'].nunique()\n\n1500\n\n\nEach row is fact for an business which is not repeated in the dataset.\n\n# count number of customers and non-customers\n\ncustomers = data[data['iscustomer']==1]['iscustomer'].count()\nnon_customers = data[data['iscustomer']==0]['iscustomer'].count()\n\nprint(f' There are {customers} customers and {non_customers} non-customers in the dataset.')\n\n There are 197 customers and 1303 non-customers in the dataset.\n\n\nNow we will compare histograms based on number of patents for customers and non-customers on two separate plots.\n\n# histogram of # of patents for Customers\n\n\ndata[data['iscustomer'] == 1]['patents'].hist()\n\nplt.xlabel('Number of Patents')\nplt.ylabel('Frequency')\nplt.title('Frequency of Patents for Customers')\n\nText(0.5, 1.0, 'Frequency of Patents for Customers')\n\n\n\n\n\n\n\n\n\n\n# histogram of # of patents for non-customers\n\n\ndata[data['iscustomer'] == 0]['patents'].hist()\n\nplt.xlabel('Number of Patents')\nplt.ylabel('Frequency')\nplt.title('Frequency of Patents for Customers')\n\nText(0.5, 1.0, 'Frequency of Patents for Customers')\n\n\n\n\n\n\n\n\n\nNext, we’ll look at mean number of patents for customers and non-customers so we have a baseline for the histograms\n\nmean_patents_customers = data[data['iscustomer'] == 1]['patents'].mean()\nmean_patents_noncustomers = data[data['iscustomer'] == 0]['patents'].mean()\n\nprint('Mean patents for customers:', round(mean_patents_customers,2))\nprint('Mean patents for non-customers:', round(mean_patents_noncustomers,2))\n\nMean patents for customers: 4.09\nMean patents for non-customers: 3.62\n\n\nThe number of patents for customers is slightly skewed right, however it has a more normal distribution than the number of patents for non-customers.\nBoth plots have a large drop off around 6 patents (For customers it is slightly less than 6). The number of non-customers is significantly higher than the number of customers, totaling to 1303 non-customers and 197 customers. The mean number of patents for customers was 4.09 and the mean number of patents for non-customers was 3.62. This is a difference of about 0.5 patents. The number of customers and non-customers is important to keep into account when conducting regression models, as non-customers have a higher weight due to the higher frequency of occurence in the dataset.\nBlueprinty customers are not selected at random. It may be important to account for systematic differences in the age and regional location of customers vs non-customers.\nFirst, we’ll create a table showing the counts for each region based on customers and non-customers. We’ll looking at the proportion which each region has respectively for customers and non-customers.\n\n# group by region and count number of customers and non-customers\n\nregion = data.groupby('region')['iscustomer'].value_counts().unstack()\n\n# Calculate respective proportion of non-customers and customers which make up each region\n\nregion['Prop_Non_cust'] = region[0]/(region[0].sum())\nregion['Prop_Cust'] = region[1]/(region[1].sum())\n\n# Rename columns from 0 to Non-Customers and 1 to Customers\n\nregion.rename(columns={0:'Non-Customers', 1:'Customers'}, inplace=True)\n\nregion\n\n\n\n\n\n\n\niscustomer\nNon-Customers\nCustomers\nProp_Non_cust\nProp_Cust\n\n\nregion\n\n\n\n\n\n\n\n\nMidwest\n207\n17\n0.158864\n0.086294\n\n\nNortheast\n488\n113\n0.374520\n0.573604\n\n\nNorthwest\n171\n16\n0.131236\n0.081218\n\n\nSouth\n171\n20\n0.131236\n0.101523\n\n\nSouthwest\n266\n31\n0.204144\n0.157360\n\n\n\n\n\n\n\nBelow, you’ll see a plot of the customers and non-customers by region.\n\n# create a plot of the number of customers and non-customers by region side by side\n\ndata.groupby('region')['iscustomer'].value_counts().unstack().plot(kind='bar', stacked=False)\n\nplt.xlabel('Region')\nplt.ylabel('Count')\nplt.title('Number of Customers and Non-Customers by Region')\nplt.legend(['Non-Customer', 'Customer'])\n\n\n\n\n\n\n\n\nThe Northeast has by far the most customers and non-customers, and the number of non-customers in each region clearly outweighs the number of customers. Although ranking each respective customer and non-customer base by region comes out to be nearly the same ranking, the proportions are different.\nFor the customers, nearly 60% are from the NE, while only 40% of non-customers are from the NE.\nNext, we’ll do the same as we did for regions, except with age. One variation between the two will be binning the age groups for every 5 years. In this case, 0-5 years is one group, 5-10 years is another group, etc.\n\n# group by age with bins every 5 years and count number of customers and non-customers\n\ndata['age_bins'] = pd.cut(data['age'], bins=range(0, 60, 5))\n\nage = data.groupby('age_bins')['iscustomer'].value_counts().unstack()\nage['Prop_Non_cust'] = age[0]/(age[0].sum())\nage['Prop_Cust'] = age[1]/(age[1].sum())\n\nage.rename(columns={0:'Non-Customers', 1:'Customers'}, inplace=True)\n\nage\n\n/tmp/ipykernel_14747/1316710865.py:5: FutureWarning:\n\nThe default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n\n\n\n\n\n\n\n\n\niscustomer\nNon-Customers\nCustomers\nProp_Non_cust\nProp_Cust\n\n\nage_bins\n\n\n\n\n\n\n\n\n(0, 5]\n0\n0\n0.000000\n0.000000\n\n\n(5, 10]\n3\n3\n0.002302\n0.015228\n\n\n(10, 15]\n52\n15\n0.039908\n0.076142\n\n\n(15, 20]\n213\n53\n0.163469\n0.269036\n\n\n(20, 25]\n318\n51\n0.244052\n0.258883\n\n\n(25, 30]\n301\n32\n0.231005\n0.162437\n\n\n(30, 35]\n244\n24\n0.187260\n0.121827\n\n\n(35, 40]\n135\n13\n0.103607\n0.065990\n\n\n(40, 45]\n34\n5\n0.026094\n0.025381\n\n\n(45, 50]\n3\n1\n0.002302\n0.005076\n\n\n(50, 55]\n0\n0\n0.000000\n0.000000\n\n\n\n\n\n\n\n\n# create a plot of the number of customers and non-customers by age side by side\n\ndata.groupby('age_bins')['iscustomer'].value_counts().unstack().plot(kind='bar', stacked=False)\nplt.xlabel('Age of Company')\nplt.ylabel('Count')\nplt.title('Number of Customers and Non-Customers by Age of Company')\nplt.legend(['Non-Customer', 'Customer'])\n\n/tmp/ipykernel_14747/196641801.py:3: FutureWarning:\n\nThe default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n\n\n\n\n\n\n\n\n\n\nFor age, we’ll also look at the mean age of customers and non-customers, and calculate the 95% confidence intervals.\n\n# find mean age for customers and non-customers\n\nmean_age_customers = data[data['iscustomer'] == 1]['age'].mean()\nmean_age_noncustomers = data[data['iscustomer'] == 0]['age'].mean()\n\nprint('Mean age for customers:', round(mean_age_customers,2))\nprint('Mean age for non-customers:', round(mean_age_noncustomers,2))\n\nMean age for customers: 24.15\nMean age for non-customers: 26.69\n\n\n\n# Calculate a 95% confidence interval for the mean age of customers and non-customers\n\nimport numpy as np\n\nstd_age_customers = data[data['iscustomer'] == 1]['age'].std()\nstd_age_noncustomers = data[data['iscustomer'] == 0]['age'].std()\n\nn_customers = data[data['iscustomer'] == 1]['age'].count()\nn_noncustomers = data[data['iscustomer'] == 0]['age'].count()\n\nz = 1.96\n\nci_customers = z * (std_age_customers/np.sqrt(n_customers))\nci_noncustomers = z * (std_age_noncustomers/np.sqrt(n_noncustomers))\n\nprint('95% CI for mean age of customers:', round(mean_age_customers-ci_customers,2), round(mean_age_customers+ci_customers,2))\nprint('95% CI for mean age of non-customers:', round(mean_age_noncustomers-ci_noncustomers,2), round(mean_age_noncustomers+ci_noncustomers,2))\n\n95% CI for mean age of customers: 23.09 25.21\n95% CI for mean age of non-customers: 26.3 27.08\n\n\nThe most customers come from companies which are between 15-20 years old, with the average company age being about 24 years. The most non-customers come from companies which are between 20-25 years old, with the average company age being about 27 years.\nThe distribution of company ages for customers and non-customers resembles a normal distribution, with a slight skew to the right. The largest disparity between the two distributions is at the 10-15 year mark, where the % of non-customers is 10% lower than that of customers (16%, 26%).\nThe confidence interval for the mean age of customers is (23.1, 25.2) and the confidence interval for the mean age of non-customers is (26.3, 27.1).\n\n\n\nSince our outcome variable of interest can only be small integer values per a set unit of time, we can use a Poisson density to model the number of patents awarded to each engineering firm over the last 5 years. We start by estimating a simple Poisson model via Maximum Likelihood.\nBelow is the log-likelihood function for the Poisson distribution. The likelihood function of lambda given Y is the exact same as the function of Y given lambda.\nℓ(λ∣Y)=−λ+Ylog(λ)−log(Y!)\nBelow is the code for the log-likelihood function for a poisson distribution:\n\ndef poisson_log_likelihood(lam, y):\n    \"\"\"\n    Parameters:\n    - lam (float): The rate parameter (lambda) of the Poisson distribution.\n    - y (array-like): Array of observed counts.\n\n    Returns:\n    - float: The log likelihood of observing the data given lam.\n    \"\"\"\n    y = np.array(y)\n    n = len(y)  # number of observations\n    sum_y = np.sum(y)  # sum of all observed counts\n\n    # Calculate each part of the log likelihood\n    # log(P(Y|lam)) = -n * lam + sum_y * log(lam) - log(y_i!)\n    # We use np.sum(np.log(y_factorials)) to sum log of factorials\n    log_likelihood = -n * lam + sum_y * np.log(lam) - np.sum([np.log(np.math.factorial(i)) for i in y])\n    return log_likelihood\n\nNext, we’ll plot the log-likelihoods using our observed number of lambdas as Y and then a range of values for lambda (1-10).\n\nyears = range(1,11)\nlog_likelihood_values = []\n\nfor i in years:\n    log_likelihood_value = poisson_log_likelihood(i, data['patents'])\n    log_likelihood_values.append(log_likelihood_value)\n\nlog_likelihood_values\n\n/tmp/ipykernel_14747/1539929848.py:17: DeprecationWarning:\n\n`np.math` is a deprecated alias for the standard library `math` module (Deprecated Numpy 1.25). Replace usages of `np.math` with `math`\n\n\n\n[-6548.886990069443,\n -4217.862523114625,\n -3476.856870600801,\n -3386.838056159808,\n -3653.52364804617,\n -4145.8324036459835,\n -4793.841596240727,\n -5555.81358920499,\n -6404.826751132159,\n -7322.4991810913525]\n\n\n\nplt.plot(years, log_likelihood_values)\nplt.xlabel('Lambda')\nplt.ylabel('Likelihood')\nplt.title('Likelihood of observing the data given Lambda')\nplt.show()\n\n\n\n\n\n\n\n\nWe’ll now create our negative poisson MLE function and analyze the output betas.\n\nfrom scipy.optimize import minimize\n\ndef neg_poisson_log_likelihood(lam, y):\n    \"\"\"\n    Parameters:\n    - lam (float): The rate parameter (lambda) of the Poisson distribution.\n    - y (array-like): Array of observed counts.\n\n    Returns:\n    - float: The log likelihood of observing the data given lam.\n    \"\"\"\n    y = np.array(y)\n    n = len(y)  # number of observations\n    sum_y = np.sum(y)  # sum of all observed counts\n\n    # Calculate each part of the log likelihood\n    # log(P(Y|lam)) = -n * lam + sum_y * log(lam) - log(y_i!)\n    # We use np.sum(np.log(y_factorials)) to sum log of factorials\n    return -(-n * lam + sum_y * np.log(lam) - np.sum([np.log(np.math.factorial(i)) for i in y]))\n\n\nmean = np.mean(data['patents'])\n\nresult = minimize(neg_poisson_log_likelihood, mean, args=(data['patents']), bounds = [(0, None)])\n\nresult\n\n/tmp/ipykernel_14747/2966890401.py:19: DeprecationWarning:\n\n`np.math` is a deprecated alias for the standard library `math` module (Deprecated Numpy 1.25). Replace usages of `np.math` with `math`\n\n\n\n  message: CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_&lt;=_PGTOL\n  success: True\n   status: 0\n      fun: 3367.683772235094\n        x: [ 3.685e+00]\n      nit: 0\n      jac: [ 0.000e+00]\n     nfev: 2\n     njev: 1\n hess_inv: &lt;1x1 LbfgsInvHessProduct with dtype=float64&gt;\n\n\n\n\n\nNext, we extend our simple Poisson model to a Poisson Regression Model such that \\(Y_i = \\text{Poisson}(\\lambda_i)\\) where \\(\\lambda_i = \\exp(X_i'\\beta)\\). The interpretation is that the success rate of patent awards is not constant across all firms (\\(\\lambda\\)) but rather is a function of firm characteristics \\(X_i\\). Specifically, we will use the covariates age, age squared, region, and whether the firm is a customer of Blueprinty.\nFirst, we’ll create our new poisson MLE function, incorporating our explanatory variables into the calculation.\n\nimport numpy as np\nfrom scipy.optimize import minimize\n\ndef poisson_regression_log_likelihood(beta, Y, X):\n    eta = np.dot(X, beta)\n    lambda_i = np.exp(eta)\n    log_likelihood = np.sum(Y * eta - lambda_i)\n    return -log_likelihood\n\nNext, we’ll change the format of the data to be able to be used in our functions and analysis.\n\n# convert regions column to boolean columns, dropping the first region to be the default value\n\ndata = pd.get_dummies(data, columns=['region'], drop_first=True)\n\n# creating function to convert boolean column to binary\n\ndef convert_boolean_to_binary(data, column):\n    data[column] = data[column].astype(int)\n    return data\n\n# coverting region's boolean values to binary\n\ndata = convert_boolean_to_binary(data, 'region_Northeast')\ndata = convert_boolean_to_binary(data, 'region_South')\ndata = convert_boolean_to_binary(data, 'region_Southwest')\ndata = convert_boolean_to_binary(data, 'region_Northwest')\n\n# creating an age^2 column in the dataset\n\ndata['age_squared'] = data['age']**2\n\n\n# Load and preprocess your data as before, ensuring that features are scaled\nfrom sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\n# Scaling age and age_squared to prevent precision loss from extremely large numbers\ndata['age'] = scaler.fit_transform(data[['age']])\ndata['age_squared'] = scaler.fit_transform(data[['age_squared']])\n\n\n# Defining X and Y variables\nX = np.c_[np.ones(len(data)), data['age'], data['age_squared'], data['iscustomer'], data['region_Southwest'], data['region_Northwest'],\n          data['region_Northeast'], data['region_South']]\nY = data['patents'].values\n\n# Initial guess for beta (0)\ninitial_beta = np.zeros(X.shape[1])\n\n# Minimization\nresult = minimize(poisson_regression_log_likelihood, initial_beta, args=(Y, X), method='BFGS')\n\nprint(\"Optimal beta:\", result.x)\n\nOptimal beta: [ 1.21543809  1.04645982 -1.14084528  0.11811438  0.05134699 -0.02009421\n  0.098596    0.05717198]\n\n\nNow that we calculated our betas from our self-built function, we’ll verify our betas with a built-in regression function.\n\nimport statsmodels.api as sm\n\n# Fit a Poisson regression model using statsmodels\n\npoisson_model = sm.GLM(Y, X, family=sm.families.Poisson()).fit()\n\n# Print the summary of the model\n\nprint(poisson_model.summary())\n\n                 Generalized Linear Model Regression Results                  \n==============================================================================\nDep. Variable:                      y   No. Observations:                 1500\nModel:                            GLM   Df Residuals:                     1492\nModel Family:                 Poisson   Df Model:                            7\nLink Function:                    Log   Scale:                          1.0000\nMethod:                          IRLS   Log-Likelihood:                -3275.9\nDate:                Wed, 01 May 2024   Deviance:                       2178.8\nTime:                        18:08:07   Pearson chi2:                 2.11e+03\nNo. Iterations:                     5   Pseudo R-squ. (CS):             0.1152\nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          z      P&gt;|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst          1.2154      0.036     33.368      0.000       1.144       1.287\nx1             1.0465      0.100     10.414      0.000       0.850       1.243\nx2            -1.1408      0.102    -11.131      0.000      -1.342      -0.940\nx3             0.1181      0.039      3.035      0.002       0.042       0.194\nx4             0.0513      0.047      1.088      0.277      -0.041       0.144\nx5            -0.0201      0.054     -0.374      0.709      -0.126       0.085\nx6             0.0986      0.042      2.347      0.019       0.016       0.181\nx7             0.0572      0.053      1.085      0.278      -0.046       0.160\n==============================================================================\n\n\nFinally, we’ll calculate our standard errors for each respective beta or coefficient and compare to our built-in regression analysis results.\n\n# Calculate Hessian at the optimal beta\nfrom scipy.linalg import inv\nhessian_inv = result.hess_inv  # Inverse Hessian is returned by BFGS\n\n# Calculating standard errors by taking the square roots of the diagonal elements of the inverse Hessian\nstd_errors = np.sqrt(np.diag(hessian_inv))\n\nprint(\"Standard Errors:\", std_errors)\n\nStandard Errors: [0.02272912 0.14333007 0.14451598 0.03888018 0.03676005 0.04672971\n 0.02526631 0.02849847]\n\n\nWe can conclude, based on our optimal beta’s through our regression model, that Blueprinty’s software has a positive effect on the number of patents awarded to a company. The coefficient or beta calculated for “iscustomer” is 0.11, meaning if they are a customer of the software, the humber of patents earned increases by 0.11.\nWe also see that p-value of “iscustomer” is 0.002, meaning there is only a 0.2% chance that the coefficient has zero affect on the number of patents given the dataset.\nOur standard errors we’re slightly off for some of the variables, which may mean our self-built function has an issue, or we are using a different optimization method (ie. BFGS, L-BFGS-B) than the built-in function."
  },
  {
    "objectID": "projects/HW2/hw2_questions.html#airbnb-case-study",
    "href": "projects/HW2/hw2_questions.html#airbnb-case-study",
    "title": "Poisson Regression Examples",
    "section": "AirBnB Case Study",
    "text": "AirBnB Case Study\n\nIntroduction\nAirBnB is a popular platform for booking short-term rentals. In March 2017, students Annika Awad, Evan Lebo, and Anna Linden scraped of 40,000 Airbnb listings from New York City. The data include the following variables:\n\n\n\n\n\n\nVariable Definitions\n\n\n\n\n\n- `id` = unique ID number for each unit\n- `last_scraped` = date when information scraped\n- `host_since` = date when host first listed the unit on Airbnb\n- `days` = `last_scraped` - `host_since` = number of days the unit has been listed\n- `room_type` = Entire home/apt., Private room, or Shared room\n- `bathrooms` = number of bathrooms\n- `bedrooms` = number of bedrooms\n- `price` = price per night (dollars)\n- `number_of_reviews` = number of reviews for the unit on Airbnb\n- `review_scores_cleanliness` = a cleanliness score from reviews (1-10)\n- `review_scores_location` = a \"quality of location\" score from reviews (1-10)\n- `review_scores_value` = a \"quality of value\" score from reviews (1-10)\n- `instant_bookable` = \"t\" if instantly bookable, \"f\" if not\n\n\n\nUsing the airbnb data, we will build a likelihood function for this poisson regression. However, we first need to read the data and look at its characteristics.\n\n# read airbnb csv\n\nairbnb = pd.read_csv('airbnb.csv')\n\nairbnb.head()\n\n\n\n\n\n\n\n\nUnnamed: 0\nid\ndays\nlast_scraped\nhost_since\nroom_type\nbathrooms\nbedrooms\nprice\nnumber_of_reviews\nreview_scores_cleanliness\nreview_scores_location\nreview_scores_value\ninstant_bookable\n\n\n\n\n0\n1\n2515\n3130\n4/2/2017\n9/6/2008\nPrivate room\n1.0\n1.0\n59\n150\n9.0\n9.0\n9.0\nf\n\n\n1\n2\n2595\n3127\n4/2/2017\n9/9/2008\nEntire home/apt\n1.0\n0.0\n230\n20\n9.0\n10.0\n9.0\nf\n\n\n2\n3\n3647\n3050\n4/2/2017\n11/25/2008\nPrivate room\n1.0\n1.0\n150\n0\nNaN\nNaN\nNaN\nf\n\n\n3\n4\n3831\n3038\n4/2/2017\n12/7/2008\nEntire home/apt\n1.0\n1.0\n89\n116\n9.0\n9.0\n9.0\nf\n\n\n4\n5\n4611\n3012\n4/2/2017\n1/2/2009\nPrivate room\nNaN\n1.0\n39\n93\n9.0\n8.0\n9.0\nt\n\n\n\n\n\n\n\n\n# count number of listings which have 0 reviews\nairbnb[airbnb['number_of_reviews'] == 0]['number_of_reviews'].count()\n\n9481\n\n\n\n# create subset of data which only shows bedrooms which are nan\n\nairbnb.isnull().sum()\n\nUnnamed: 0                       0\nid                               0\ndays                             0\nlast_scraped                     0\nhost_since                      35\nroom_type                        0\nbathrooms                      160\nbedrooms                        76\nprice                            0\nnumber_of_reviews                0\nreview_scores_cleanliness    10195\nreview_scores_location       10254\nreview_scores_value          10256\ninstant_bookable                 0\ndtype: int64\n\n\nLooking at the data initally, we see there are many cells having null values which we need to address. The majority of them are within the review score columns. To address this, we will drop the cells which have 0 days listed (brand new listings). For bedrooms and bathrooms in a shared or private room, we will assume bedrooms is 1 and bathrooms are 0. It is very common for rooms to not have a bathroom. For all other data which is null, we cannot say with reasonable certainty what the value would be. For example, if bedrooms or bathrooms are null for entire home or apartment, we cannot say with reasonable certainty the number of bedrooms or bathrooms.\n\n# fill bedrooms with 1 and bathrooms with 0 if nan and if room type is private or shared room\n\nairbnb.loc[(airbnb['bedrooms'].isnull()) & (airbnb['room_type'] == 'Private room'), 'bedrooms'] = 1\nairbnb.loc[(airbnb['bedrooms'].isnull()) & (airbnb['room_type'] == 'Shared room'), 'bedrooms'] = 1\nairbnb.loc[(airbnb['bathrooms'].isnull()) & (airbnb['room_type'] == 'Private room'), 'bathrooms'] = 0\nairbnb.loc[(airbnb['bathrooms'].isnull()) & (airbnb['room_type'] == 'Private room'), 'bathrooms'] = 0\n\n\n# Dropping all other rows which have null values\nairbnb = airbnb.dropna()\n\n\nairbnb.shape\n\n(30234, 14)\n\n\n\nairbnb.describe()\n\n\n\n\n\n\n\n\nUnnamed: 0\nid\ndays\nbathrooms\nbedrooms\nprice\nnumber_of_reviews\nreview_scores_cleanliness\nreview_scores_location\nreview_scores_value\n\n\n\n\ncount\n30234.000000\n3.023400e+04\n30234.000000\n30234.000000\n30234.000000\n30234.000000\n30234.000000\n30234.000000\n30234.000000\n30234.000000\n\n\nmean\n18633.661937\n8.955396e+06\n1114.714560\n1.118856\n1.150989\n140.062777\n21.245981\n9.200370\n9.414070\n9.332837\n\n\nstd\n11340.011572\n5.388450e+06\n646.007986\n0.389266\n0.698002\n188.205058\n32.137276\n1.114729\n0.843788\n0.900678\n\n\nmin\n1.000000\n2.515000e+03\n7.000000\n0.000000\n0.000000\n10.000000\n1.000000\n2.000000\n2.000000\n2.000000\n\n\n25%\n8554.500000\n4.244554e+06\n585.000000\n1.000000\n1.000000\n70.000000\n3.000000\n9.000000\n9.000000\n9.000000\n\n\n50%\n18187.000000\n9.128754e+06\n1044.000000\n1.000000\n1.000000\n103.000000\n8.000000\n10.000000\n10.000000\n10.000000\n\n\n75%\n28500.500000\n1.391076e+07\n1595.000000\n1.000000\n1.000000\n169.000000\n26.000000\n10.000000\n10.000000\n10.000000\n\n\nmax\n40504.000000\n1.797369e+07\n3317.000000\n6.000000\n10.000000\n10000.000000\n421.000000\n10.000000\n10.000000\n10.000000\n\n\n\n\n\n\n\n\nairbnb.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 30234 entries, 0 to 40503\nData columns (total 14 columns):\n #   Column                     Non-Null Count  Dtype  \n---  ------                     --------------  -----  \n 0   Unnamed: 0                 30234 non-null  int64  \n 1   id                         30234 non-null  int64  \n 2   days                       30234 non-null  int64  \n 3   last_scraped               30234 non-null  object \n 4   host_since                 30234 non-null  object \n 5   room_type                  30234 non-null  object \n 6   bathrooms                  30234 non-null  float64\n 7   bedrooms                   30234 non-null  float64\n 8   price                      30234 non-null  int64  \n 9   number_of_reviews          30234 non-null  int64  \n 10  review_scores_cleanliness  30234 non-null  float64\n 11  review_scores_location     30234 non-null  float64\n 12  review_scores_value        30234 non-null  float64\n 13  instant_bookable           30234 non-null  object \ndtypes: float64(5), int64(5), object(4)\nmemory usage: 3.5+ MB\n\n\nThe average number of reviews is 15.9 for this dataset, however the standard deviation is 29.25, meaning there is a large spread in the number of reviews, with many locations having 0 reviews (Right skewed)\nMost of the listings in the dataset rank very high in cleanliness, location, and value, with all having an average of above 9 out of 10.\nThe price of the listings is also left skewed, with the average price being $145, but the standard deviation being $211, meaning there is a large spread in the price of listings like the number of reviews. However with this spread, due to the fact that the price can’t be negative, the distribution is left skewed.\nWhile looking at the info for the columns, we will need to convert some of the variables for regression analysis. The columns need be an integer or float data type. We will be changing room type and instant bookable columns.\n\n# convert room_type to boolean\n\nairbnb = pd.get_dummies(airbnb, columns=['room_type'], drop_first=True)\n\n\nfor i in airbnb['instant_bookable']:\n    if i == 't':\n        airbnb['instant_bookable'] = 1\n    else:\n        airbnb['instant_bookable'] = 0\n\nPrior to building the regression model, we need to make sure the data is all in the same time interval since we are assuming it’s a poisson distribution.\n\n# Converting number of reviews to reviews per year\n\nairbnb['reviews_per_year'] = airbnb['number_of_reviews'] / airbnb['days'] * 365\n\nWe will also be scaling the data in order to prevent precision loss during our regression analysis.\n\nfrom sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\n# Assuming 'age' and 'age_squared' need scaling\nairbnb['days_scaled'] = scaler.fit_transform(airbnb[['days']])\nairbnb['price'] = scaler.fit_transform(airbnb[['price']])\nairbnb['review_scores_cleanliness'] = scaler.fit_transform(airbnb[['review_scores_cleanliness']])\nairbnb['review_scores_location'] = scaler.fit_transform(airbnb[['review_scores_location']])\nairbnb['review_scores_value'] = scaler.fit_transform(airbnb[['review_scores_value']])\n\nNext, we will use the previously generated MLE function to get the maximum likelihood estimators to predict our reviews per year.\n\nX = np.c_[np.ones(len(airbnb)), airbnb['days_scaled'], airbnb['bathrooms'], airbnb['bedrooms'], airbnb['price'], \n          airbnb['review_scores_cleanliness'], airbnb['review_scores_location'], airbnb['review_scores_value'], \n          airbnb['instant_bookable'], airbnb['room_type_Private room'], airbnb['room_type_Shared room']]\nY = airbnb['reviews_per_year'].values\n\n# Initial guess for beta\ninitial_beta = np.zeros(X.shape[1])\n\n# Minimization\nresult = minimize(poisson_regression_log_likelihood, initial_beta, args=(Y, X), method='BFGS')\n\nprint(\"Optimal beta:\", result.x)\n\nOptimal beta: [ 2.05080599 -0.43566627 -0.02481847  0.0721578  -0.02531794  0.13872818\n -0.05758502 -0.05235691  0.         -0.00724632 -0.07017587]\n\n\nNext, we will confirm our betas with a built-in regression function.\n\n# Fit a Poisson regression model using statsmodels\n\npoisson_model = sm.GLM(Y, X, family=sm.families.Poisson()).fit()\n\n# Print the summary of the model\n\nprint(poisson_model.summary())\n\n                 Generalized Linear Model Regression Results                  \n==============================================================================\nDep. Variable:                      y   No. Observations:                30234\nModel:                            GLM   Df Residuals:                    30224\nModel Family:                 Poisson   Df Model:                            9\nLink Function:                    Log   Scale:                          1.0000\nMethod:                          IRLS   Log-Likelihood:            -2.2032e+05\nDate:                Wed, 01 May 2024   Deviance:                   3.4336e+05\nTime:                        18:08:09   Pearson chi2:                 4.58e+05\nNo. Iterations:                     5   Pseudo R-squ. (CS):             0.8026\nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          z      P&gt;|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst          2.0508      0.007    313.513      0.000       2.038       2.064\nx1            -0.4357      0.002   -201.380      0.000      -0.440      -0.431\nx2            -0.0248      0.005     -4.572      0.000      -0.035      -0.014\nx3             0.0722      0.003     23.812      0.000       0.066       0.078\nx4            -0.0253      0.003     -7.690      0.000      -0.032      -0.019\nx5             0.1387      0.003     54.015      0.000       0.134       0.144\nx6            -0.0576      0.002    -27.251      0.000      -0.062      -0.053\nx7            -0.0524      0.002    -20.945      0.000      -0.057      -0.047\nx8           4.66e-18   3.65e-19     12.771      0.000    3.94e-18    5.37e-18\nx9            -0.0072      0.004     -1.663      0.096      -0.016       0.001\nx10           -0.0702      0.012     -6.036      0.000      -0.093      -0.047\n==============================================================================\n\n\nOur coefficients match, meaning per the data and our manipulation, we calculated the betas correctly.\nBelow, we are grouping the some of the data to potentially get a better explanation and generate a reasonable hypothesis for the outcome of the betas.\n\n# grouping the data based on days (binning the days) and reviews_per_year (binning reviews per year)\n\nairbnb['days_bins'] = pd.cut(airbnb['days'], bins=range(0, 400, 50))\n\nairbnb['reviews_per_year_bins'] = pd.cut(airbnb['reviews_per_year'], bins=range(0, 150, 25))\n\ndays_reviews = airbnb.groupby('days_bins')['reviews_per_year_bins'].value_counts().unstack()\n\ndays_reviews\n\n/tmp/ipykernel_14747/4142779578.py:7: FutureWarning:\n\nThe default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n\n\n\n\n\n\n\n\n\nreviews_per_year_bins\n(0, 25]\n(25, 50]\n(50, 75]\n(75, 100]\n(100, 125]\n\n\ndays_bins\n\n\n\n\n\n\n\n\n\n(0, 50]\n111\n62\n13\n10\n3\n\n\n(50, 100]\n187\n71\n18\n4\n2\n\n\n(100, 150]\n439\n117\n31\n5\n4\n\n\n(150, 200]\n438\n110\n51\n11\n4\n\n\n(200, 250]\n478\n100\n37\n10\n2\n\n\n(250, 300]\n616\n113\n32\n4\n2\n\n\n(300, 350]\n656\n133\n23\n4\n1\n\n\n\n\n\n\n\n\n# making each column in days_reviews proportional to the sum of the number in each row\n\ndays_reviews['sum'] = days_reviews.sum(axis=1)\ndays_reviews = days_reviews.div(days_reviews['sum'], axis=0)\ndays_reviews.drop(columns='sum', inplace=True)\n\ndays_reviews\n\n\n\n\n\n\n\nreviews_per_year_bins\n(0, 25]\n(25, 50]\n(50, 75]\n(75, 100]\n(100, 125]\n\n\ndays_bins\n\n\n\n\n\n\n\n\n\n(0, 50]\n0.557789\n0.311558\n0.065327\n0.050251\n0.015075\n\n\n(50, 100]\n0.663121\n0.251773\n0.063830\n0.014184\n0.007092\n\n\n(100, 150]\n0.736577\n0.196309\n0.052013\n0.008389\n0.006711\n\n\n(150, 200]\n0.713355\n0.179153\n0.083062\n0.017915\n0.006515\n\n\n(200, 250]\n0.762360\n0.159490\n0.059011\n0.015949\n0.003190\n\n\n(250, 300]\n0.803129\n0.147327\n0.041721\n0.005215\n0.002608\n\n\n(300, 350]\n0.802938\n0.162791\n0.028152\n0.004896\n0.001224\n\n\n\n\n\n\n\nLooking at the betas calculated from our Poisson Regression Likelihood model, the first explanatory variable analyzed was days listed. Days listed seems to have a negative affect on the number of reviews, with a beta of -0.91. This is significant considering the other betas. The explanation for this may be that people like to see brand new listings since as time goes on, the listings have more wear and tear, thus review scores start to decrease. However, the other explanation is there is an omitted variable we are missing that was not captured in this dataset.\nIn the days_reviews table above, it shows the proportion of reviews_per_year in comparison to each days bin. It does seem like as the number of days increases, the number of reviews per year decreases with the exception of the first bin (0-25 reviews per year).\nBathrooms also has a slightly negative affect on the number of reviews, with a beta of -0.011. This is not as significant as some of the other variables, and may be leading to omitted variable bias. However, an explanation may be that rooms are more popular than homes for the individuals in this area. If they are staying short term, they may have a bathroom outside the room.\nBedrooms do have a positive effect on the number of reviews, with a beta of 0.10. Meaning an entire house or apartment is more likely to get more reviews than a shared room. This is significant, and may be due to the fact that people are more likely to stay in a place with more bedrooms if they are traveling with a group. They also may be more likely to leave a review if traveling with a group.\nPrice has a negative effect on reviews per year, with a beta of -0.088. This may be explained by the income class of those staying and the increased prices in NYC. Most people try and spend the least amount of money as possible in order to have a satisfactory experience.\nReview scores all have a positive effect on reviews_per_year with value having the largest affect. As stated above, most people want to spend the least amount of money for a satisfactory experience, so if a place is respectively inexpensive and has a high value rating by others, than that will attract others to stay.\nInstant bookable has a significant positive effect on reviews_per_year. This aligns with the American society values. People want to be able to have control at the touch of a button. They lose interest quickly if the have to wait and want immediate feedback. Instant bookings provide that instant feedback.\nPrivate and shared rooms have a slightly negative effect, which means that entire homes or apartments are more desirable. Although price is a concern and entire homes or apartments are more expensive, they provide more privacy and space for the guests. If the guests are traveling in groups, that makes this option more affordable, which may explain why it’s positive."
  },
  {
    "objectID": "projects/HW2/HW2.html",
    "href": "projects/HW2/HW2.html",
    "title": "Variable Definitions",
    "section": "",
    "text": "todo: Read in data.\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Load the data\n\ndata = pd.read_csv('blueprinty.csv')\n\ndata.head()\n\n\n\n\n\n\n\n\nUnnamed: 0\npatents\nregion\nage\niscustomer\n\n\n\n\n0\n1\n0\nMidwest\n32.5\n0\n\n\n1\n786\n3\nSouthwest\n37.5\n0\n\n\n2\n348\n4\nNorthwest\n27.0\n1\n\n\n3\n927\n3\nNortheast\n24.5\n0\n\n\n4\n830\n3\nSouthwest\n37.0\n0\n\n\n\n\n\n\n\n\n# coutn the number of unique values in the column 'Unnamed: 0'\n\ndata['Unnamed: 0'].nunique()\n\n1500\n\n\n\ndata.shape\n\n(1500, 5)\n\n\ntodo: Compare histograms and means of number of patents by customer status. What do you observe?\n\n# count number of customers and non-customers\n\ndata[data['iscustomer']==1]['iscustomer'].count()\n\n197\n\n\n\ndata[data['iscustomer']==0]['iscustomer'].count()\n\n1303\n\n\n\n# histogram of # of patents for Customers\n\n\ndata[data['iscustomer'] == 1]['patents'].hist()\n\nplt.xlabel('Number of Patents')\nplt.ylabel('Frequency')\nplt.title('Frequency of Patents for Customers')\n\nText(0.5, 1.0, 'Frequency of Patents for Customers')\n\n\n\n\n\n\n\n\n\n\n# make histogram of # of patents for Non-Customers\n\ndata[data['iscustomer'] == 0]['patents'].hist(color='orange')\nplt.xlabel('Number of Patents')\nplt.ylabel('Frequency')\nplt.title('Frequency of Patents for Non-Customers')\n\n\nText(0.5, 1.0, 'Frequency of Patents for Non-Customers')\n\n\n\n\n\n\n\n\n\nThe number of patents for customers is slightly skewed right, however it has a more normal distribution than the number of patents for non-customers.\nBoth plots have a large drop off around 6 patents (For customers it is slightly less than 6). The number of non-customers is significantly higher than the number of customers, totaling to 1303 non-customers and 197 customers. The mean number of patents for customers was 4.09 and the mean number of patents for non-customers was 3.62. This is a difference of about 0.5 patents.\n\nmean_patents_customers = data[data['iscustomer'] == 1]['patents'].mean()\nmean_patents_noncustomers = data[data['iscustomer'] == 0]['patents'].mean()\n\nprint('Mean patents for customers:', round(mean_patents_customers,2))\nprint('Mean patents for non-customers:', round(mean_patents_noncustomers,2))\n\nMean patents for customers: 4.09\nMean patents for non-customers: 3.62\n\n\ntodo: Compare regions and ages by customer status. What do you observe?\n\n# group by region and count number of customers and non-customers\n\nregion = data.groupby('region')['iscustomer'].value_counts().unstack()\n\n# Calculate respective proportion of non-customers and customers which make up each region\n\nregion['Prop_Non_cust'] = region[0]/(region[0].sum())\nregion['Prop_Cust'] = region[1]/(region[1].sum())\n\n# Rename columns from 0 to Non-Customers and 1 to Customers\n\nregion.rename(columns={0:'Non-Customers', 1:'Customers'}, inplace=True)\n\nregion\n\n\n\n\n\n\n\niscustomer\nNon-Customers\nCustomers\nProp_Non_cust\nProp_Cust\n\n\nregion\n\n\n\n\n\n\n\n\nMidwest\n207\n17\n0.158864\n0.086294\n\n\nNortheast\n488\n113\n0.374520\n0.573604\n\n\nNorthwest\n171\n16\n0.131236\n0.081218\n\n\nSouth\n171\n20\n0.131236\n0.101523\n\n\nSouthwest\n266\n31\n0.204144\n0.157360\n\n\n\n\n\n\n\n\n# rename column 0 to non-customers, 1 to customers\n\nregion.rename(columns={0:'Non-Customers', 1:'Customers'}, inplace=True)\n\n\n# create a plot of the number of customers and non-customers by region side by side\n\ndata.groupby('region')['iscustomer'].value_counts().unstack().plot(kind='bar', stacked=False)\n\nplt.xlabel('Region')\nplt.ylabel('Count')\nplt.title('Number of Customers and Non-Customers by Region')\nplt.legend(['Non-Customer', 'Customer'])\n\n\n\n\n\n\n\n\nThe Northeast has by far the most customers and non-customers, and the number of non-customers in each region clearly outweighs the number of customers. Although ranking each respective customer and non-customer base by region comes out to be nearly the same ranking, the proportions are different.\nFor the customers, nearly 60% are from the NE, while only 40% of non-customers are from the NE.\n\n# group by age with bins every 5 years and count number of customers and non-customers\n\ndata['age_bins'] = pd.cut(data['age'], bins=range(0, 60, 5))\n\nage = data.groupby('age_bins')['iscustomer'].value_counts().unstack()\nage['Prop_Non_cust'] = age[0]/(age[0].sum())\nage['Prop_Cust'] = age[1]/(age[1].sum())\n\nage.rename(columns={0:'Non-Customers', 1:'Customers'}, inplace=True)\n\nage\n\n/tmp/ipykernel_27929/4171049143.py:5: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n  age = data.groupby('age_bins')['iscustomer'].value_counts().unstack()\n\n\n\n\n\n\n\n\niscustomer\nNon-Customers\nCustomers\nProp_Non_cust\nProp_Cust\n\n\nage_bins\n\n\n\n\n\n\n\n\n(0, 5]\n0\n0\n0.000000\n0.000000\n\n\n(5, 10]\n3\n3\n0.002302\n0.015228\n\n\n(10, 15]\n52\n15\n0.039908\n0.076142\n\n\n(15, 20]\n213\n53\n0.163469\n0.269036\n\n\n(20, 25]\n318\n51\n0.244052\n0.258883\n\n\n(25, 30]\n301\n32\n0.231005\n0.162437\n\n\n(30, 35]\n244\n24\n0.187260\n0.121827\n\n\n(35, 40]\n135\n13\n0.103607\n0.065990\n\n\n(40, 45]\n34\n5\n0.026094\n0.025381\n\n\n(45, 50]\n3\n1\n0.002302\n0.005076\n\n\n(50, 55]\n0\n0\n0.000000\n0.000000\n\n\n\n\n\n\n\n\n# create a plot of the number of customers and non-customers by age side by side\n\ndata.groupby('age_bins')['iscustomer'].value_counts().unstack().plot(kind='bar', stacked=False)\nplt.xlabel('Age of Company')\nplt.ylabel('Count')\nplt.title('Number of Customers and Non-Customers by Age of Company')\nplt.legend(['Non-Customer', 'Customer'])\n\n/tmp/ipykernel_27929/196641801.py:3: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n  data.groupby('age_bins')['iscustomer'].value_counts().unstack().plot(kind='bar', stacked=False)\n\n\n\n\n\n\n\n\n\n\n# find mean age for customers and non-customers\n\nmean_age_customers = data[data['iscustomer'] == 1]['age'].mean()\nmean_age_noncustomers = data[data['iscustomer'] == 0]['age'].mean()\n\n\nprint('Mean age for customers:', round(mean_age_customers,2))\nprint('Mean age for non-customers:', round(mean_age_noncustomers,2))\n\nMean age for customers: 24.15\nMean age for non-customers: 26.69\n\n\n\n# Calculate a 95% confidence interval for the mean age of customers and non-customers\n\nimport numpy as np\n\nstd_age_customers = data[data['iscustomer'] == 1]['age'].std()\nstd_age_noncustomers = data[data['iscustomer'] == 0]['age'].std()\n\nn_customers = data[data['iscustomer'] == 1]['age'].count()\nn_noncustomers = data[data['iscustomer'] == 0]['age'].count()\n\nz = 1.96\n\nci_customers = z * (std_age_customers/np.sqrt(n_customers))\nci_noncustomers = z * (std_age_noncustomers/np.sqrt(n_noncustomers))\n\nprint('95% CI for mean age of customers:', round(mean_age_customers-ci_customers,2), round(mean_age_customers+ci_customers,2))\nprint('95% CI for mean age of non-customers:', round(mean_age_noncustomers-ci_noncustomers,2), round(mean_age_noncustomers+ci_noncustomers,2))\n\n95% CI for mean age of customers: 23.09 25.21\n95% CI for mean age of non-customers: 26.3 27.08\n\n\nThe most customers come from companies which are between 15-20 years old, with the average company age being about 24 years. The most non-customers come from companies which are between 20-25 years old, with the average company age being about 27 years.\nThe distribution of company ages for customers and non-customers resembles a normal distribution, with a slight skew to the right. The largest disparity between the two distributions is at the 10-15 year mark, where the % of non-customers is 10% lower than that of customers (16%, 26%).\nThe confidence interval for the mean age of customers is (23.1, 25.2) and the confidence interval for the mean age of non-customers is (26.3, 27.1).\nWrite down mathematically the likelihood for_ \\(Y \\sim \\text{Poisson}(\\lambda)\\). Note that \\(f(Y|\\lambda) = e^{-\\lambda}\\lambda^Y/Y!\\).\nBelow is the log-likelihood function for the Poisson distribution. The likelihood function of lambda given Y is the exact same as the function of Y given lambda.\nℓ(λ∣Y)=−λ+Ylog(λ)−log(Y!)\nCode the likelihood (or log-likelihood) function for the Poisson model. This is a function of lambda and Y. For example:_\n\ndef poisson_log_likelihood(lam, y):\n    \"\"\"\n    Parameters:\n    - lam (float): The rate parameter (lambda) of the Poisson distribution.\n    - y (array-like): Array of observed counts.\n\n    Returns:\n    - float: The log likelihood of observing the data given lam.\n    \"\"\"\n    y = np.array(y)\n    n = len(y)  # number of observations\n    sum_y = np.sum(y)  # sum of all observed counts\n\n    # Calculate each part of the log likelihood\n    # log(P(Y|lam)) = -n * lam + sum_y * log(lam) - log(y_i!)\n    # We use np.sum(np.log(y_factorials)) to sum log of factorials\n    log_likelihood = -n * lam + sum_y * np.log(lam) - np.sum([np.log(np.math.factorial(i)) for i in y])\n    return log_likelihood\n\n\nyears = range(1,11)\nlog_likelihood_values = []\n\nfor i in years:\n    log_likelihood_value = poisson_log_likelihood(i, data['patents'])\n    log_likelihood_values.append(log_likelihood_value)\n\nlog_likelihood_values\n\n/tmp/ipykernel_27929/1539929848.py:17: DeprecationWarning: `np.math` is a deprecated alias for the standard library `math` module (Deprecated Numpy 1.25). Replace usages of `np.math` with `math`\n  log_likelihood = -n * lam + sum_y * np.log(lam) - np.sum([np.log(np.math.factorial(i)) for i in y])\n\n\n[-6548.886990069443,\n -4217.862523114625,\n -3476.856870600801,\n -3386.838056159808,\n -3653.52364804617,\n -4145.8324036459835,\n -4793.841596240727,\n -5555.81358920499,\n -6404.826751132159,\n -7322.4991810913525]\n\n\n_todo: Use your function to plot lambda on the horizontal axis and the likelihood (or log-likelihood) on the vertical axis for a range of lambdas (use the observed number of patents as the input for Y).\n\n# _todo: Use your function to plot lambda on the horizontal axis and the likelihood (or log-likelihood) on the vertical axis for a range of lambdas (use the observed number of patents as the input for Y).\n\nplt.plot(years, log_likelihood_values)\nplt.xlabel('Years (lambda)')\nplt.ylabel('Likelihood')\nplt.title('Likelihood of observing the data given Years')\nplt.show()\n\n\n\n\n\n\n\n\n\nfrom scipy.optimize import minimize\n\ndef neg_poisson_log_likelihood(lam, y):\n    \"\"\"\n    Compute the log likelihood of observing data y under a Poisson distribution with parameter lam.\n\n    Parameters:\n    - lam (float): The rate parameter (lambda) of the Poisson distribution.\n    - y (array-like): Array of observed counts.\n\n    Returns:\n    - float: The log likelihood of observing the data given lam.\n    \"\"\"\n    y = np.array(y)\n    n = len(y)  # number of observations\n    sum_y = np.sum(y)  # sum of all observed counts\n\n    # Calculate each part of the log likelihood\n    # log(P(Y|lam)) = -n * lam + sum_y * log(lam) - log(y_i!)\n    # We use np.sum(np.log(y_factorials)) to sum log of factorials\n    return -(-n * lam + sum_y * np.log(lam) - np.sum([np.log(np.math.factorial(i)) for i in y]))\n    \n\n\nmean = np.mean(data['patents'])\n\nresult = minimize(neg_poisson_log_likelihood, mean, args=(data['patents']), bounds = [(0, None)])\n\nresult\n\n/tmp/ipykernel_27929/354120658.py:21: DeprecationWarning: `np.math` is a deprecated alias for the standard library `math` module (Deprecated Numpy 1.25). Replace usages of `np.math` with `math`\n  return -(-n * lam + sum_y * np.log(lam) - np.sum([np.log(np.math.factorial(i)) for i in y]))\n\n\n  message: CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_&lt;=_PGTOL\n  success: True\n   status: 0\n      fun: 3367.683772235094\n        x: [ 3.685e+00]\n      nit: 0\n      jac: [ 0.000e+00]\n     nfev: 2\n     njev: 1\n hess_inv: &lt;1x1 LbfgsInvHessProduct with dtype=float64&gt;\n\n\nNext, we extend our simple Poisson model to a Poisson Regression Model such that \\(Y_i = \\text{Poisson}(\\lambda_i)\\) where \\(\\lambda_i = \\exp(X_i'\\beta)\\). The interpretation is that the success rate of patent awards is not constant across all firms (\\(\\lambda\\)) but rather is a function of firm characteristics \\(X_i\\). Specifically, we will use the covariates age, age squared, region, and whether the firm is a customer of Blueprinty.\ntodo: Update your likelihood or log-likelihood function with an additional argument to take in a covariate matrix X. Also change the parameter of the model from lambda to the beta vector. In this model, lambda must be a positive number, so we choose the inverse link function g() to be exp() so that \\(\\lambda_i = e^{X_i'\\beta}\\). For example:\npoisson_regression_likelihood &lt;- function(beta, Y, X){\n   ...\n}\n\n::: {#cell-33 .cell execution_count=73}\n``` {.python .cell-code}\nimport numpy as np\nfrom scipy.optimize import minimize\nfrom scipy.special import factorial\n\ndef poisson_regression_log_likelihood(beta, Y, X):\n \n    # Calculate lambda for each observation\n    lambda_i = np.exp(np.dot(X, beta))\n    \n    # Calculate the log likelihood\n    log_likelihood = np.sum(Y * np.log(lambda_i) - lambda_i - np.log(factorial(Y)))\n    \n    # Return the negative log likelihood\n    return -log_likelihood\n:::\ntodo: Use your function along with R’s optim() or Python’s sp.optimize() to find the MLE vector and the Hessian of the Poisson model with covariates. Specifically, the first column of X should be all 1’s to enable a constant term in the model, and the subsequent columns should be age, age squared, binary variables for all but one of the regions, and the binary customer variable. Use the Hessian to find standard errors of the beta parameter estimates and present a table of coefficients and standard errors.\n\n# convert regions column to binary columns\n\ndata = pd.get_dummies(data, columns=['region'], drop_first=True)\n\n\n# creating function to convert boolean column to binary\n\ndef convert_boolean_to_binary(data, column):\n    data[column] = data[column].astype(int)\n    return data\n\n# coverting region's boolean values to binary\n\ndata = convert_boolean_to_binary(data, 'region_Northeast')\ndata = convert_boolean_to_binary(data, 'region_South')\ndata = convert_boolean_to_binary(data, 'region_Southwest')\ndata = convert_boolean_to_binary(data, 'region_Northwest')\n\n\ndata['age_squared'] = data['age']**2\n\n\n\nn = len(data)  # Number of observations\nX = np.c_[np.ones(n).tolist(), data['age'], data['age_squared'], data['iscustomer'], data['region_Southwest'], data['region_Northwest'],\n          data['region_Northeast'], data['region_South']]\nY = data['patents']\n\n# Initial guess for beta\ninitial_beta = np.zeros(X.shape[1])\n\n# Minimization\nresult = minimize(poisson_regression_log_likelihood, initial_beta, args = (Y, X), method= 'BFGS')\n\nprint(\"Optimal beta:\", result.x)\nprint(\"Success:\", result.success)\nprint(\"Message:\", result.message)\n\nOptimal beta: [0. 0. 0. 0. 0. 0. 0. 0.]\nSuccess: False\nMessage: Desired error not necessarily achieved due to precision loss.\n\n\n/tmp/ipykernel_27929/1165065463.py:8: RuntimeWarning: overflow encountered in exp\n  lambda_i = np.exp(np.dot(X, beta))\n/opt/conda/lib/python3.11/site-packages/numpy/core/_methods.py:49: RuntimeWarning: overflow encountered in reduce\n  return umr_sum(a, axis, dtype, out, keepdims, initial, where)\n/opt/conda/lib/python3.11/site-packages/scipy/optimize/_numdiff.py:576: RuntimeWarning: invalid value encountered in subtract\n  df = fun(x) - f0\n/tmp/ipykernel_27929/1165065463.py:8: RuntimeWarning: overflow encountered in exp\n  lambda_i = np.exp(np.dot(X, beta))\n/opt/conda/lib/python3.11/site-packages/numpy/core/_methods.py:49: RuntimeWarning: overflow encountered in reduce\n  return umr_sum(a, axis, dtype, out, keepdims, initial, where)\n\n\n\nimport numpy as np\nfrom scipy.optimize import minimize\n\n\ndef poisson_regression_log_likelihood(beta, Y, X):\n    eta = np.dot(X, beta)\n    lambda_i = np.exp(eta)\n    log_likelihood = np.sum(Y * eta - lambda_i)\n    return -log_likelihood\n\n# Load and preprocess your data as before, ensuring that features are scaled\nfrom sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\n# Assuming 'age' and 'age_squared' need scaling\ndata['age'] = scaler.fit_transform(data[['age']])\ndata['age_squared'] = scaler.fit_transform(data[['age_squared']])\n\n# Assuming that you already converted regions into dummy variables and set up the data correctly\nX = np.c_[np.ones(len(data)), data['age'], data['age_squared'], data['iscustomer'], data['region_Southwest'], data['region_Northwest'],\n          data['region_Northeast'], data['region_South']]\nY = data['patents'].values\n\n# Initial guess for beta\ninitial_beta = np.zeros(X.shape[1])\n\n# Minimization using a more controlled approach\nresult = minimize(poisson_regression_log_likelihood, initial_beta, args=(Y, X), method='BFGS')\n\nprint(\"Optimal beta:\", result.x)\nprint(\"Success:\", result.success)\nprint(\"Message:\", result.message)\n\nOptimal beta: [ 1.21543809  1.04645982 -1.14084528  0.11811438  0.05134699 -0.02009421\n  0.098596    0.05717198]\nSuccess: False\nMessage: Desired error not necessarily achieved due to precision loss.\n\n\n\n# _todo: Check your results using R's glm() function or Python sm.GLM() function._\n\nimport statsmodels.api as sm\n\n# Fit a Poisson regression model using statsmodels\n\npoisson_model = sm.GLM(Y, X, family=sm.families.Poisson()).fit()\n\n# Print the summary of the model\n\nprint(poisson_model.summary())\n\n                 Generalized Linear Model Regression Results                  \n==============================================================================\nDep. Variable:                      y   No. Observations:                 1500\nModel:                            GLM   Df Residuals:                     1492\nModel Family:                 Poisson   Df Model:                            7\nLink Function:                    Log   Scale:                          1.0000\nMethod:                          IRLS   Log-Likelihood:                -3275.9\nDate:                Wed, 01 May 2024   Deviance:                       2178.8\nTime:                        17:06:55   Pearson chi2:                 2.11e+03\nNo. Iterations:                     5   Pseudo R-squ. (CS):             0.1152\nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          z      P&gt;|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst          1.2154      0.036     33.368      0.000       1.144       1.287\nx1             1.0465      0.100     10.414      0.000       0.850       1.243\nx2            -1.1408      0.102    -11.131      0.000      -1.342      -0.940\nx3             0.1181      0.039      3.035      0.002       0.042       0.194\nx4             0.0513      0.047      1.088      0.277      -0.041       0.144\nx5            -0.0201      0.054     -0.374      0.709      -0.126       0.085\nx6             0.0986      0.042      2.347      0.019       0.016       0.181\nx7             0.0572      0.053      1.085      0.278      -0.046       0.160\n==============================================================================\n\n\n\n# Calculate Hessian at the optimal beta\nfrom scipy.linalg import inv\nhessian_inv = result.hess_inv  # Inverse Hessian is returned by BFGS\n\n# Standard errors are the square roots of the diagonal elements of the inverse Hessian\nstd_errors = np.sqrt(np.diag(hessian_inv))\n\nprint(\"Standard Errors:\", std_errors)\n\nStandard Errors: [0.02272912 0.14333007 0.14451598 0.03888018 0.03676005 0.04672971\n 0.02526631 0.02849847]\n\n\nWe can conclude, based on our optimal beta’s through our regression model, that Blueprinty’s software has a positive effect on the number of patents awarded to a company. The coefficient or beta calculated for “iscustomer” is 0.11, meaning if they are a customer of the software, the humber of patents earned increases by 0.11.\nWe also see that p-value of “iscustomer” is 0.002, meaning there is only a 0.2% chance that the coefficient has zero affect on the number of patents given the dataset.\nAirBnB is a popular platform for booking short-term rentals. In March 2017, students Annika Awad, Evan Lebo, and Anna Linden scraped of 40,000 Airbnb listings from New York City. The data include the following variables:\n\n\n\n\n\n\nNote\n\n\n\n\n\n- `id` = unique ID number for each unit\n- `last_scraped` = date when information scraped\n- `host_since` = date when host first listed the unit on Airbnb\n- `days` = `last_scraped` - `host_since` = number of days the unit has been listed\n- `room_type` = Entire home/apt., Private room, or Shared room\n- `bathrooms` = number of bathrooms\n- `bedrooms` = number of bedrooms\n- `price` = price per night (dollars)\n- `number_of_reviews` = number of reviews for the unit on Airbnb\n- `review_scores_cleanliness` = a cleanliness score from reviews (1-10)\n- `review_scores_location` = a \"quality of location\" score from reviews (1-10)\n- `review_scores_value` = a \"quality of value\" score from reviews (1-10)\n- `instant_bookable` = \"t\" if instantly bookable, \"f\" if not\n\n\n\ntodo: Assume the number of reviews is a good proxy for the number of bookings. Perform some exploratory data analysis to get a feel for the data, handle or drop observations with missing values on relevant variables, build one or more models (e.g., a poisson regression model for the number of bookings as proxied by the number of reviews), and interpret model coefficients to describe variation in the number of reviews as a function of the variables provided.\n\n# read airbnb csv\n\nairbnb = pd.read_csv('airbnb.csv')\n\nairbnb.head()\n\n\n\n\n\n\n\n\nUnnamed: 0\nid\ndays\nlast_scraped\nhost_since\nroom_type\nbathrooms\nbedrooms\nprice\nnumber_of_reviews\nreview_scores_cleanliness\nreview_scores_location\nreview_scores_value\ninstant_bookable\n\n\n\n\n0\n1\n2515\n3130\n4/2/2017\n9/6/2008\nPrivate room\n1.0\n1.0\n59\n150\n9.0\n9.0\n9.0\nf\n\n\n1\n2\n2595\n3127\n4/2/2017\n9/9/2008\nEntire home/apt\n1.0\n0.0\n230\n20\n9.0\n10.0\n9.0\nf\n\n\n2\n3\n3647\n3050\n4/2/2017\n11/25/2008\nPrivate room\n1.0\n1.0\n150\n0\nNaN\nNaN\nNaN\nf\n\n\n3\n4\n3831\n3038\n4/2/2017\n12/7/2008\nEntire home/apt\n1.0\n1.0\n89\n116\n9.0\n9.0\n9.0\nf\n\n\n4\n5\n4611\n3012\n4/2/2017\n1/2/2009\nPrivate room\nNaN\n1.0\n39\n93\n9.0\n8.0\n9.0\nt\n\n\n\n\n\n\n\n\nairbnb.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 40628 entries, 0 to 40627\nData columns (total 14 columns):\n #   Column                     Non-Null Count  Dtype  \n---  ------                     --------------  -----  \n 0   Unnamed: 0                 40628 non-null  int64  \n 1   id                         40628 non-null  int64  \n 2   days                       40628 non-null  int64  \n 3   last_scraped               40628 non-null  object \n 4   host_since                 40593 non-null  object \n 5   room_type                  40628 non-null  object \n 6   bathrooms                  40468 non-null  float64\n 7   bedrooms                   40552 non-null  float64\n 8   price                      40628 non-null  int64  \n 9   number_of_reviews          40628 non-null  int64  \n 10  review_scores_cleanliness  30433 non-null  float64\n 11  review_scores_location     30374 non-null  float64\n 12  review_scores_value        30372 non-null  float64\n 13  instant_bookable           40628 non-null  object \ndtypes: float64(5), int64(5), object(4)\nmemory usage: 4.3+ MB\n\n\n\nairbnb.shape\n\n(40628, 14)\n\n\n\nairbnb.describe()\n\n\n\n\n\n\n\n\nUnnamed: 0\nid\ndays\nbathrooms\nbedrooms\nprice\nnumber_of_reviews\nreview_scores_cleanliness\nreview_scores_location\nreview_scores_value\n\n\n\n\ncount\n40628.000000\n4.062800e+04\n40628.000000\n40468.000000\n40552.000000\n40628.000000\n40628.000000\n30433.000000\n30374.000000\n30372.000000\n\n\nmean\n20314.500000\n9.698889e+06\n1102.368219\n1.124592\n1.147046\n144.760732\n15.904426\n9.198370\n9.413544\n9.331522\n\n\nstd\n11728.437705\n5.460166e+06\n1383.269358\n0.385884\n0.691746\n210.657597\n29.246009\n1.119935\n0.844949\n0.902966\n\n\nmin\n1.000000\n2.515000e+03\n1.000000\n0.000000\n0.000000\n10.000000\n0.000000\n2.000000\n2.000000\n2.000000\n\n\n25%\n10157.750000\n4.889868e+06\n542.000000\n1.000000\n1.000000\n70.000000\n1.000000\n9.000000\n9.000000\n9.000000\n\n\n50%\n20314.500000\n9.862878e+06\n996.000000\n1.000000\n1.000000\n100.000000\n4.000000\n10.000000\n10.000000\n10.000000\n\n\n75%\n30471.250000\n1.466789e+07\n1535.000000\n1.000000\n1.000000\n170.000000\n17.000000\n10.000000\n10.000000\n10.000000\n\n\nmax\n40628.000000\n1.800967e+07\n42828.000000\n8.000000\n10.000000\n10000.000000\n421.000000\n10.000000\n10.000000\n10.000000\n\n\n\n\n\n\n\n\n# create subset of data which only shows bedrooms which are nan\n\nairbnb[airbnb['room_type']=='Entire home/apt'].isnull().sum()\n\n# drop Entire home/apt that are null for bedrooms or bathrooms\n\nairbnb = airbnb.dropna(subset=['bedrooms', 'bathrooms'])\n\nUnnamed: 0                      0\nid                              0\ndays                            0\nlast_scraped                    0\nhost_since                     19\nroom_type                       0\nbathrooms                      41\nbedrooms                       55\nprice                           0\nnumber_of_reviews               0\nreview_scores_cleanliness    4208\nreview_scores_location       4228\nreview_scores_value          4230\ninstant_bookable                0\ndtype: int64\n\n\n\n# fill bedrooms with 1 and bathrooms with 0 if nan and if room type is private or shared room\n\nairbnb.loc[(airbnb['bedrooms'].isnull()) & (airbnb['room_type'] == 'Private room'), 'bedrooms'] = 1\nairbnb.loc[(airbnb['bedrooms'].isnull()) & (airbnb['room_type'] == 'Shared room'), 'bedrooms'] = 1\nairbnb.loc[(airbnb['bathrooms'].isnull()) & (airbnb['room_type'] == 'Private room'), 'bathrooms'] = 0\nairbnb.loc[(airbnb['bathrooms'].isnull()) & (airbnb['room_type'] == 'Private room'), 'bathrooms'] = 0\n\n\n# count number of 0 value reviews\nairbnb[airbnb['number_of_reviews'] == 0]['number_of_reviews'].count()\n\n9481\n\n\n\n# convert bedrooms NaN to 0\n\nairbnb['bedrooms'] = airbnb['bedrooms'].fillna(0)\nairbnb['bathrooms'] = airbnb['bathrooms'].fillna(0)\n\n# convert all columns with review_score as value of NaN to 0\n\nairbnb['review_scores_value'] = airbnb['review_scores_value'].fillna(0)\nairbnb['review_scores_location'] = airbnb['review_scores_location'].fillna(0)\nairbnb['review_scores_cleanliness'] = airbnb['review_scores_cleanliness'].fillna(0)\n\n# convert room_type to boolean\n\nairbnb = pd.get_dummies(airbnb, columns=['room_type'], drop_first=True)\n\n\nfor i in airbnb['instant_bookable']:\n    if i == 't':\n        airbnb['instant_bookable'] = 1\n    else:\n        airbnb['instant_bookable'] = 0\n\n\n# convert room_type_Private_room to binary\n\nairbnb['room_type_Private room'] = airbnb['room_type_Private room'].astype(int)\nairbnb['room_type_Shared room'] = airbnb['room_type_Shared room'].astype(int)\n\nThe average number of reviews is 15.9 for this dataset, however the standard deviation is 29.25, meaning there is a large spread in the number of reviews, with many locations having 0 reviews (Right skewed)\nMost of the listings in the dataset rank very high in cleanliness, location, and value, with all having an average of above 9 out of 10.\nThe price of the listings is also left skewed, with the average price being $145, but the standard deviation being $211, meaning there is a large spread in the price of listings like the number of reviews. However with this spread, due to the fact that the price can’t be negative, the distribution is left skewed.\n\nairbnb['reviews_per_year'] = airbnb['number_of_reviews'] / airbnb['days'] * 365\n\n\n# Load and preprocess your data as before, ensuring that features are scaled\nfrom sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\n# Assuming 'age' and 'age_squared' need scaling\ndata['age'] = scaler.fit_transform(data[['age']])\ndata['age_squared'] = scaler.fit_transform(data[['age_squared']])\n\n\nairbnb.columns\n\n\ncolumns = ['days', 'bathrooms', 'bedrooms', 'price', \n           'review_scores_cleanliness', 'review_scores_location', \n           'review_scores_value', 'instant_bookable', \n           'room_type_Private room', 'room_type_Shared room']\n\n\n# from the days column, drop all rows with a value of 0\n\nairbnb = airbnb[airbnb['days'] != 0]\n\n\nimport numpy as np\nfrom scipy.optimize import minimize\n\n\n\nfrom sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\n# Assuming 'age' and 'age_squared' need scaling\nairbnb['days_scaled'] = scaler.fit_transform(airbnb[['days']])\nairbnb['price'] = scaler.fit_transform(airbnb[['price']])\nairbnb['review_scores_cleanliness'] = scaler.fit_transform(airbnb[['review_scores_cleanliness']])\nairbnb['review_scores_location'] = scaler.fit_transform(airbnb[['review_scores_location']])\nairbnb['review_scores_value'] = scaler.fit_transform(airbnb[['review_scores_value']])\n\n\n# Assuming that you already converted regions into dummy variables and set up the data correctly\nX = np.c_[np.ones(len(airbnb)), airbnb['days_scaled'], airbnb['bathrooms'], airbnb['bedrooms'], airbnb['price'], \n          airbnb['review_scores_cleanliness'], airbnb['review_scores_location'], airbnb['review_scores_value'], \n          airbnb['instant_bookable'], airbnb['room_type_Private room'], airbnb['room_type_Shared room']]\nY = airbnb['reviews_per_year'].values\n\n# Initial guess for beta\ninitial_beta = np.zeros(X.shape[1])\n\n# Minimization using a more controlled approach\nresult = minimize(poisson_regression_log_likelihood, initial_beta, args=(Y, X), method='BFGS')\n\nprint(\"Optimal beta:\", result.x)\nprint(\"Success:\", result.success)\nprint(\"Message:\", result.message)\n\nOptimal beta: [ 0.60331738 -0.91467491 -0.01100322  0.10389168 -0.08852837  0.76816195\n  0.39240359  0.17957291  0.60331738 -0.00616376 -0.04999207]\nSuccess: False\nMessage: Desired error not necessarily achieved due to precision loss.\n\n\n\n# group the data based on days (binning the days) and reviews_per_year (binning reviews per year)\n\nairbnb['days_bins'] = pd.cut(airbnb['days'], bins=range(0, 400, 50))\n\nairbnb['reviews_per_year_bins'] = pd.cut(airbnb['reviews_per_year'], bins=range(0, 150, 25))\n\ndays_reviews = airbnb.groupby('days_bins')['reviews_per_year_bins'].value_counts().unstack()\n\ndays_reviews\n\n/tmp/ipykernel_52532/3527047454.py:7: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n  days_reviews = airbnb.groupby('days_bins')['reviews_per_year_bins'].value_counts().unstack()\n\n\n\n\n\n\n\n\nreviews_per_year_bins\n(0, 25]\n(25, 50]\n(50, 75]\n(75, 100]\n(100, 125]\n\n\ndays_bins\n\n\n\n\n\n\n\n\n\n(0, 50]\n114\n62\n13\n10\n3\n\n\n(50, 100]\n200\n71\n18\n4\n2\n\n\n(100, 150]\n452\n117\n31\n5\n4\n\n\n(150, 200]\n456\n110\n51\n11\n4\n\n\n(200, 250]\n494\n100\n37\n10\n2\n\n\n(250, 300]\n647\n113\n33\n4\n2\n\n\n(300, 350]\n685\n133\n23\n4\n1\n\n\n\n\n\n\n\n\n# make each column in days_reviews proportional to the sum of the number in each row\n\ndays_reviews['sum'] = days_reviews.sum(axis=1)\ndays_reviews = days_reviews.div(days_reviews['sum'], axis=0)\ndays_reviews.drop(columns='sum', inplace=True)\n\ndays_reviews\n\n\n\n\n\n\n\nreviews_per_year_bins\n(0, 25]\n(25, 50]\n(50, 75]\n(75, 100]\n(100, 125]\n\n\ndays_bins\n\n\n\n\n\n\n\n\n\n(0, 50]\n0.564356\n0.306931\n0.064356\n0.049505\n0.014851\n\n\n(50, 100]\n0.677966\n0.240678\n0.061017\n0.013559\n0.006780\n\n\n(100, 150]\n0.742200\n0.192118\n0.050903\n0.008210\n0.006568\n\n\n(150, 200]\n0.721519\n0.174051\n0.080696\n0.017405\n0.006329\n\n\n(200, 250]\n0.768274\n0.155521\n0.057543\n0.015552\n0.003110\n\n\n(250, 300]\n0.809762\n0.141427\n0.041302\n0.005006\n0.002503\n\n\n(300, 350]\n0.809693\n0.157210\n0.027187\n0.004728\n0.001182\n\n\n\n\n\n\n\nLooking at the betas calculated from our Poisson Regression Likelihood model, the first explanatory variable analyzed was days listed. Days listed seems to have a negative affect on the number of reviews, with a beta of -0.91. This is significant considering the other betas. The explanation for this may be that people like to see brand new listings since as time goes on, the listings have more wear and tear, thus review scores start to decrease. However, the other explanation is there is an omitted variable we are missing that was not captured in this dataset.\nIn the days_reviews table above, it shows the proportion of reviews_per_year in comparison to each days bin. It does seem like as the number of days increases, the number of reviews per year decreases with the exception of the first bin (0-25 reviews per year).\nBathrooms also has a slightly negative affect on the number of reviews, with a beta of -0.011. This is not as significant as some of the other variables, and may be leading to omitted variable bias. However, an explanation may be that rooms are more popular than homes for the individuals in this area. If they are staying short term, they may have a bathroom outside the room.\nBedrooms do have a positive effect on the number of reviews, with a beta of 0.10. Meaning an entire house or apartment is more likely to get more reviews than a shared room. This is significant, and may be due to the fact that people are more likely to stay in a place with more bedrooms if they are traveling with a group. They also may be more likely to leave a review if traveling with a group.\nPrice has a negative effect on reviews per year, with a beta of -0.088. This may be explained by the income class of those staying and the increased prices in NYC. Most people try and spend the least amount of money as possible in order to have a satisfactory experience.\nReview scores all have a positive effect on reviews_per_year with value having the largest affect. As stated above, most people want to spend the least amount of money for a satisfactory experience, so if a place is respectively inexpensive and has a high value rating by others, than that will attract others to stay.\nInstant bookable has a significant positive effect on reviews_per_year. This aligns with the American society values. People want to be able to have control at the touch of a button. They lose interest quickly if the have to wait and want immediate feedback. Instant bookings provide that instant feedback.\nPrivate and shared rooms have a slightly negative effect, which means that entire homes or apartments are more desirable. Although price is a concern and entire homes or apartments are more expensive, they provide more privacy and space for the guests. If the guests are traveling in groups, that makes this option more affordable, which may explain why it’s positive."
  }
]