[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Nick Shuckerow",
    "section": "",
    "text": "Welcome to my website!\nI am a Systems Engineer for Raytheon"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "resume.html",
    "href": "resume.html",
    "title": "Nick Shuckerow’s Resume",
    "section": "",
    "text": "Download PDF file."
  },
  {
    "objectID": "projects/project1/index.html",
    "href": "projects/project1/index.html",
    "title": "Analysis of Cars",
    "section": "",
    "text": "Let’s investigate the relationship between fuel efficiency (mpg) and engine displacement (disp) from the mtcars dataset. Those variables have a correlation of -0.85.\n\n\nHere is a plot:\n\nlibrary(tidyverse)\ndata(mtcars)\nmtcars |&gt;\n  ggplot(aes(mpg, disp)) + \n  geom_point(color=\"dodgerblue4\", size=2)"
  },
  {
    "objectID": "projects/project1/index.html#sub-header",
    "href": "projects/project1/index.html#sub-header",
    "title": "Analysis of Cars",
    "section": "",
    "text": "Here is a plot:\n\nlibrary(tidyverse)\ndata(mtcars)\nmtcars |&gt;\n  ggplot(aes(mpg, disp)) + \n  geom_point(color=\"dodgerblue4\", size=2)"
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "My Projects",
    "section": "",
    "text": "A Replication of Karlan and List (2007)\n\n\n\n\n\n\nNick Shuckerow\n\n\nMay 29, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nAnalysis of Cars\n\n\n\n\n\n\nYour Name\n\n\nMay 29, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nKey Drivers Analysis\n\n\n\n\n\n\nNicholas Shuckerow\n\n\nMay 29, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nPearson Correlations\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPoisson\n\n\n\n\n\n\nNicholas Shuckerow\n\n\nMay 29, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nPoisson Regression Examples\n\n\n\n\n\n\nNicholas Shuckerow\n\n\nMay 29, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nVariable Definitions\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "projects/HW1/hw1_questions.html",
    "href": "projects/HW1/hw1_questions.html",
    "title": "A Replication of Karlan and List (2007)",
    "section": "",
    "text": "Dean Karlan at Yale and John List at the University of Chicago conducted a field experiment to test the effectiveness of different fundraising letters. They sent out 50,000 fundraising letters to potential donors, randomly assigning each letter to one of three treatments: a standard letter, a matching grant letter, or a challenge grant letter. They published the results of this experiment in the American Economic Review in 2007. The article and supporting data are available from the AEA website and from Innovations for Poverty Action as part of Harvard’s Dataverse.\nThe experiment was conducted through a direct mail campaign for a liberal nonprofit organization. The authors assigned 67% of the 50,000 to the treatment group and 33% to the control group.\nThe letters sent to the treatment and control group were the same except in the following regards:\n\nThe treatment group letter had an additional paragraph saying that their donation will be matched.\nThe reply card for the treatment group had details on the matching grant.\n\nThe letters were randomized through 3 different dimensions.\n\nmatching price ratio\nmaximum match amount\nsuggested donation amount\n\nThe experiment found that using matching donations increases the revenue per solicitation and the probability that someone donates.\nIt also found that larger match ratios relative to smaller match ratios have no additional impact. This project seeks to replicate their results.\nOverall, the authors found a unique way to measure the scope of a public good (donations) in a real world setting, rather than hypothetical scenarios. This is a valuable contribution to the literature on charitable giving and public goods."
  },
  {
    "objectID": "projects/HW1/hw1_questions.html#introduction",
    "href": "projects/HW1/hw1_questions.html#introduction",
    "title": "A Replication of Karlan and List (2007)",
    "section": "",
    "text": "Dean Karlan at Yale and John List at the University of Chicago conducted a field experiment to test the effectiveness of different fundraising letters. They sent out 50,000 fundraising letters to potential donors, randomly assigning each letter to one of three treatments: a standard letter, a matching grant letter, or a challenge grant letter. They published the results of this experiment in the American Economic Review in 2007. The article and supporting data are available from the AEA website and from Innovations for Poverty Action as part of Harvard’s Dataverse.\nThe experiment was conducted through a direct mail campaign for a liberal nonprofit organization. The authors assigned 67% of the 50,000 to the treatment group and 33% to the control group.\nThe letters sent to the treatment and control group were the same except in the following regards:\n\nThe treatment group letter had an additional paragraph saying that their donation will be matched.\nThe reply card for the treatment group had details on the matching grant.\n\nThe letters were randomized through 3 different dimensions.\n\nmatching price ratio\nmaximum match amount\nsuggested donation amount\n\nThe experiment found that using matching donations increases the revenue per solicitation and the probability that someone donates.\nIt also found that larger match ratios relative to smaller match ratios have no additional impact. This project seeks to replicate their results.\nOverall, the authors found a unique way to measure the scope of a public good (donations) in a real world setting, rather than hypothetical scenarios. This is a valuable contribution to the literature on charitable giving and public goods."
  },
  {
    "objectID": "projects/HW1/hw1_questions.html#data",
    "href": "projects/HW1/hw1_questions.html#data",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Data",
    "text": "Data\n\nDescription\nThe dataset seen below which Karlan and List gathered on the 50,083 subjects has over 52 columns or variables. The primary variables we are concerned with for our research are ‘gave’ (if the individual donated any amount), ‘amount’ (what amount they donated), ‘treatment’ (if they were a part of the treatment or control group), and ‘ratio’ (matching ratio given to that subject).\nMany of the columns are redundant (treatmeant vs control columns, ratio columns vs ratio1, ratio2, ratio3 columns), however they allow us to more easily filter the data and make calculations since they are in binary format, rather than categorical (text).\nIn the second cell, you can see the description of variables, including the first two variables, ‘treatment’ and ‘control’. Through this, it is shown that 66% of the participants were a part of the treatment group, and 33% were a part of the control group.\nSince many of these variables are binary, a 1 or 0 is used to describe whether the participant had that treatment, variables, or attribute, which allows analysts to easily calculate proportions or percentages like with treatment and control.\n\nimport pandas as pd\nkarlan = pd.read_stata(\"karlan_list_2007.dta\")\nkarlan\n\n\n\n\n\n\n\n\ntreatment\ncontrol\nratio\nratio2\nratio3\nsize\nsize25\nsize50\nsize100\nsizeno\n...\nredcty\nbluecty\npwhite\npblack\npage18_39\nave_hh_sz\nmedian_hhincome\npowner\npsch_atlstba\npop_propurban\n\n\n\n\n0\n0\n1\nControl\n0\n0\nControl\n0\n0\n0\n0\n...\n0.0\n1.0\n0.446493\n0.527769\n0.317591\n2.10\n28517.0\n0.499807\n0.324528\n1.000000\n\n\n1\n0\n1\nControl\n0\n0\nControl\n0\n0\n0\n0\n...\n1.0\n0.0\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n2\n1\n0\n1\n0\n0\n$100,000\n0\n0\n1\n0\n...\n0.0\n1.0\n0.935706\n0.011948\n0.276128\n2.48\n51175.0\n0.721941\n0.192668\n1.000000\n\n\n3\n1\n0\n1\n0\n0\nUnstated\n0\n0\n0\n1\n...\n1.0\n0.0\n0.888331\n0.010760\n0.279412\n2.65\n79269.0\n0.920431\n0.412142\n1.000000\n\n\n4\n1\n0\n1\n0\n0\n$50,000\n0\n1\n0\n0\n...\n0.0\n1.0\n0.759014\n0.127421\n0.442389\n1.85\n40908.0\n0.416072\n0.439965\n1.000000\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n50078\n1\n0\n1\n0\n0\n$25,000\n1\n0\n0\n0\n...\n0.0\n1.0\n0.872797\n0.089959\n0.257265\n2.13\n45047.0\n0.771316\n0.263744\n1.000000\n\n\n50079\n0\n1\nControl\n0\n0\nControl\n0\n0\n0\n0\n...\n0.0\n1.0\n0.688262\n0.108889\n0.288792\n2.67\n74655.0\n0.741931\n0.586466\n1.000000\n\n\n50080\n0\n1\nControl\n0\n0\nControl\n0\n0\n0\n0\n...\n1.0\n0.0\n0.900000\n0.021311\n0.178689\n2.36\n26667.0\n0.778689\n0.107930\n0.000000\n\n\n50081\n1\n0\n3\n0\n1\nUnstated\n0\n0\n0\n1\n...\n1.0\n0.0\n0.917206\n0.008257\n0.225619\n2.57\n39530.0\n0.733988\n0.184768\n0.634903\n\n\n50082\n1\n0\n3\n0\n1\n$25,000\n1\n0\n0\n0\n...\n0.0\n1.0\n0.530023\n0.074112\n0.340698\n3.70\n48744.0\n0.717843\n0.127941\n0.994181\n\n\n\n\n50083 rows × 51 columns\n\n\n\n\n\n\n\n\n\n\nkarlan.describe()\n\n\n\n\n\n\n\n\ntreatment\ncontrol\nratio2\nratio3\nsize25\nsize50\nsize100\nsizeno\naskd1\naskd2\n...\nredcty\nbluecty\npwhite\npblack\npage18_39\nave_hh_sz\nmedian_hhincome\npowner\npsch_atlstba\npop_propurban\n\n\n\n\ncount\n50083.000000\n50083.000000\n50083.000000\n50083.000000\n50083.000000\n50083.000000\n50083.000000\n50083.000000\n50083.000000\n50083.000000\n...\n49978.000000\n49978.000000\n48217.000000\n48047.000000\n48217.000000\n48221.000000\n48209.000000\n48214.000000\n48215.000000\n48217.000000\n\n\nmean\n0.666813\n0.333187\n0.222311\n0.222211\n0.166723\n0.166623\n0.166723\n0.166743\n0.222311\n0.222291\n...\n0.510245\n0.488715\n0.819599\n0.086710\n0.321694\n2.429012\n54815.700533\n0.669418\n0.391661\n0.871968\n\n\nstd\n0.471357\n0.471357\n0.415803\n0.415736\n0.372732\n0.372643\n0.372732\n0.372750\n0.415803\n0.415790\n...\n0.499900\n0.499878\n0.168560\n0.135868\n0.103039\n0.378105\n22027.316665\n0.193405\n0.186599\n0.258633\n\n\nmin\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n...\n0.000000\n0.000000\n0.009418\n0.000000\n0.000000\n0.000000\n5000.000000\n0.000000\n0.000000\n0.000000\n\n\n25%\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n...\n0.000000\n0.000000\n0.755845\n0.014729\n0.258311\n2.210000\n39181.000000\n0.560222\n0.235647\n0.884929\n\n\n50%\n1.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n...\n1.000000\n0.000000\n0.872797\n0.036554\n0.305534\n2.440000\n50673.000000\n0.712296\n0.373744\n1.000000\n\n\n75%\n1.000000\n1.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n...\n1.000000\n1.000000\n0.938827\n0.090882\n0.369132\n2.660000\n66005.000000\n0.816798\n0.530036\n1.000000\n\n\nmax\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n...\n1.000000\n1.000000\n1.000000\n0.989622\n0.997544\n5.270000\n200001.000000\n1.000000\n1.000000\n1.000000\n\n\n\n\n8 rows × 48 columns\n\n\n\n\nVariable Definitions\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\ntreatment\nTreatment\n\n\ncontrol\nControl\n\n\nratio\nMatch ratio\n\n\nratio2\n2:1 match ratio\n\n\nratio3\n3:1 match ratio\n\n\nsize\nMatch threshold\n\n\nsize25\n$25,000 match threshold\n\n\nsize50\n$50,000 match threshold\n\n\nsize100\n$100,000 match threshold\n\n\nsizeno\nUnstated match threshold\n\n\nask\nSuggested donation amount\n\n\naskd1\nSuggested donation was highest previous contribution\n\n\naskd2\nSuggested donation was 1.25 x highest previous contribution\n\n\naskd3\nSuggested donation was 1.50 x highest previous contribution\n\n\nask1\nHighest previous contribution (for suggestion)\n\n\nask2\n1.25 x highest previous contribution (for suggestion)\n\n\nask3\n1.50 x highest previous contribution (for suggestion)\n\n\namount\nDollars given\n\n\ngave\nGave anything\n\n\namountchange\nChange in amount given\n\n\nhpa\nHighest previous contribution\n\n\nltmedmra\nSmall prior donor: last gift was less than median $35\n\n\nfreq\nNumber of prior donations\n\n\nyears\nNumber of years since initial donation\n\n\nyear5\nAt least 5 years since initial donation\n\n\nmrm2\nNumber of months since last donation\n\n\ndormant\nAlready donated in 2005\n\n\nfemale\nFemale\n\n\ncouple\nCouple\n\n\nstate50one\nState tag: 1 for one observation of each of 50 states; 0 otherwise\n\n\nnonlit\nNonlitigation\n\n\ncases\nCourt cases from state in 2004-5 in which organization was involved\n\n\nstatecnt\nPercent of sample from state\n\n\nstateresponse\nProportion of sample from the state who gave\n\n\nstateresponset\nProportion of treated sample from the state who gave\n\n\nstateresponsec\nProportion of control sample from the state who gave\n\n\nstateresponsetminc\nstateresponset - stateresponsec\n\n\nperbush\nState vote share for Bush\n\n\nclose25\nState vote share for Bush between 47.5% and 52.5%\n\n\nred0\nRed state\n\n\nblue0\nBlue state\n\n\nredcty\nRed county\n\n\nbluecty\nBlue county\n\n\npwhite\nProportion white within zip code\n\n\npblack\nProportion black within zip code\n\n\npage18_39\nProportion age 18-39 within zip code\n\n\nave_hh_sz\nAverage household size within zip code\n\n\nmedian_hhincome\nMedian household income within zip code\n\n\npowner\nProportion house owner within zip code\n\n\npsch_atlstba\nProportion who finished college within zip code\n\n\npop_propurban\nProportion of population urban within zip code\n\n\n\n\n\n\n\n\n\nBalance Test\nAs an ad hoc test of the randomization mechanism, I provide a series of tests that compare aspects of the treatment and control groups to assess whether they are statistically significantly different from one another.\n\n\nMonths since last Donation Variable\n\n# Months since last donation t-test\n\nmrm2_treat = karlan[karlan['treatment'] == 1]['mrm2']\nmrm2_control = karlan[karlan['treatment'] == 0]['mrm2']\n\nmrm2_t_mean = mrm2_treat.mean()\nmrm2_c_mean = mrm2_control.mean()\n\nmrm2_t_std = mrm2_treat.std()\nmrm2_c_std = mrm2_control.std()\n\nmrm2_t_n = mrm2_treat.count()\nmrm2_c_n = mrm2_control.count()\n\nt_mrm2 = (mrm2_t_mean - mrm2_c_mean) / ((mrm2_t_std**2/mrm2_t_n) + (mrm2_c_std**2/mrm2_c_n))**0.5\n\nprint(t_mrm2)\n\n0.11953155228176905\n\n\n\n# calculate p-value\nfrom scipy import stats\n\np_mrm2 = stats.t.sf(abs(t_mrm2), mrm2_t_n + mrm2_c_n - 2) * 2\nprint(p_mrm2)\n\n# Not statistically significant\n\n0.9048547235822526\n\n\n\n# Linear Regression - mrm2\n\nimport pyrsm as rsm\n\nlr_mrm2 = rsm.regress(\n    data = karlan[['treatment', 'mrm2']],\n    evar = \"treatment\",\n    rvar = \"mrm2\"\n    )\n\nlr_mrm2.summary()\n\nLinear regression (OLS)\nData                 : Not provided\nResponse variable    : mrm2\nExplanatory variables: treatment\nNull hyp.: the effect of x on mrm2 is zero\nAlt. hyp.: the effect of x on mrm2 is not zero\n\n           coefficient  std.error  t.value p.value     \nIntercept       12.998      0.094  138.979  &lt; .001  ***\ntreatment        0.014      0.115    0.119   0.905     \n\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nR-squared: 0.0, Adjusted R-squared: -0.0\nF-statistic: 0.014 df(1, 50080), p.value 0.905\nNr obs: 50,082\n\n\n\n\nCouple Variable\n\ncouple_treat = karlan[karlan['treatment'] == 1]['couple']\ncouple_control = karlan[karlan['treatment'] == 0]['couple']\n\ncouple_t_mean = couple_treat.mean()\ncouple_c_mean = couple_control.mean()\n\ncouple_t_std = couple_treat.std()\ncouple_c_std = couple_control.std()\n\ncouple_t_n = couple_treat.count()\ncouple_c_n = couple_control.count()\n\nt_couple = (couple_t_mean - couple_c_mean) / ((couple_t_std**2/couple_t_n) + (couple_c_std**2/couple_c_n))**0.5\n\nprint(t_couple)\n\n-0.5822577486768489\n\n\n\n# calculate p-value\n\np_couple = stats.t.sf(abs(t_couple), couple_t_n + couple_c_n - 2) * 2\nprint(p_couple)\n\n# Not statistically significant\n\n0.5603957630249871\n\n\n\nlr_couple = rsm.regress(\n    data = karlan[['treatment', 'couple']],\n    evar = \"treatment\",\n    rvar = \"couple\"\n    )\n\nlr_couple.summary()\n\nLinear regression (OLS)\nData                 : Not provided\nResponse variable    : couple\nExplanatory variables: treatment\nNull hyp.: the effect of x on couple is zero\nAlt. hyp.: the effect of x on couple is not zero\n\n           coefficient  std.error  t.value p.value     \nIntercept        0.093      0.002   41.124  &lt; .001  ***\ntreatment       -0.002      0.003   -0.584   0.559     \n\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nR-squared: 0.0, Adjusted R-squared: -0.0\nF-statistic: 0.341 df(1, 48933), p.value 0.559\nNr obs: 48,935\n\n\n\n\nBalance Test Results\nThe intercept coefficients calculated for mrm2 and couple variables are exactly the same as in table 1 of the paper for the control group. Also, when incorporating the coefficients for treatment (treatment = 1), they also equal the mean values in table 1 for the treatment group.\nFor mrm2 (months since last donation), the control group mean was 12.998 and that increases to 13.012 if they were a part of the treatment group. This is not a large disparity and as such did not prove to be statistically significant on a 95% confidence interval.\nFor couple (whether the donor was a couple), the control group mean was 0.093 (interpreted as 9.3% of donors in the control group were couples) and that decreases to 0.091 if they were a part of the treatment group. This is also not a large disparity and as such did not prove to be statistically significant on a 95% confidence interval."
  },
  {
    "objectID": "projects/HW1/hw1_questions.html#experimental-results",
    "href": "projects/HW1/hw1_questions.html#experimental-results",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Experimental Results",
    "text": "Experimental Results\n\nCharitable Contribution Made\nFirst, I analyze whether matched donations lead to an increased response rate of making a donation.\n\n\n# find proportion of treatment group that gave money\n\ntreat_gave = karlan[karlan['treatment'] == 1]['gave'].mean()\ncontrol_gave = karlan[karlan['treatment'] == 0]['gave'].mean()\n\nprint(treat_gave, control_gave)\n\n0.02203856749311295 0.017858212980164198\n\n\n\n# create bar graph for proportion of treatment/control group that gave money\n\nimport matplotlib.pyplot as plt\n\nfig, ax = plt.subplots()\nax.bar(['Treatment', 'Control'], [treat_gave, control_gave])\nax.set_ylabel('Proportion of Group that Gave Money')\nax.set_title('Proportion of Treatment and Control Group that Gave Money')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n# Calculate t-stat for those who donated if they were a part of treatment or control group\n\ngave_treat = karlan[karlan['treatment'] == 1]['gave']\ngave_control = karlan[karlan['treatment'] == 0]['gave']\n\ngave_t_mean = gave_treat.mean()\ngave_c_mean = gave_control.mean()\n\ngave_t_std = gave_treat.std()\ngave_c_std = gave_control.std()\n\ngave_t_n = gave_treat.count()\ngave_c_n = gave_control.count()\n\nt_gave = (gave_t_mean - gave_c_mean) / ((gave_t_std**2/gave_t_n) + (gave_c_std**2/gave_c_n))**0.5\n\nprint(t_gave)\n\n3.2094621908279835\n\n\n\n# Calculate p-value on 95% confidence interval\n\np_gave = stats.t.sf(abs(t_gave), gave_t_n + gave_c_n - 2) * 2\np_gave\n\n# Statistically significant\n\n0.0013306730060655475\n\n\n\n# Run a linear regression which 'gave' is the response variable and 'treatment' is the explanatory variable\n\nlr_gave = rsm.regress(\n    data = karlan[['treatment', 'gave']],\n    evar = \"treatment\",\n    rvar = \"gave\"\n    )\n\nlr_gave.summary()\n\nLinear regression (OLS)\nData                 : Not provided\nResponse variable    : gave\nExplanatory variables: treatment\nNull hyp.: the effect of x on gave is zero\nAlt. hyp.: the effect of x on gave is not zero\n\n           coefficient  std.error  t.value p.value     \nIntercept        0.018      0.001   16.225  &lt; .001  ***\ntreatment        0.004      0.001    3.101   0.002   **\n\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nR-squared: 0.0, Adjusted R-squared: 0.0\nF-statistic: 9.618 df(1, 50081), p.value 0.002\nNr obs: 50,083\n\n\nThe outcome of our t-test and linear regression were both similar in that it was found at a 95% confidence interval that the treatment did cause an increase in the number of donations. The linear regression did not explain any variance in the data, however that is not as important because we are using what should be a logistic regression (binary outcome of 1 or 0 if they donated or not) with a linear regression model.\n\n\n# Run probit regression on gave and treatment variables\n\nimport statsmodels.formula.api as smf\n\nmod = smf.probit('gave ~ treatment', data=karlan)\nres = mod.fit()\nres.summary()\n\nOptimization terminated successfully.\n         Current function value: 0.100443\n         Iterations 7\n\n\n\nProbit Regression Results\n\n\nDep. Variable:\ngave\nNo. Observations:\n50083\n\n\nModel:\nProbit\nDf Residuals:\n50081\n\n\nMethod:\nMLE\nDf Model:\n1\n\n\nDate:\nMon, 15 Apr 2024\nPseudo R-squ.:\n0.0009783\n\n\nTime:\n20:30:57\nLog-Likelihood:\n-5030.5\n\n\nconverged:\nTrue\nLL-Null:\n-5035.4\n\n\nCovariance Type:\nnonrobust\nLLR p-value:\n0.001696\n\n\n\n\n\n\n\ncoef\nstd err\nz\nP&gt;|z|\n[0.025\n0.975]\n\n\nIntercept\n-2.1001\n0.023\n-90.073\n0.000\n-2.146\n-2.054\n\n\ntreatment\n0.0868\n0.028\n3.113\n0.002\n0.032\n0.141\n\n\n\n\n\n\n\nDifferences between Match Rates\nNext, I assess the effectiveness of different sizes of matched donations on the response rate.\n\nRatio of 1:1 vs Ratio of 2:1\n\n# Calculate t-stat for Ratio 1:1 vs Ratio 2:1\n\ngave_ratio1 = karlan[karlan['ratio'] == 1]['gave']\ngave_ratio2 = karlan[karlan['ratio'] == 2]['gave']\n\ngave_1_mean = gave_ratio1.mean()\ngave_2_mean = gave_ratio2.mean()\n\ngave_1_std = gave_ratio1.std()\ngave_2_std = gave_ratio2.std()\n\ngave_1_n = gave_ratio1.count()\ngave_2_n = gave_ratio2.count()\n\nt_gave_ratio = (gave_2_mean - gave_1_mean) / ((gave_2_std**2/gave_2_n) + (gave_1_std**2/gave_1_n))**0.5\n\nprint(t_gave_ratio)\n\n0.965048975142932\n\n\n\np_gave_ratio = stats.t.sf(abs(t_gave_ratio), gave_2_n + gave_1_n - 2) * 2\np_gave_ratio\n\n# Not statistically significant\n\n0.3345307635444439\n\n\n\n\nRatio of 2:1 vs Ratio of 3:1\n\ngave_ratio3 = karlan[karlan['ratio'] == 3]['gave']\ngave_ratio2 = karlan[karlan['ratio'] == 2]['gave']\n\ngave_3_mean = gave_ratio3.mean()\ngave_2_mean = gave_ratio2.mean()\n\ngave_3_std = gave_ratio3.std()\ngave_2_std = gave_ratio2.std()\n\ngave_3_n = gave_ratio3.count()\ngave_2_n = gave_ratio2.count()\n\nt_gave_ratio_23 = (gave_2_mean - gave_3_mean) / ((gave_2_std**2/gave_2_n) + (gave_3_std**2/gave_3_n))**0.5\n\nprint(t_gave_ratio_23)\n\n-0.05011581369764474\n\n\n\np_gave_ratio_23 = stats.t.sf(abs(t_gave_ratio_23), gave_2_n + gave_3_n - 2) * 2\np_gave_ratio_23\n\n# Not statistically significant\n\n0.9600305476910405\n\n\nThe above calculations matches what Karlan suggests in his paper, that increasing the match ratio does not increase the probability of making a donation. The above calculations show a 1:1 match ratio when compared to a 2:1 match ratio, and a 2:1 match ratio when compared to a 3:1 match ratio. We did a two-sided t-test, which has a null hypothesis stating that 2:1 is not the same as 1:1, and 3:1 is not the same as 2:1.\n\n\n#create a variable ratio1 where if ratio column equals 1, then ratio1 equlas 1, else 0\n\nkarlan['ratio1'] = karlan['ratio'].apply(lambda x: 1 if x == 1 else 0)\n\n\n# Linear regression for match ratios and treatment\n\nlr_ratio = rsm.regress(\n    data = karlan[['gave', 'ratio1', 'ratio2', 'ratio3']],\n    evar = ['ratio1', 'ratio2', 'ratio3'],\n    rvar = 'gave'\n    )  \n\nlr_ratio.summary()\n\nLinear regression (OLS)\nData                 : Not provided\nResponse variable    : gave\nExplanatory variables: ratio1, ratio2, ratio3\nNull hyp.: the effect of x on gave is zero\nAlt. hyp.: the effect of x on gave is not zero\n\n           coefficient  std.error  t.value p.value     \nIntercept        0.018      0.001   16.225  &lt; .001  ***\nratio1           0.003      0.002    1.661   0.097    .\nratio2           0.005      0.002    2.744   0.006   **\nratio3           0.005      0.002    2.802   0.005   **\n\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nR-squared: 0.0, Adjusted R-squared: 0.0\nF-statistic: 3.665 df(3, 50079), p.value 0.012\nNr obs: 50,083\n\n\nThe results of linear regression model on match ratios of 1:1, 2:1, and 3:1 show that all but a ratio of 1:1 are statistically significant. The intercept coefficient (meaning when there is no match) is 0.018, and then for ratio 1:1 it has a coefficient of 0.003, which added to 0.018 is 0.021. For ratio 2:1 and 3:1, they both have coefficients of 0.005, creating a response rate of 0.023 for both. All these response rates match table 2A in the paper.\nThe precision of these estimates also match what is in the paper for standard error. Each ratio has a standard error of 0.002, meaning that ratio 1:1 could have a varying effect on response rate between 0.001 and 0.005, and ratios 2:1 and 3:1 could vary between 0.003 and 0.007. All however remain positive when incorporating standard error, meaning that the match ratio does have a positive effect on response rate when comparing to control.\n\n\n\n\n\nSize of Charitable Contribution\nIn this subsection, I analyze the effect of the size of matched donation on the size of the charitable contribution.\n\n\n# Linear Regression on Treatment to Predict donation amount\nlr_d_amount = rsm.regress(\n    data = karlan[['amount', 'treatment']],\n    evar = 'treatment',\n    rvar = 'amount'\n    )  \n\nlr_d_amount.summary()\n\nLinear regression (OLS)\nData                 : Not provided\nResponse variable    : amount\nExplanatory variables: treatment\nNull hyp.: the effect of x on amount is zero\nAlt. hyp.: the effect of x on amount is not zero\n\n           coefficient  std.error  t.value p.value     \nIntercept        0.813      0.067   12.063  &lt; .001  ***\ntreatment        0.154      0.083    1.861   0.063    .\n\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nR-squared: 0.0, Adjusted R-squared: 0.0\nF-statistic: 3.461 df(1, 50081), p.value 0.063\nNr obs: 50,083\n\n\nFrom our linear regression model to calculate dollars donated amount from whether the donor was in the treatment or control group, we can conclude that the control group donates $0.813 and being a part of the treatment group increases that amount by $0.154, bringing the estimated dollars donated amount to $0.967. Per a 95% confidence interval, the model is not statistically significant, however it is very close to being so.\n\n# Linear Regression on treatment to predict donation amounts above 0\n\nlr_d_amount_above0 = rsm.regress(\n    data = karlan[karlan['amount'] &gt; 0][['amount', 'treatment']],\n    evar = 'treatment',\n    rvar = 'amount'\n    )  \n\nlr_d_amount_above0.summary()\n\nLinear regression (OLS)\nData                 : Not provided\nResponse variable    : amount\nExplanatory variables: treatment\nNull hyp.: the effect of x on amount is zero\nAlt. hyp.: the effect of x on amount is not zero\n\n           coefficient  std.error  t.value p.value     \nIntercept       45.540      2.423   18.792  &lt; .001  ***\ntreatment       -1.668      2.872   -0.581   0.561     \n\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nR-squared: 0.0, Adjusted R-squared: -0.001\nF-statistic: 0.337 df(1, 1032), p.value 0.561\nNr obs: 1,034\n\n\nAfter filtering the data to only include those who donated, the linear regression model changed significantly. The intercept coefficient is now $45.54, and the treatment coefficient is now negative $1.67. Now, the treatment group has a negative affect on amount donated, meaning that the control group donates $45.54 and the treatment group donates $43.87.\nHowever, the model is not statistically significant at 95% confidence interval.\n\n\n\n# create a histogram for all the donation amounts above 0 for the control group\n\nfig, ax = plt.subplots()\nax.hist(karlan[(karlan['amount'] &gt; 0) & (karlan['treatment'] == 0)]['amount'], bins=25)\nax.axvline(43.87, color='red')\nax.text(43.87 + 1, ax.get_ylim()[1] * 0.9, f'{43.87}', color='red')\nax.set_ylabel('Frequency')\nax.set_xlabel('Donation Amount')\nax.set_title('Frequency of Donation Amounts for Control Group')\nplt.show()\n\n\n\n\n\n\n\n\n\n# create a histogram for all the donation amounts above 0 for the treatment group\n\nfig, ax = plt.subplots()\nax.hist(karlan[(karlan['amount'] &gt; 0) & (karlan['treatment'] == 1)]['amount'], bins=25)\nax.axvline(45.54, color='red')\nax.text(45.54 + 1, ax.get_ylim()[1] * 0.9, f'{45.54}', color='red')\nax.set_ylabel('Frequency')\nax.set_xlabel('Donation Amount')\nax.set_title('Frequency of Donation Amounts for Treatment Group')\nplt.show()"
  },
  {
    "objectID": "projects/HW1/hw1_questions.html#simulation-experiment",
    "href": "projects/HW1/hw1_questions.html#simulation-experiment",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Simulation Experiment",
    "text": "Simulation Experiment\nAs a reminder of how the t-statistic “works,” in this section I use simulation to demonstrate the Law of Large Numbers and the Central Limit Theorem.\nSuppose the true distribution of respondents who do not get a charitable donation match is Bernoulli with probability p=0.018 that a donation is made.\nFurther suppose that the true distribution of respondents who do get a charitable donation match of any size is Bernoulli with probability p=0.022 that a donation is made.\n\nLaw of Large Numbers\n\n\n# use a bernoulli distribution to simulate 10,000 trials for control and treatment groups using their mean percentage donated as the probabilities respectively\n\nimport numpy as np\nfrom scipy.stats import bernoulli\n\ncontrol = karlan[karlan['treatment'] == 0]['gave'].mean()\ntreatment = karlan[karlan['treatment'] == 1]['gave'].mean()\n\ncontrol_sim = bernoulli.rvs(p = control, size = 10000)\ntreatment_sim = bernoulli.rvs(p = treatment, size = 10000)\n\n# calculate cumulative average of the differences for the first 10,000 draws\n\ncum_avg = np.cumsum(treatment_sim - control_sim) / np.arange(1, 10001)\n\ncum_avg\n\narray([0.        , 0.        , 0.        , ..., 0.00830166, 0.00830083,\n       0.0083    ])\n\n\n\n# plot the cumulative average of the differences with a line plot\n\nfig, ax = plt.subplots()\nax.plot(cum_avg)\nax.axhline(treatment - control, color='red')\nax.set_ylabel('Cumulative Average of Differences')\nax.set_xlabel('Number of Draws')\nax.set_title('Cumulative Average of Differences in Proportion of Giving Money')\nplt.show()\n\n\n\n\n\n\n\n\nThe above graph shows a simulation of the differences between simulated probabilities of an individual actually donating in the treatment group and an individual donating in the control group. The control group was given a probability of 0.018 and treatment group was given a probability of 0.022, which were both calculating from the given dataset.\nWe simulated this for the control and treatment group 10,000 times to generate the graph. The red horizontal line on the graph shows the calculated difference between the treatment and control probabilities from the dataset (0.004).\nAs shown, the cumulative difference between the treatment and control group varies greatly initially, then slowly starts to vary less and less around the average difference of 0.004. With more trials, it is expected there would be even less variability and the cumulative difference would be extremely close to 0.004.\n\n\nCentral Limit Theorem\n\n\nSimulating 50 trials\n\nn_50_c = np.random.binomial(50, 0.018, 1000)\n\nn_50_c = n_50_c / 50\n\nn_50_t = np.random.binomial(50, 0.022, 1000)\n\nn_50_t = n_50_t / 50\n\nn_50 = n_50_t - n_50_c\n\n# make a histogram of n_50 so there is no spacing between the bars\n\nfig, ax = plt.subplots()\nax.hist(n_50, bins=10, rwidth=1)\nax.axvline(n_50.mean(), color='red')\nax.text(n_50.mean(), ax.get_ylim()[1]*0.9, f'{np.round(n_50.mean(),4)}', color='red', verticalalignment='center', horizontalalignment='center')\nax.set_ylabel('Frequency')\nax.set_xlabel('Difference in Proportion of Donors (Treatment - Control) in 50 Draws')\nax.set_title('Frequency of Proportion of Control Group that Gave Money in 50 Draws')\nplt.show()\n\n\n\n\n\n\n\n\n\n\nSimulating 200 trials\n\nn_200_c = np.random.binomial(200, 0.018, 1000)/200\n\nn_200_t = np.random.binomial(200, 0.022, 1000)/200\n\n\nn_200 = n_200_t - n_200_c\n\nn_200_mean = n_200.mean()\n\nfig, ax = plt.subplots()\nax.hist(n_200, bins=10, rwidth=1)\nax.axvline(n_200_mean, color='red')\nax.text(n_200_mean, ax.get_ylim()[1]*0.9, f'{np.round(n_200_mean,4)}', color='red', verticalalignment='center', horizontalalignment='center')\nax.set_ylabel('Frequency')\nax.set_xlabel('Difference in Proportion of Donors (Treatment - Control) in 200 Draws')\nax.set_title('Frequency of Proportion of Control Group that Gave Money in 200 Draws')\nplt.show()\n\n\n\n\n\n\n\n\n\n\nSimulating 500 trials\n\nn_500_c = np.random.binomial(500, 0.018, 1000)/500\n\nn_500_t = np.random.binomial(500, 0.022, 1000)/500\n\n\nn_500 = n_500_t - n_500_c\n\nn_500_mean = n_500.mean()\n\nfig, ax = plt.subplots()\nax.hist(n_500, bins=10, rwidth=1)\nax.axvline(n_500_mean, color='red')\nax.text(n_500_mean, ax.get_ylim()[1]*0.9, f'{np.round(n_500_mean,4)}', color='red', verticalalignment='center', horizontalalignment='center')\nax.set_ylabel('Frequency')\nax.set_xlabel('Difference in Proportion of Donors (Treatment - Control) in 500 Draws')\nax.set_title('Frequency of Proportion of Control Group that Gave Money in 500 Draws')\nplt.show()\n\n\n\n\n\n\n\n\n\n\nSimulating 1000 trials\n\nn_1000_c = np.random.binomial(1000, 0.018, 1000)/1000\n\nn_1000_t = np.random.binomial(1000, 0.022, 1000)/1000\n\nn_1000 = n_1000_t - n_1000_c\n\nn_1000_mean = n_1000.mean()\n\nfig, ax = plt.subplots()\nax.hist(n_1000, bins=10, rwidth=1)\nax.axvline(n_1000_mean, color='red')\nax.text(n_1000_mean, ax.get_ylim()[1]*0.9, f'{np.round(n_1000_mean,4)}', color='red', verticalalignment='center', horizontalalignment='right')\nax.set_ylabel('Frequency')\nax.set_xlabel('Difference in Proportion of Donors (Treatment - Control) in 500 Draws')\nax.set_title('Frequency of Proportion of Control Group that Gave Money in 500 Draws')\nplt.show()\n\n\n\n\n\n\n\n\nFrom the above histograms, you can see from the red vertical line that the average difference decreases as we increase the trial size of the simulation and starts to get closer to 0.004, which is the calculated difference from the dataset between the percentage that donated from the treatment group versus the control group.\nAlthough the 0 mark on the x-axis varies because of graph sizing, you can see that the outer limits of the distribution gets smaller as we increase the number of trials. This follows the central limit theorem, since as we increase the number of trials in each simulation, we will get closer and closer to the true mean or percentage. The variation or wide distribution we see in the 50 trial simulation is no longer there in the 1000 trial distribution.\nOverall, the more trials we have in our simulation or experiment, the closer we will get to the mean."
  },
  {
    "objectID": "projects/HW1/HW1.html",
    "href": "projects/HW1/HW1.html",
    "title": "Nick_Shuckerow",
    "section": "",
    "text": "import pandas as pd\nfrom scipy import stats\nkarlan = pd.read_stata(\"karlan_list_2007.dta\")\nkarlan\n\n\n\n\n\n\n\n\ntreatment\ncontrol\nratio\nratio2\nratio3\nsize\nsize25\nsize50\nsize100\nsizeno\n...\nredcty\nbluecty\npwhite\npblack\npage18_39\nave_hh_sz\nmedian_hhincome\npowner\npsch_atlstba\npop_propurban\n\n\n\n\n0\n0\n1\nControl\n0\n0\nControl\n0\n0\n0\n0\n...\n0.0\n1.0\n0.446493\n0.527769\n0.317591\n2.10\n28517.0\n0.499807\n0.324528\n1.000000\n\n\n1\n0\n1\nControl\n0\n0\nControl\n0\n0\n0\n0\n...\n1.0\n0.0\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n2\n1\n0\n1\n0\n0\n$100,000\n0\n0\n1\n0\n...\n0.0\n1.0\n0.935706\n0.011948\n0.276128\n2.48\n51175.0\n0.721941\n0.192668\n1.000000\n\n\n3\n1\n0\n1\n0\n0\nUnstated\n0\n0\n0\n1\n...\n1.0\n0.0\n0.888331\n0.010760\n0.279412\n2.65\n79269.0\n0.920431\n0.412142\n1.000000\n\n\n4\n1\n0\n1\n0\n0\n$50,000\n0\n1\n0\n0\n...\n0.0\n1.0\n0.759014\n0.127421\n0.442389\n1.85\n40908.0\n0.416072\n0.439965\n1.000000\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n50078\n1\n0\n1\n0\n0\n$25,000\n1\n0\n0\n0\n...\n0.0\n1.0\n0.872797\n0.089959\n0.257265\n2.13\n45047.0\n0.771316\n0.263744\n1.000000\n\n\n50079\n0\n1\nControl\n0\n0\nControl\n0\n0\n0\n0\n...\n0.0\n1.0\n0.688262\n0.108889\n0.288792\n2.67\n74655.0\n0.741931\n0.586466\n1.000000\n\n\n50080\n0\n1\nControl\n0\n0\nControl\n0\n0\n0\n0\n...\n1.0\n0.0\n0.900000\n0.021311\n0.178689\n2.36\n26667.0\n0.778689\n0.107930\n0.000000\n\n\n50081\n1\n0\n3\n0\n1\nUnstated\n0\n0\n0\n1\n...\n1.0\n0.0\n0.917206\n0.008257\n0.225619\n2.57\n39530.0\n0.733988\n0.184768\n0.634903\n\n\n50082\n1\n0\n3\n0\n1\n$25,000\n1\n0\n0\n0\n...\n0.0\n1.0\n0.530023\n0.074112\n0.340698\n3.70\n48744.0\n0.717843\n0.127941\n0.994181\n\n\n\n\n50083 rows × 51 columns\n\n\n\n\nkarlan.describe()\n\n\n\n\n\n\n\n\ntreatment\ncontrol\nratio2\nratio3\nsize25\nsize50\nsize100\nsizeno\naskd1\naskd2\n...\nredcty\nbluecty\npwhite\npblack\npage18_39\nave_hh_sz\nmedian_hhincome\npowner\npsch_atlstba\npop_propurban\n\n\n\n\ncount\n50083.000000\n50083.000000\n50083.000000\n50083.000000\n50083.000000\n50083.000000\n50083.000000\n50083.000000\n50083.000000\n50083.000000\n...\n49978.000000\n49978.000000\n48217.000000\n48047.000000\n48217.000000\n48221.000000\n48209.000000\n48214.000000\n48215.000000\n48217.000000\n\n\nmean\n0.666813\n0.333187\n0.222311\n0.222211\n0.166723\n0.166623\n0.166723\n0.166743\n0.222311\n0.222291\n...\n0.510245\n0.488715\n0.819599\n0.086710\n0.321694\n2.429012\n54815.700533\n0.669418\n0.391661\n0.871968\n\n\nstd\n0.471357\n0.471357\n0.415803\n0.415736\n0.372732\n0.372643\n0.372732\n0.372750\n0.415803\n0.415790\n...\n0.499900\n0.499878\n0.168560\n0.135868\n0.103039\n0.378105\n22027.316665\n0.193405\n0.186599\n0.258633\n\n\nmin\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n...\n0.000000\n0.000000\n0.009418\n0.000000\n0.000000\n0.000000\n5000.000000\n0.000000\n0.000000\n0.000000\n\n\n25%\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n...\n0.000000\n0.000000\n0.755845\n0.014729\n0.258311\n2.210000\n39181.000000\n0.560222\n0.235647\n0.884929\n\n\n50%\n1.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n...\n1.000000\n0.000000\n0.872797\n0.036554\n0.305534\n2.440000\n50673.000000\n0.712296\n0.373744\n1.000000\n\n\n75%\n1.000000\n1.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n...\n1.000000\n1.000000\n0.938827\n0.090882\n0.369132\n2.660000\n66005.000000\n0.816798\n0.530036\n1.000000\n\n\nmax\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n...\n1.000000\n1.000000\n1.000000\n0.989622\n0.997544\n5.270000\n200001.000000\n1.000000\n1.000000\n1.000000\n\n\n\n\n8 rows × 48 columns\n\n\n\n\nkarlan.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 50083 entries, 0 to 50082\nData columns (total 51 columns):\n #   Column              Non-Null Count  Dtype   \n---  ------              --------------  -----   \n 0   treatment           50083 non-null  int8    \n 1   control             50083 non-null  int8    \n 2   ratio               50083 non-null  category\n 3   ratio2              50083 non-null  int8    \n 4   ratio3              50083 non-null  int8    \n 5   size                50083 non-null  category\n 6   size25              50083 non-null  int8    \n 7   size50              50083 non-null  int8    \n 8   size100             50083 non-null  int8    \n 9   sizeno              50083 non-null  int8    \n 10  ask                 50083 non-null  category\n 11  askd1               50083 non-null  int8    \n 12  askd2               50083 non-null  int8    \n 13  askd3               50083 non-null  int8    \n 14  ask1                50083 non-null  int16   \n 15  ask2                50083 non-null  int16   \n 16  ask3                50083 non-null  int16   \n 17  amount              50083 non-null  float32 \n 18  gave                50083 non-null  int8    \n 19  amountchange        50083 non-null  float32 \n 20  hpa                 50083 non-null  float32 \n 21  ltmedmra            50083 non-null  int8    \n 22  freq                50083 non-null  int16   \n 23  years               50082 non-null  float64 \n 24  year5               50083 non-null  int8    \n 25  mrm2                50082 non-null  float64 \n 26  dormant             50083 non-null  int8    \n 27  female              48972 non-null  float64 \n 28  couple              48935 non-null  float64 \n 29  state50one          50083 non-null  int8    \n 30  nonlit              49631 non-null  float64 \n 31  cases               49631 non-null  float64 \n 32  statecnt            50083 non-null  float32 \n 33  stateresponse       50083 non-null  float32 \n 34  stateresponset      50083 non-null  float32 \n 35  stateresponsec      50080 non-null  float32 \n 36  stateresponsetminc  50080 non-null  float32 \n 37  perbush             50048 non-null  float32 \n 38  close25             50048 non-null  float64 \n 39  red0                50048 non-null  float64 \n 40  blue0               50048 non-null  float64 \n 41  redcty              49978 non-null  float64 \n 42  bluecty             49978 non-null  float64 \n 43  pwhite              48217 non-null  float32 \n 44  pblack              48047 non-null  float32 \n 45  page18_39           48217 non-null  float32 \n 46  ave_hh_sz           48221 non-null  float32 \n 47  median_hhincome     48209 non-null  float64 \n 48  powner              48214 non-null  float32 \n 49  psch_atlstba        48215 non-null  float32 \n 50  pop_propurban       48217 non-null  float32 \ndtypes: category(3), float32(16), float64(12), int16(4), int8(16)\nmemory usage: 8.9 MB\n\n\n\n# write karlan to csv\n\nkarlan.to_csv(\"karlan.csv\")\n\n\n# Months since last donation t-test\n\nmrm2_treat = karlan[karlan['treatment'] == 1]['mrm2']\nmrm2_control = karlan[karlan['treatment'] == 0]['mrm2']\n\nmrm2_t_mean = mrm2_treat.mean()\nmrm2_c_mean = mrm2_control.mean()\n\nmrm2_t_std = mrm2_treat.std()\nmrm2_c_std = mrm2_control.std()\n\nmrm2_t_n = mrm2_treat.count()\nmrm2_c_n = mrm2_control.count()\n\nt_mrm2 = (mrm2_t_mean - mrm2_c_mean) / ((mrm2_t_std**2/mrm2_t_n) + (mrm2_c_std**2/mrm2_c_n))**0.5\n\nprint(t_mrm2)\n\n\n0.11953155228176905\n\n\n\n# calculate p-value\n\np_mrm2 = stats.t.sf(abs(t_mrm2), mrm2_t_n + mrm2_c_n - 2) * 2\np_mrm2\n\n# Not statistically significant\n\n0.9048547235822526\n\n\n\ncouple_treat = karlan[karlan['treatment'] == 1]['couple']\ncouple_control = karlan[karlan['treatment'] == 0]['couple']\n\ncouple_t_mean = couple_treat.mean()\ncouple_c_mean = couple_control.mean()\n\ncouple_t_std = couple_treat.std()\ncouple_c_std = couple_control.std()\n\ncouple_t_n = couple_treat.count()\ncouple_c_n = couple_control.count()\n\nt_couple = (couple_t_mean - couple_c_mean) / ((couple_t_std**2/couple_t_n) + (couple_c_std**2/couple_c_n))**0.5\n\nt_couple\n\n-0.5822577486768489\n\n\n\n# calculate p-value\n\np_couple = stats.t.sf(abs(t_couple), couple_t_n + couple_c_n - 2) * 2\np_couple\n\n# Not statistically significant\n\n0.5603957630249871\n\n\n\n# Linear Regression - mrm2\n\nimport pyrsm as rsm\n\nlr_mrm2 = rsm.regress(\n    data = karlan[['treatment', 'mrm2']],\n    evar = \"treatment\",\n    rvar = \"mrm2\"\n    )\n\nlr_mrm2.summary()\n\n\n\n\nLinear regression (OLS)\nData                 : Not provided\nResponse variable    : mrm2\nExplanatory variables: treatment\nNull hyp.: the effect of x on mrm2 is zero\nAlt. hyp.: the effect of x on mrm2 is not zero\n\n           coefficient  std.error  t.value p.value     \nIntercept       12.998      0.094  138.979  &lt; .001  ***\ntreatment        0.014      0.115    0.119   0.905     \n\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nR-squared: 0.0, Adjusted R-squared: -0.0\nF-statistic: 0.014 df(1, 50080), p.value 0.905\nNr obs: 50,082\n\n\n\nlr_couple = rsm.regress(\n    data = karlan[['treatment', 'couple']],\n    evar = \"treatment\",\n    rvar = \"couple\"\n    )\n\nlr_couple.summary()\n\nLinear regression (OLS)\nData                 : Not provided\nResponse variable    : couple\nExplanatory variables: treatment\nNull hyp.: the effect of x on couple is zero\nAlt. hyp.: the effect of x on couple is not zero\n\n           coefficient  std.error  t.value p.value     \nIntercept        0.093      0.002   41.124  &lt; .001  ***\ntreatment       -0.002      0.003   -0.584   0.559     \n\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nR-squared: 0.0, Adjusted R-squared: -0.0\nF-statistic: 0.341 df(1, 48933), p.value 0.559\nNr obs: 48,935\n\n\nThe intercept coefficients calculated for mrm2 and couple variables are exactly the same as in table 1 of the paper for the control group. Also, when incorporating the coefficients for treatment (treatment = 1), they also equal the mean values in table 1 for the treatment group.\nFor mrm2 (months since last donation), the control group mean was 12.998 and that increases to 13.012 if they were a part of the treatment group. This is not a large disparity and as such did not prove to be statistically significant on a 95% confidence interval.\nFor couple (whether the donor was a couple), the control group mean was 0.093 (interpreted as 9.3% of donors in the control group were couples) and that decreases to 0.091 if they were a part of the treatment group. This is also not a large disparity and as such did not prove to be statistically significant on a 95% confidence interval.\n\n# find proportion of treatment group that gave money\n\ntreat_gave = karlan[karlan['treatment'] == 1]['gave'].mean()\ncontrol_gave = karlan[karlan['treatment'] == 0]['gave'].mean()\n\ntreat_gave, control_gave\n\n(0.02203856749311295, 0.017858212980164198)\n\n\n\n# create bar graph for proportion of treatment/control group that gave money\n\nimport matplotlib.pyplot as plt\n\nfig, ax = plt.subplots()\nax.bar(['Treatment', 'Control'], [treat_gave, control_gave])\nax.set_ylabel('Proportion of Group that Gave Money')\nax.set_title('Proportion of Treatment and Control Group that Gave Money')\nplt.show()\n\n\n\n\n\n\n\n\n\ngave_treat = karlan[karlan['treatment'] == 1]['gave']\ngave_control = karlan[karlan['treatment'] == 0]['gave']\n\ngave_t_mean = gave_treat.mean()\ngave_c_mean = gave_control.mean()\n\ngave_t_std = gave_treat.std()\ngave_c_std = gave_control.std()\n\ngave_t_n = gave_treat.count()\ngave_c_n = gave_control.count()\n\nt_gave = (gave_t_mean - gave_c_mean) / ((gave_t_std**2/gave_t_n) + (gave_c_std**2/gave_c_n))**0.5\n\nt_gave\n\n3.2094621908279835\n\n\n\np_gave = stats.t.sf(abs(t_gave), gave_t_n + gave_c_n - 2) * 2\np_gave\n\n# Statistically significant\n\n0.0013306730060655475\n\n\n\nlr_gave = rsm.regress(\n    data = karlan[['treatment', 'gave']],\n    evar = \"treatment\",\n    rvar = \"gave\"\n    )\n\nlr_gave.summary()\n\nLinear regression (OLS)\nData                 : Not provided\nResponse variable    : gave\nExplanatory variables: treatment\nNull hyp.: the effect of x on gave is zero\nAlt. hyp.: the effect of x on gave is not zero\n\n           coefficient  std.error  t.value p.value     \nIntercept        0.018      0.001   16.225  &lt; .001  ***\ntreatment        0.004      0.001    3.101   0.002   **\n\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nR-squared: 0.0, Adjusted R-squared: 0.0\nF-statistic: 9.618 df(1, 50081), p.value 0.002\nNr obs: 50,083\n\n\nThe outcome of our t-test and linear regression were both similar in that it was found at a 95% confidence interval that the treatment did cause an increase in the number of donations. The linear regression did not explain any variance in the data, however that is not as important because we are using what should be a logistic regression (binary outcome of 1 or 0 for if they donated or not) with a linear regression model.\n\nimport statsmodels.formula.api as smf\n\n\nmod = smf.probit('gave ~ treatment', data=karlan)\n\n\nres = mod.fit()\n\nOptimization terminated successfully.\n         Current function value: 0.100443\n         Iterations 7\n\n\n\nres.summary()\n\n\nProbit Regression Results\n\n\nDep. Variable:\ngave\nNo. Observations:\n50083\n\n\nModel:\nProbit\nDf Residuals:\n50081\n\n\nMethod:\nMLE\nDf Model:\n1\n\n\nDate:\nSun, 14 Apr 2024\nPseudo R-squ.:\n0.0009783\n\n\nTime:\n21:43:24\nLog-Likelihood:\n-5030.5\n\n\nconverged:\nTrue\nLL-Null:\n-5035.4\n\n\nCovariance Type:\nnonrobust\nLLR p-value:\n0.001696\n\n\n\n\n\n\n\ncoef\nstd err\nz\nP&gt;|z|\n[0.025\n0.975]\n\n\nIntercept\n-2.1001\n0.023\n-90.073\n0.000\n-2.146\n-2.054\n\n\ntreatment\n0.0868\n0.028\n3.113\n0.002\n0.032\n0.141\n\n\n\n\n\n\ngave_ratio1 = karlan[karlan['ratio'] == 1]['gave']\ngave_ratio2 = karlan[karlan['ratio'] == 2]['gave']\n\ngave_1_mean = gave_ratio1.mean()\ngave_2_mean = gave_ratio2.mean()\n\ngave_1_std = gave_ratio1.std()\ngave_2_std = gave_ratio2.std()\n\ngave_1_n = gave_ratio1.count()\ngave_2_n = gave_ratio2.count()\n\nt_gave_ratio = (gave_2_mean - gave_1_mean) / ((gave_2_std**2/gave_2_n) + (gave_1_std**2/gave_1_n))**0.5\n\nt_gave_ratio\n\n0.965048975142932\n\n\n\ngave_1_mean, gave_2_mean\n\n(0.020749124225276205, 0.0226333752469912)\n\n\n\np_gave_ratio = stats.t.sf(abs(t_gave_ratio), gave_2_n + gave_1_n - 2) * 2\np_gave_ratio\n\n# Not statistically significant\n\n0.3345307635444439\n\n\n\ngave_ratio3 = karlan[karlan['ratio'] == 3]['gave']\ngave_ratio2 = karlan[karlan['ratio'] == 2]['gave']\n\ngave_3_mean = gave_ratio3.mean()\ngave_2_mean = gave_ratio2.mean()\n\ngave_3_std = gave_ratio3.std()\ngave_2_std = gave_ratio2.std()\n\ngave_3_n = gave_ratio3.count()\ngave_2_n = gave_ratio2.count()\n\nt_gave_ratio_23 = (gave_2_mean - gave_3_mean) / ((gave_2_std**2/gave_2_n) + (gave_3_std**2/gave_3_n))**0.5\n\nt_gave_ratio_23\n\n-0.05011581369764474\n\n\n\np_gave_ratio_23 = stats.t.sf(abs(t_gave_ratio_23), gave_2_n + gave_3_n - 2) * 2\np_gave_ratio_23\n\n# Not statistically significant\n\n0.9600305476910405\n\n\nThe above calculations matches what Karlan suggests in his paper, that increasing the match ratio does not increase the probability of making a donation. The above calculations show a 1:1 match ratio when compared to a 2:1 match ratio, and a 2:1 match ratio when compared to a 3:1 match ratio. We did a two-sided t-test, which has a null hypothesis stating that 2:1 is not the same as 1:1, and 3:1 is not the same as 2:1.\n\n#create a variable ratio1 where if ratio column equals 1, then ratio1 equlas 1, else 0\n\nkarlan['ratio1'] = karlan['ratio'].apply(lambda x: 1 if x == 1 else 0)\n\n\n# Linear regression for match ratios and treatment\n\nlr_ratio = rsm.regress(\n    data = karlan[['gave', 'ratio1', 'ratio2', 'ratio3']],\n    evar = ['ratio1', 'ratio2', 'ratio3'],\n    rvar = 'gave'\n    )  \n\nlr_ratio.summary()\n\nLinear regression (OLS)\nData                 : Not provided\nResponse variable    : gave\nExplanatory variables: ratio1, ratio2, ratio3\nNull hyp.: the effect of x on gave is zero\nAlt. hyp.: the effect of x on gave is not zero\n\n           coefficient  std.error  t.value p.value     \nIntercept        0.018      0.001   16.225  &lt; .001  ***\nratio1           0.003      0.002    1.661   0.097    .\nratio2           0.005      0.002    2.744   0.006   **\nratio3           0.005      0.002    2.802   0.005   **\n\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nR-squared: 0.0, Adjusted R-squared: 0.0\nF-statistic: 3.665 df(3, 50079), p.value 0.012\nNr obs: 50,083\n\n\nThe results of linear regression model on match ratios of 1:1, 2:1, and 3:1 show that all but a ratio of 1:1 are statistically significant. The intercept coefficient (meaning when there is no match) is 0.018, and then for ratio 1:1 it has a coefficient of 0.003, which added to 0.018 is 0.021. For ratio 2:1 and 3:1, they both have coefficients of 0.005, creating a response rate of 0.023 for both. All these response rates match table 2A in the paper.\nThe precision of these estimates also match what is in the paper for standard error. Each ratio has a standard error of 0.002, meaning that ratio 1:1 could have a varying effect on response rate between 0.001 and 0.005, and ratios 2:1 and 3:1 could vary between 0.003 and 0.007. All however remain positive when incorporating standard error, meaning that the match ratio does have a positive effect on response rate when comparing to control.\nThe difference in responsive rate between 1:1 and 2:1 match ratios is 0.002, and between 2:1 and 3:1 match ratios is 0. From this, we can conclude that increasing makes a very small difference from 1:1 to 2:1 but no difference between 2:1 and 3:1. Overall, increasing the match donation above 1:1 does not make a large difference in response rate.\n\nlr_d_amount = rsm.regress(\n    data = karlan[['amount', 'treatment']],\n    evar = 'treatment',\n    rvar = 'amount'\n    )  \n\nlr_d_amount.summary()\n\nLinear regression (OLS)\nData                 : Not provided\nResponse variable    : amount\nExplanatory variables: treatment\nNull hyp.: the effect of x on amount is zero\nAlt. hyp.: the effect of x on amount is not zero\n\n           coefficient  std.error  t.value p.value     \nIntercept        0.813      0.067   12.063  &lt; .001  ***\ntreatment        0.154      0.083    1.861   0.063    .\n\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nR-squared: 0.0, Adjusted R-squared: 0.0\nF-statistic: 3.461 df(1, 50081), p.value 0.063\nNr obs: 50,083\n\n\nFrom our linear regression model to calculate dollars donated amount from whether the donor was in the treatment or control group, we can conclude that the control group donates $0.813 and being a part of the treatment group increases that amount by $0.154, bringing the estimated dollars donated amount to $0.967. Per a 95% confidence interval, the model is not statistically significant, however it is very close to being so.\n\nlr_d_amount_above0 = rsm.regress(\n    data = karlan[karlan['amount'] &gt; 0][['amount', 'treatment']],\n    evar = 'treatment',\n    rvar = 'amount'\n    )  \n\nlr_d_amount_above0.summary()\n\nLinear regression (OLS)\nData                 : Not provided\nResponse variable    : amount\nExplanatory variables: treatment\nNull hyp.: the effect of x on amount is zero\nAlt. hyp.: the effect of x on amount is not zero\n\n           coefficient  std.error  t.value p.value     \nIntercept       45.540      2.423   18.792  &lt; .001  ***\ntreatment       -1.668      2.872   -0.581   0.561     \n\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nR-squared: 0.0, Adjusted R-squared: -0.001\nF-statistic: 0.337 df(1, 1032), p.value 0.561\nNr obs: 1,034\n\n\nAfter filtering the data to only include those who donated, the linear regression model changed significantly. The intercept coefficient is now $45.54, and the treatment coefficient is now negative $1.67. Now, the treatment group has a negative affect on amount donated, meaning that the control group donates $45.54 and the treatment group donates $43.87.\nHowever, the model is not statistically significant at 95% confidence interval.\n\n# create a histogram for all the donation amounts above 0 for the control group\n# have bin intervals be 25\n\nfig, ax = plt.subplots()\nax.hist(karlan[(karlan['amount'] &gt; 0) & (karlan['treatment'] == 0)]['amount'], bins=25)\nax.axvline(43.87, color='red')\nax.text(43.87 + 1, ax.get_ylim()[1] * 0.9, f'{43.87}', color='red')\nax.set_ylabel('Frequency')\nax.set_xlabel('Donation Amount')\nax.set_title('Frequency of Donation Amounts for Control Group')\nplt.show()\n\n\n\n\n\n\n\n\n\n\nfig, ax = plt.subplots()\nax.hist(karlan[(karlan['amount'] &gt; 0) & (karlan['treatment'] == 1)]['amount'], bins=25)\nax.axvline(45.54, color='red')\nax.text(45.54 + 1, ax.get_ylim()[1] * 0.9, f'{45.54}', color='red')\nax.set_ylabel('Frequency')\nax.set_xlabel('Donation Amount')\nax.set_title('Frequency of Donation Amounts for Treatment Group')\nplt.show()\n\n# add a line to the histogram at 43.87 and display the number 43.87\n\n\n\n\n\n\n\n\n\n\n\n\n\n# simulate 10,000 draws from the control distribution and 10,000 draws from the treatment distribution. You'll then calculate a vector of 10,000 differences, and then you'll plot the cumulative average of that vector of differences. Comment on whether the cumulative average approaches the true difference in means.\n\nimport numpy as np\nfrom scipy.stats import bernoulli\n\ncontrol = karlan[karlan['treatment'] == 0]['gave'].mean()\ntreatment = karlan[karlan['treatment'] == 1]['gave'].mean()\n\nprint(control, treatment)\n\n(0.017858212980164198, 0.02203856749311295)\n\n\n\ncontrol_sim = bernoulli.rvs(p = control, size = 10000)\ntreatment_sim = bernoulli.rvs(p = treatment, size = 10000)\n\n\ncontrol_sim.mean(), treatment_sim.mean()\n\n(0.0163, 0.0213)\n\n\n\n# calculate cumulative average of the differences for the first 10,000 draws\n\ncum_avg = np.cumsum(treatment_sim - control_sim) / np.arange(1, 10001)\n\ncum_avg\n\narray([0.       , 0.       , 0.       , ..., 0.005001 , 0.0050005,\n       0.005    ])\n\n\n\n# plot the cumulative average of the differences with a line plot\n\nfig, ax = plt.subplots()\nax.plot(cum_avg)\nax.axhline(treatment - control, color='red')\nax.set_ylabel('Cumulative Average of Differences')\nax.set_xlabel('Number of Draws')\nax.set_title('Cumulative Average of Differences in Proportion of Giving Money')\nplt.show()\n\n\n\n\n\n\n\n\nThe above graph shows a simulation of the differences between simulated probabilities of an individual actually donating in the treatment group and an individual donating in the control group. The control group was given a probability of 0.018 and treatment group was given a probability of 0.022, which were both calculating from the given dataset.\nWe simulated this for the control and treatment group 10,000 times to generate the graph. The red horizontal line on the graph shows the calculated difference between the treatment and control probabilities from the dataset (0.004).\nAs shown, the cumulative difference between the treatment and control group varies greatly initially, then slowly starts to vary less and less around the average difference of 0.004. With more trials, it is expected there would be even less variability and the cumulative difference would be extremely close to 0.004.\n\nn_50_c = np.random.binomial(50, 0.018, 1000)\n\nn_50_c = n_50_c / 50\n\nn_50_t = np.random.binomial(50, 0.022, 1000)\n\nn_50_t = n_50_t / 50\n\nn_50 = n_50_t - n_50_c\n\n\n# make a histogram of n_50 so there is no spacing between the bars\n\nfig, ax = plt.subplots()\nax.hist(n_50, bins=10, rwidth=1)\nax.axvline(n_50.mean(), color='red')\nax.text(n_50.mean(), ax.get_ylim()[1], f'{np.round(n_50.mean(),4)}', color='red', verticalalignment='center', horizontalalignment='center')\nax.set_ylabel('Frequency')\nax.set_xlabel('Difference in Proportion of Donors (Treatment - Control) in 50 Draws')\nax.set_title('Frequency of Proportion of Control Group that Gave Money in 50 Draws')\nplt.show()\n\n\n\n\n\n\n\n\n\nn_200_c = np.random.binomial(200, 0.018, 1000)/200\n\nn_200_t = np.random.binomial(200, 0.022, 1000)/200\n\n\nn_200 = n_200_t - n_200_c\n\nn_200_mean = n_200.mean()\n\nn_200_mean\n\n0.00452\n\n\n\nfig, ax = plt.subplots()\nax.hist(n_200, bins=10, rwidth=1)\nax.axvline(n_200_mean, color='red')\nax.text(n_200_mean, ax.get_ylim()[1], f'{np.round(n_200_mean,4)}', color='red', verticalalignment='center', horizontalalignment='center')\nax.set_ylabel('Frequency')\nax.set_xlabel('Difference in Proportion of Donors (Treatment - Control) in 200 Draws')\nax.set_title('Frequency of Proportion of Control Group that Gave Money in 200 Draws')\nplt.show()\n\n\n\n\n\n\n\n\n\nn_500_c = np.random.binomial(500, 0.018, 1000)/500\n\nn_500_t = np.random.binomial(500, 0.022, 1000)/500\n\n\nn_500 = n_500_t - n_500_c\n\nn_500_mean = n_500.mean()\n\nn_500_mean\n\n0.0040739999999999995\n\n\n\nfig, ax = plt.subplots()\nax.hist(n_500, bins=10, rwidth=1)\nax.axvline(n_500_mean, color='red')\nax.text(n_500_mean, ax.get_ylim()[1], f'{np.round(n_500_mean,4)}', color='red', verticalalignment='center', horizontalalignment='center')\nax.set_ylabel('Frequency')\nax.set_xlabel('Difference in Proportion of Donors (Treatment - Control) in 500 Draws')\nax.set_title('Frequency of Proportion of Control Group that Gave Money in 500 Draws')\nplt.show()\n\n\n\n\n\n\n\n\n\nn_1000_c = np.random.binomial(1000, 0.018, 1000)/1000\n\nn_1000_t = np.random.binomial(1000, 0.022, 1000)/1000\n\nn_1000 = n_1000_t - n_1000_c\n\nn_1000_mean = n_1000.mean()\n\nn_1000_mean\n\n0.004085\n\n\n\nfig, ax = plt.subplots()\nax.hist(n_1000, bins=10, rwidth=1)\nax.axvline(n_1000_mean, color='red')\nax.text(n_1000_mean, ax.get_ylim()[1]*0.9, f'{np.round(n_1000_mean,4)}', color='red', verticalalignment='center', horizontalalignment='right')\nax.set_ylabel('Frequency')\nax.set_xlabel('Difference in Proportion of Donors (Treatment - Control) in 500 Draws')\nax.set_title('Frequency of Proportion of Control Group that Gave Money in 500 Draws')\nplt.show()"
  },
  {
    "objectID": "projects/HW2/hw2_questions.html",
    "href": "projects/HW2/hw2_questions.html",
    "title": "Poisson Regression Examples",
    "section": "",
    "text": "Blueprinty is a small firm that makes software for developing blueprints specifically for submitting patent applications to the US patent office. Their marketing team would like to make the claim that patent applicants using Blueprinty’s software are more successful in getting their patent applications approved. Ideal data to study such an effect might include the success rate of patent applications before using Blueprinty’s software and after using it. unfortunately, such data is not available.\nHowever, Blueprinty has collected data on 1,500 mature (non-startup) engineering firms. The data include each firm’s number of patents awarded over the last 5 years, regional location, age since incorporation, and whether or not the firm uses Blueprinty’s software. The marketing team would like to use this data to make the claim that firms using Blueprinty’s software are more successful in getting their patent applications approved.\n\n\n\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom scipy.optimize import minimize\n\n# Load the data\n\ndata = pd.read_csv('blueprinty.csv')\n\ndata.head()\n\n\n\n\n\n\n\n\nUnnamed: 0\npatents\nregion\nage\niscustomer\n\n\n\n\n0\n1\n0\nMidwest\n32.5\n0\n\n\n1\n786\n3\nSouthwest\n37.5\n0\n\n\n2\n348\n4\nNorthwest\n27.0\n1\n\n\n3\n927\n3\nNortheast\n24.5\n0\n\n\n4\n830\n3\nSouthwest\n37.0\n0\n\n\n\n\n\n\n\nAfter reading in the data, we want to confirm that all businesses are unique and are not listed twice in the data\n\n# count the number of unique values in the column 'Unnamed: 0'\n\ndata['Unnamed: 0'].nunique()\n\n1500\n\n\nEach row is fact for an business which is not repeated in the dataset.\n\n# count number of customers and non-customers\n\ncustomers = data[data['iscustomer']==1]['iscustomer'].count()\nnon_customers = data[data['iscustomer']==0]['iscustomer'].count()\n\nprint(f' There are {customers} customers and {non_customers} non-customers in the dataset.')\n\n There are 197 customers and 1303 non-customers in the dataset.\n\n\nNow we will compare histograms based on number of patents for customers and non-customers on two separate plots.\n\n# histogram of # of patents for Customers\n\n\ndata[data['iscustomer'] == 1]['patents'].hist()\n\nplt.xlabel('Number of Patents')\nplt.ylabel('Frequency')\nplt.title('Frequency of Patents for Customers')\n\nText(0.5, 1.0, 'Frequency of Patents for Customers')\n\n\n\n\n\n\n\n\n\n\n# histogram of # of patents for non-customers\n\n\ndata[data['iscustomer'] == 0]['patents'].hist()\n\nplt.xlabel('Number of Patents')\nplt.ylabel('Frequency')\nplt.title('Frequency of Patents for Customers')\n\nText(0.5, 1.0, 'Frequency of Patents for Customers')\n\n\n\n\n\n\n\n\n\nNext, we’ll look at mean number of patents for customers and non-customers so we have a baseline for the histograms\n\nmean_patents_customers = data[data['iscustomer'] == 1]['patents'].mean()\nmean_patents_noncustomers = data[data['iscustomer'] == 0]['patents'].mean()\n\nprint('Mean patents for customers:', round(mean_patents_customers,2))\nprint('Mean patents for non-customers:', round(mean_patents_noncustomers,2))\n\nMean patents for customers: 4.09\nMean patents for non-customers: 3.62\n\n\nThe number of patents for customers is slightly skewed right, however it has a more normal distribution than the number of patents for non-customers.\nBoth plots have a large drop off around 6 patents (For customers it is slightly less than 6). The number of non-customers is significantly higher than the number of customers, totaling to 1303 non-customers and 197 customers. The mean number of patents for customers was 4.09 and the mean number of patents for non-customers was 3.62. This is a difference of about 0.5 patents. The number of customers and non-customers is important to keep into account when conducting regression models, as non-customers have a higher weight due to the higher frequency of occurence in the dataset.\nBlueprinty customers are not selected at random. It may be important to account for systematic differences in the age and regional location of customers vs non-customers.\nFirst, we’ll create a table showing the counts for each region based on customers and non-customers. We’ll looking at the proportion which each region has respectively for customers and non-customers.\n\n# group by region and count number of customers and non-customers\n\nregion = data.groupby('region')['iscustomer'].value_counts().unstack()\n\n# Calculate respective proportion of non-customers and customers which make up each region\n\nregion['Prop_Non_cust'] = region[0]/(region[0].sum())\nregion['Prop_Cust'] = region[1]/(region[1].sum())\n\n# Rename columns from 0 to Non-Customers and 1 to Customers\n\nregion.rename(columns={0:'Non-Customers', 1:'Customers'}, inplace=True)\n\nregion\n\n\n\n\n\n\n\niscustomer\nNon-Customers\nCustomers\nProp_Non_cust\nProp_Cust\n\n\nregion\n\n\n\n\n\n\n\n\nMidwest\n207\n17\n0.158864\n0.086294\n\n\nNortheast\n488\n113\n0.374520\n0.573604\n\n\nNorthwest\n171\n16\n0.131236\n0.081218\n\n\nSouth\n171\n20\n0.131236\n0.101523\n\n\nSouthwest\n266\n31\n0.204144\n0.157360\n\n\n\n\n\n\n\nBelow, you’ll see a plot of the customers and non-customers by region.\n\n# create a plot of the number of customers and non-customers by region side by side\n\ndata.groupby('region')['iscustomer'].value_counts().unstack().plot(kind='bar', stacked=False)\n\nplt.xlabel('Region')\nplt.ylabel('Count')\nplt.title('Number of Customers and Non-Customers by Region')\nplt.legend(['Non-Customer', 'Customer'])\n\n\n\n\n\n\n\n\nThe Northeast has by far the most customers and non-customers, and the number of non-customers in each region clearly outweighs the number of customers. Although ranking each respective customer and non-customer base by region comes out to be nearly the same ranking, the proportions are different.\nFor the customers, nearly 60% are from the NE, while only 40% of non-customers are from the NE.\nNext, we’ll do the same as we did for regions, except with age. One variation between the two will be binning the age groups for every 5 years. In this case, 0-5 years is one group, 5-10 years is another group, etc.\n\n# group by age with bins every 5 years and count number of customers and non-customers\n\ndata['age_bins'] = pd.cut(data['age'], bins=range(0, 60, 5))\n\nage = data.groupby('age_bins')['iscustomer'].value_counts().unstack()\nage['Prop_Non_cust'] = age[0]/(age[0].sum())\nage['Prop_Cust'] = age[1]/(age[1].sum())\n\nage.rename(columns={0:'Non-Customers', 1:'Customers'}, inplace=True)\n\nage\n\n/tmp/ipykernel_14747/1316710865.py:5: FutureWarning:\n\nThe default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n\n\n\n\n\n\n\n\n\niscustomer\nNon-Customers\nCustomers\nProp_Non_cust\nProp_Cust\n\n\nage_bins\n\n\n\n\n\n\n\n\n(0, 5]\n0\n0\n0.000000\n0.000000\n\n\n(5, 10]\n3\n3\n0.002302\n0.015228\n\n\n(10, 15]\n52\n15\n0.039908\n0.076142\n\n\n(15, 20]\n213\n53\n0.163469\n0.269036\n\n\n(20, 25]\n318\n51\n0.244052\n0.258883\n\n\n(25, 30]\n301\n32\n0.231005\n0.162437\n\n\n(30, 35]\n244\n24\n0.187260\n0.121827\n\n\n(35, 40]\n135\n13\n0.103607\n0.065990\n\n\n(40, 45]\n34\n5\n0.026094\n0.025381\n\n\n(45, 50]\n3\n1\n0.002302\n0.005076\n\n\n(50, 55]\n0\n0\n0.000000\n0.000000\n\n\n\n\n\n\n\n\n# create a plot of the number of customers and non-customers by age side by side\n\ndata.groupby('age_bins')['iscustomer'].value_counts().unstack().plot(kind='bar', stacked=False)\nplt.xlabel('Age of Company')\nplt.ylabel('Count')\nplt.title('Number of Customers and Non-Customers by Age of Company')\nplt.legend(['Non-Customer', 'Customer'])\n\n/tmp/ipykernel_14747/196641801.py:3: FutureWarning:\n\nThe default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n\n\n\n\n\n\n\n\n\n\nFor age, we’ll also look at the mean age of customers and non-customers, and calculate the 95% confidence intervals.\n\n# find mean age for customers and non-customers\n\nmean_age_customers = data[data['iscustomer'] == 1]['age'].mean()\nmean_age_noncustomers = data[data['iscustomer'] == 0]['age'].mean()\n\nprint('Mean age for customers:', round(mean_age_customers,2))\nprint('Mean age for non-customers:', round(mean_age_noncustomers,2))\n\nMean age for customers: 24.15\nMean age for non-customers: 26.69\n\n\n\n# Calculate a 95% confidence interval for the mean age of customers and non-customers\n\nimport numpy as np\n\nstd_age_customers = data[data['iscustomer'] == 1]['age'].std()\nstd_age_noncustomers = data[data['iscustomer'] == 0]['age'].std()\n\nn_customers = data[data['iscustomer'] == 1]['age'].count()\nn_noncustomers = data[data['iscustomer'] == 0]['age'].count()\n\nz = 1.96\n\nci_customers = z * (std_age_customers/np.sqrt(n_customers))\nci_noncustomers = z * (std_age_noncustomers/np.sqrt(n_noncustomers))\n\nprint('95% CI for mean age of customers:', round(mean_age_customers-ci_customers,2), round(mean_age_customers+ci_customers,2))\nprint('95% CI for mean age of non-customers:', round(mean_age_noncustomers-ci_noncustomers,2), round(mean_age_noncustomers+ci_noncustomers,2))\n\n95% CI for mean age of customers: 23.09 25.21\n95% CI for mean age of non-customers: 26.3 27.08\n\n\nThe most customers come from companies which are between 15-20 years old, with the average company age being about 24 years. The most non-customers come from companies which are between 20-25 years old, with the average company age being about 27 years.\nThe distribution of company ages for customers and non-customers resembles a normal distribution, with a slight skew to the right. The largest disparity between the two distributions is at the 10-15 year mark, where the % of non-customers is 10% lower than that of customers (16%, 26%).\nThe confidence interval for the mean age of customers is (23.1, 25.2) and the confidence interval for the mean age of non-customers is (26.3, 27.1).\n\n\n\nSince our outcome variable of interest can only be small integer values per a set unit of time, we can use a Poisson density to model the number of patents awarded to each engineering firm over the last 5 years. We start by estimating a simple Poisson model via Maximum Likelihood.\nBelow is the log-likelihood function for the Poisson distribution. The likelihood function of lambda given Y is the exact same as the function of Y given lambda.\nℓ(λ∣Y)=−λ+Ylog(λ)−log(Y!)\nBelow is the code for the log-likelihood function for a poisson distribution:\n\ndef poisson_log_likelihood(lam, y):\n    \"\"\"\n    Parameters:\n    - lam (float): The rate parameter (lambda) of the Poisson distribution.\n    - y (array-like): Array of observed counts.\n\n    Returns:\n    - float: The log likelihood of observing the data given lam.\n    \"\"\"\n    y = np.array(y)\n    n = len(y)  # number of observations\n    sum_y = np.sum(y)  # sum of all observed counts\n\n    # Calculate each part of the log likelihood\n    # log(P(Y|lam)) = -n * lam + sum_y * log(lam) - log(y_i!)\n    # We use np.sum(np.log(y_factorials)) to sum log of factorials\n    log_likelihood = -n * lam + sum_y * np.log(lam) - np.sum([np.log(np.math.factorial(i)) for i in y])\n    return log_likelihood\n\nNext, we’ll plot the log-likelihoods using our observed number of lambdas as Y and then a range of values for lambda (1-10).\n\nyears = range(1,11)\nlog_likelihood_values = []\n\nfor i in years:\n    log_likelihood_value = poisson_log_likelihood(i, data['patents'])\n    log_likelihood_values.append(log_likelihood_value)\n\nlog_likelihood_values\n\n/tmp/ipykernel_14747/1539929848.py:17: DeprecationWarning:\n\n`np.math` is a deprecated alias for the standard library `math` module (Deprecated Numpy 1.25). Replace usages of `np.math` with `math`\n\n\n\n[-6548.886990069443,\n -4217.862523114625,\n -3476.856870600801,\n -3386.838056159808,\n -3653.52364804617,\n -4145.8324036459835,\n -4793.841596240727,\n -5555.81358920499,\n -6404.826751132159,\n -7322.4991810913525]\n\n\n\nplt.plot(years, log_likelihood_values)\nplt.xlabel('Lambda')\nplt.ylabel('Likelihood')\nplt.title('Likelihood of observing the data given Lambda')\nplt.show()\n\n\n\n\n\n\n\n\nWe’ll now create our negative poisson MLE function and analyze the output betas.\n\nfrom scipy.optimize import minimize\n\ndef neg_poisson_log_likelihood(lam, y):\n    \"\"\"\n    Parameters:\n    - lam (float): The rate parameter (lambda) of the Poisson distribution.\n    - y (array-like): Array of observed counts.\n\n    Returns:\n    - float: The log likelihood of observing the data given lam.\n    \"\"\"\n    y = np.array(y)\n    n = len(y)  # number of observations\n    sum_y = np.sum(y)  # sum of all observed counts\n\n    # Calculate each part of the log likelihood\n    # log(P(Y|lam)) = -n * lam + sum_y * log(lam) - log(y_i!)\n    # We use np.sum(np.log(y_factorials)) to sum log of factorials\n    return -(-n * lam + sum_y * np.log(lam) - np.sum([np.log(np.math.factorial(i)) for i in y]))\n\n\nmean = np.mean(data['patents'])\n\nresult = minimize(neg_poisson_log_likelihood, mean, args=(data['patents']), bounds = [(0, None)])\n\nresult\n\n/tmp/ipykernel_14747/2966890401.py:19: DeprecationWarning:\n\n`np.math` is a deprecated alias for the standard library `math` module (Deprecated Numpy 1.25). Replace usages of `np.math` with `math`\n\n\n\n  message: CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_&lt;=_PGTOL\n  success: True\n   status: 0\n      fun: 3367.683772235094\n        x: [ 3.685e+00]\n      nit: 0\n      jac: [ 0.000e+00]\n     nfev: 2\n     njev: 1\n hess_inv: &lt;1x1 LbfgsInvHessProduct with dtype=float64&gt;\n\n\n\n\n\nNext, we extend our simple Poisson model to a Poisson Regression Model such that \\(Y_i = \\text{Poisson}(\\lambda_i)\\) where \\(\\lambda_i = \\exp(X_i'\\beta)\\). The interpretation is that the success rate of patent awards is not constant across all firms (\\(\\lambda\\)) but rather is a function of firm characteristics \\(X_i\\). Specifically, we will use the covariates age, age squared, region, and whether the firm is a customer of Blueprinty.\nFirst, we’ll create our new poisson MLE function, incorporating our explanatory variables into the calculation.\n\nimport numpy as np\nfrom scipy.optimize import minimize\n\ndef poisson_regression_log_likelihood(beta, Y, X):\n    eta = np.dot(X, beta)\n    lambda_i = np.exp(eta)\n    log_likelihood = np.sum(Y * eta - lambda_i)\n    return -log_likelihood\n\nNext, we’ll change the format of the data to be able to be used in our functions and analysis.\n\n# convert regions column to boolean columns, dropping the first region to be the default value\n\ndata = pd.get_dummies(data, columns=['region'], drop_first=True)\n\n# creating function to convert boolean column to binary\n\ndef convert_boolean_to_binary(data, column):\n    data[column] = data[column].astype(int)\n    return data\n\n# coverting region's boolean values to binary\n\ndata = convert_boolean_to_binary(data, 'region_Northeast')\ndata = convert_boolean_to_binary(data, 'region_South')\ndata = convert_boolean_to_binary(data, 'region_Southwest')\ndata = convert_boolean_to_binary(data, 'region_Northwest')\n\n# creating an age^2 column in the dataset\n\ndata['age_squared'] = data['age']**2\n\n\n# Load and preprocess your data as before, ensuring that features are scaled\nfrom sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\n# Scaling age and age_squared to prevent precision loss from extremely large numbers\ndata['age'] = scaler.fit_transform(data[['age']])\ndata['age_squared'] = scaler.fit_transform(data[['age_squared']])\n\n\n# Defining X and Y variables\nX = np.c_[np.ones(len(data)), data['age'], data['age_squared'], data['iscustomer'], data['region_Southwest'], data['region_Northwest'],\n          data['region_Northeast'], data['region_South']]\nY = data['patents'].values\n\n# Initial guess for beta (0)\ninitial_beta = np.zeros(X.shape[1])\n\n# Minimization\nresult = minimize(poisson_regression_log_likelihood, initial_beta, args=(Y, X), method='BFGS')\n\nprint(\"Optimal beta:\", result.x)\n\nOptimal beta: [ 1.21543809  1.04645982 -1.14084528  0.11811438  0.05134699 -0.02009421\n  0.098596    0.05717198]\n\n\nNow that we calculated our betas from our self-built function, we’ll verify our betas with a built-in regression function.\n\nimport statsmodels.api as sm\n\n# Fit a Poisson regression model using statsmodels\n\npoisson_model = sm.GLM(Y, X, family=sm.families.Poisson()).fit()\n\n# Print the summary of the model\n\nprint(poisson_model.summary())\n\n                 Generalized Linear Model Regression Results                  \n==============================================================================\nDep. Variable:                      y   No. Observations:                 1500\nModel:                            GLM   Df Residuals:                     1492\nModel Family:                 Poisson   Df Model:                            7\nLink Function:                    Log   Scale:                          1.0000\nMethod:                          IRLS   Log-Likelihood:                -3275.9\nDate:                Wed, 01 May 2024   Deviance:                       2178.8\nTime:                        18:08:07   Pearson chi2:                 2.11e+03\nNo. Iterations:                     5   Pseudo R-squ. (CS):             0.1152\nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          z      P&gt;|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst          1.2154      0.036     33.368      0.000       1.144       1.287\nx1             1.0465      0.100     10.414      0.000       0.850       1.243\nx2            -1.1408      0.102    -11.131      0.000      -1.342      -0.940\nx3             0.1181      0.039      3.035      0.002       0.042       0.194\nx4             0.0513      0.047      1.088      0.277      -0.041       0.144\nx5            -0.0201      0.054     -0.374      0.709      -0.126       0.085\nx6             0.0986      0.042      2.347      0.019       0.016       0.181\nx7             0.0572      0.053      1.085      0.278      -0.046       0.160\n==============================================================================\n\n\nFinally, we’ll calculate our standard errors for each respective beta or coefficient and compare to our built-in regression analysis results.\n\n# Calculate Hessian at the optimal beta\nfrom scipy.linalg import inv\nhessian_inv = result.hess_inv  # Inverse Hessian is returned by BFGS\n\n# Calculating standard errors by taking the square roots of the diagonal elements of the inverse Hessian\nstd_errors = np.sqrt(np.diag(hessian_inv))\n\nprint(\"Standard Errors:\", std_errors)\n\nStandard Errors: [0.02272912 0.14333007 0.14451598 0.03888018 0.03676005 0.04672971\n 0.02526631 0.02849847]\n\n\nWe can conclude, based on our optimal beta’s through our regression model, that Blueprinty’s software has a positive effect on the number of patents awarded to a company. The coefficient or beta calculated for “iscustomer” is 0.11, meaning if they are a customer of the software, the humber of patents earned increases by 0.11.\nWe also see that p-value of “iscustomer” is 0.002, meaning there is only a 0.2% chance that the coefficient has zero affect on the number of patents given the dataset.\nOur standard errors we’re slightly off for some of the variables, which may mean our self-built function has an issue, or we are using a different optimization method (ie. BFGS, L-BFGS-B) than the built-in function."
  },
  {
    "objectID": "projects/HW2/hw2_questions.html#blueprinty-case-study",
    "href": "projects/HW2/hw2_questions.html#blueprinty-case-study",
    "title": "Poisson Regression Examples",
    "section": "",
    "text": "Blueprinty is a small firm that makes software for developing blueprints specifically for submitting patent applications to the US patent office. Their marketing team would like to make the claim that patent applicants using Blueprinty’s software are more successful in getting their patent applications approved. Ideal data to study such an effect might include the success rate of patent applications before using Blueprinty’s software and after using it. unfortunately, such data is not available.\nHowever, Blueprinty has collected data on 1,500 mature (non-startup) engineering firms. The data include each firm’s number of patents awarded over the last 5 years, regional location, age since incorporation, and whether or not the firm uses Blueprinty’s software. The marketing team would like to use this data to make the claim that firms using Blueprinty’s software are more successful in getting their patent applications approved.\n\n\n\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom scipy.optimize import minimize\n\n# Load the data\n\ndata = pd.read_csv('blueprinty.csv')\n\ndata.head()\n\n\n\n\n\n\n\n\nUnnamed: 0\npatents\nregion\nage\niscustomer\n\n\n\n\n0\n1\n0\nMidwest\n32.5\n0\n\n\n1\n786\n3\nSouthwest\n37.5\n0\n\n\n2\n348\n4\nNorthwest\n27.0\n1\n\n\n3\n927\n3\nNortheast\n24.5\n0\n\n\n4\n830\n3\nSouthwest\n37.0\n0\n\n\n\n\n\n\n\nAfter reading in the data, we want to confirm that all businesses are unique and are not listed twice in the data\n\n# count the number of unique values in the column 'Unnamed: 0'\n\ndata['Unnamed: 0'].nunique()\n\n1500\n\n\nEach row is fact for an business which is not repeated in the dataset.\n\n# count number of customers and non-customers\n\ncustomers = data[data['iscustomer']==1]['iscustomer'].count()\nnon_customers = data[data['iscustomer']==0]['iscustomer'].count()\n\nprint(f' There are {customers} customers and {non_customers} non-customers in the dataset.')\n\n There are 197 customers and 1303 non-customers in the dataset.\n\n\nNow we will compare histograms based on number of patents for customers and non-customers on two separate plots.\n\n# histogram of # of patents for Customers\n\n\ndata[data['iscustomer'] == 1]['patents'].hist()\n\nplt.xlabel('Number of Patents')\nplt.ylabel('Frequency')\nplt.title('Frequency of Patents for Customers')\n\nText(0.5, 1.0, 'Frequency of Patents for Customers')\n\n\n\n\n\n\n\n\n\n\n# histogram of # of patents for non-customers\n\n\ndata[data['iscustomer'] == 0]['patents'].hist()\n\nplt.xlabel('Number of Patents')\nplt.ylabel('Frequency')\nplt.title('Frequency of Patents for Customers')\n\nText(0.5, 1.0, 'Frequency of Patents for Customers')\n\n\n\n\n\n\n\n\n\nNext, we’ll look at mean number of patents for customers and non-customers so we have a baseline for the histograms\n\nmean_patents_customers = data[data['iscustomer'] == 1]['patents'].mean()\nmean_patents_noncustomers = data[data['iscustomer'] == 0]['patents'].mean()\n\nprint('Mean patents for customers:', round(mean_patents_customers,2))\nprint('Mean patents for non-customers:', round(mean_patents_noncustomers,2))\n\nMean patents for customers: 4.09\nMean patents for non-customers: 3.62\n\n\nThe number of patents for customers is slightly skewed right, however it has a more normal distribution than the number of patents for non-customers.\nBoth plots have a large drop off around 6 patents (For customers it is slightly less than 6). The number of non-customers is significantly higher than the number of customers, totaling to 1303 non-customers and 197 customers. The mean number of patents for customers was 4.09 and the mean number of patents for non-customers was 3.62. This is a difference of about 0.5 patents. The number of customers and non-customers is important to keep into account when conducting regression models, as non-customers have a higher weight due to the higher frequency of occurence in the dataset.\nBlueprinty customers are not selected at random. It may be important to account for systematic differences in the age and regional location of customers vs non-customers.\nFirst, we’ll create a table showing the counts for each region based on customers and non-customers. We’ll looking at the proportion which each region has respectively for customers and non-customers.\n\n# group by region and count number of customers and non-customers\n\nregion = data.groupby('region')['iscustomer'].value_counts().unstack()\n\n# Calculate respective proportion of non-customers and customers which make up each region\n\nregion['Prop_Non_cust'] = region[0]/(region[0].sum())\nregion['Prop_Cust'] = region[1]/(region[1].sum())\n\n# Rename columns from 0 to Non-Customers and 1 to Customers\n\nregion.rename(columns={0:'Non-Customers', 1:'Customers'}, inplace=True)\n\nregion\n\n\n\n\n\n\n\niscustomer\nNon-Customers\nCustomers\nProp_Non_cust\nProp_Cust\n\n\nregion\n\n\n\n\n\n\n\n\nMidwest\n207\n17\n0.158864\n0.086294\n\n\nNortheast\n488\n113\n0.374520\n0.573604\n\n\nNorthwest\n171\n16\n0.131236\n0.081218\n\n\nSouth\n171\n20\n0.131236\n0.101523\n\n\nSouthwest\n266\n31\n0.204144\n0.157360\n\n\n\n\n\n\n\nBelow, you’ll see a plot of the customers and non-customers by region.\n\n# create a plot of the number of customers and non-customers by region side by side\n\ndata.groupby('region')['iscustomer'].value_counts().unstack().plot(kind='bar', stacked=False)\n\nplt.xlabel('Region')\nplt.ylabel('Count')\nplt.title('Number of Customers and Non-Customers by Region')\nplt.legend(['Non-Customer', 'Customer'])\n\n\n\n\n\n\n\n\nThe Northeast has by far the most customers and non-customers, and the number of non-customers in each region clearly outweighs the number of customers. Although ranking each respective customer and non-customer base by region comes out to be nearly the same ranking, the proportions are different.\nFor the customers, nearly 60% are from the NE, while only 40% of non-customers are from the NE.\nNext, we’ll do the same as we did for regions, except with age. One variation between the two will be binning the age groups for every 5 years. In this case, 0-5 years is one group, 5-10 years is another group, etc.\n\n# group by age with bins every 5 years and count number of customers and non-customers\n\ndata['age_bins'] = pd.cut(data['age'], bins=range(0, 60, 5))\n\nage = data.groupby('age_bins')['iscustomer'].value_counts().unstack()\nage['Prop_Non_cust'] = age[0]/(age[0].sum())\nage['Prop_Cust'] = age[1]/(age[1].sum())\n\nage.rename(columns={0:'Non-Customers', 1:'Customers'}, inplace=True)\n\nage\n\n/tmp/ipykernel_14747/1316710865.py:5: FutureWarning:\n\nThe default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n\n\n\n\n\n\n\n\n\niscustomer\nNon-Customers\nCustomers\nProp_Non_cust\nProp_Cust\n\n\nage_bins\n\n\n\n\n\n\n\n\n(0, 5]\n0\n0\n0.000000\n0.000000\n\n\n(5, 10]\n3\n3\n0.002302\n0.015228\n\n\n(10, 15]\n52\n15\n0.039908\n0.076142\n\n\n(15, 20]\n213\n53\n0.163469\n0.269036\n\n\n(20, 25]\n318\n51\n0.244052\n0.258883\n\n\n(25, 30]\n301\n32\n0.231005\n0.162437\n\n\n(30, 35]\n244\n24\n0.187260\n0.121827\n\n\n(35, 40]\n135\n13\n0.103607\n0.065990\n\n\n(40, 45]\n34\n5\n0.026094\n0.025381\n\n\n(45, 50]\n3\n1\n0.002302\n0.005076\n\n\n(50, 55]\n0\n0\n0.000000\n0.000000\n\n\n\n\n\n\n\n\n# create a plot of the number of customers and non-customers by age side by side\n\ndata.groupby('age_bins')['iscustomer'].value_counts().unstack().plot(kind='bar', stacked=False)\nplt.xlabel('Age of Company')\nplt.ylabel('Count')\nplt.title('Number of Customers and Non-Customers by Age of Company')\nplt.legend(['Non-Customer', 'Customer'])\n\n/tmp/ipykernel_14747/196641801.py:3: FutureWarning:\n\nThe default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n\n\n\n\n\n\n\n\n\n\nFor age, we’ll also look at the mean age of customers and non-customers, and calculate the 95% confidence intervals.\n\n# find mean age for customers and non-customers\n\nmean_age_customers = data[data['iscustomer'] == 1]['age'].mean()\nmean_age_noncustomers = data[data['iscustomer'] == 0]['age'].mean()\n\nprint('Mean age for customers:', round(mean_age_customers,2))\nprint('Mean age for non-customers:', round(mean_age_noncustomers,2))\n\nMean age for customers: 24.15\nMean age for non-customers: 26.69\n\n\n\n# Calculate a 95% confidence interval for the mean age of customers and non-customers\n\nimport numpy as np\n\nstd_age_customers = data[data['iscustomer'] == 1]['age'].std()\nstd_age_noncustomers = data[data['iscustomer'] == 0]['age'].std()\n\nn_customers = data[data['iscustomer'] == 1]['age'].count()\nn_noncustomers = data[data['iscustomer'] == 0]['age'].count()\n\nz = 1.96\n\nci_customers = z * (std_age_customers/np.sqrt(n_customers))\nci_noncustomers = z * (std_age_noncustomers/np.sqrt(n_noncustomers))\n\nprint('95% CI for mean age of customers:', round(mean_age_customers-ci_customers,2), round(mean_age_customers+ci_customers,2))\nprint('95% CI for mean age of non-customers:', round(mean_age_noncustomers-ci_noncustomers,2), round(mean_age_noncustomers+ci_noncustomers,2))\n\n95% CI for mean age of customers: 23.09 25.21\n95% CI for mean age of non-customers: 26.3 27.08\n\n\nThe most customers come from companies which are between 15-20 years old, with the average company age being about 24 years. The most non-customers come from companies which are between 20-25 years old, with the average company age being about 27 years.\nThe distribution of company ages for customers and non-customers resembles a normal distribution, with a slight skew to the right. The largest disparity between the two distributions is at the 10-15 year mark, where the % of non-customers is 10% lower than that of customers (16%, 26%).\nThe confidence interval for the mean age of customers is (23.1, 25.2) and the confidence interval for the mean age of non-customers is (26.3, 27.1).\n\n\n\nSince our outcome variable of interest can only be small integer values per a set unit of time, we can use a Poisson density to model the number of patents awarded to each engineering firm over the last 5 years. We start by estimating a simple Poisson model via Maximum Likelihood.\nBelow is the log-likelihood function for the Poisson distribution. The likelihood function of lambda given Y is the exact same as the function of Y given lambda.\nℓ(λ∣Y)=−λ+Ylog(λ)−log(Y!)\nBelow is the code for the log-likelihood function for a poisson distribution:\n\ndef poisson_log_likelihood(lam, y):\n    \"\"\"\n    Parameters:\n    - lam (float): The rate parameter (lambda) of the Poisson distribution.\n    - y (array-like): Array of observed counts.\n\n    Returns:\n    - float: The log likelihood of observing the data given lam.\n    \"\"\"\n    y = np.array(y)\n    n = len(y)  # number of observations\n    sum_y = np.sum(y)  # sum of all observed counts\n\n    # Calculate each part of the log likelihood\n    # log(P(Y|lam)) = -n * lam + sum_y * log(lam) - log(y_i!)\n    # We use np.sum(np.log(y_factorials)) to sum log of factorials\n    log_likelihood = -n * lam + sum_y * np.log(lam) - np.sum([np.log(np.math.factorial(i)) for i in y])\n    return log_likelihood\n\nNext, we’ll plot the log-likelihoods using our observed number of lambdas as Y and then a range of values for lambda (1-10).\n\nyears = range(1,11)\nlog_likelihood_values = []\n\nfor i in years:\n    log_likelihood_value = poisson_log_likelihood(i, data['patents'])\n    log_likelihood_values.append(log_likelihood_value)\n\nlog_likelihood_values\n\n/tmp/ipykernel_14747/1539929848.py:17: DeprecationWarning:\n\n`np.math` is a deprecated alias for the standard library `math` module (Deprecated Numpy 1.25). Replace usages of `np.math` with `math`\n\n\n\n[-6548.886990069443,\n -4217.862523114625,\n -3476.856870600801,\n -3386.838056159808,\n -3653.52364804617,\n -4145.8324036459835,\n -4793.841596240727,\n -5555.81358920499,\n -6404.826751132159,\n -7322.4991810913525]\n\n\n\nplt.plot(years, log_likelihood_values)\nplt.xlabel('Lambda')\nplt.ylabel('Likelihood')\nplt.title('Likelihood of observing the data given Lambda')\nplt.show()\n\n\n\n\n\n\n\n\nWe’ll now create our negative poisson MLE function and analyze the output betas.\n\nfrom scipy.optimize import minimize\n\ndef neg_poisson_log_likelihood(lam, y):\n    \"\"\"\n    Parameters:\n    - lam (float): The rate parameter (lambda) of the Poisson distribution.\n    - y (array-like): Array of observed counts.\n\n    Returns:\n    - float: The log likelihood of observing the data given lam.\n    \"\"\"\n    y = np.array(y)\n    n = len(y)  # number of observations\n    sum_y = np.sum(y)  # sum of all observed counts\n\n    # Calculate each part of the log likelihood\n    # log(P(Y|lam)) = -n * lam + sum_y * log(lam) - log(y_i!)\n    # We use np.sum(np.log(y_factorials)) to sum log of factorials\n    return -(-n * lam + sum_y * np.log(lam) - np.sum([np.log(np.math.factorial(i)) for i in y]))\n\n\nmean = np.mean(data['patents'])\n\nresult = minimize(neg_poisson_log_likelihood, mean, args=(data['patents']), bounds = [(0, None)])\n\nresult\n\n/tmp/ipykernel_14747/2966890401.py:19: DeprecationWarning:\n\n`np.math` is a deprecated alias for the standard library `math` module (Deprecated Numpy 1.25). Replace usages of `np.math` with `math`\n\n\n\n  message: CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_&lt;=_PGTOL\n  success: True\n   status: 0\n      fun: 3367.683772235094\n        x: [ 3.685e+00]\n      nit: 0\n      jac: [ 0.000e+00]\n     nfev: 2\n     njev: 1\n hess_inv: &lt;1x1 LbfgsInvHessProduct with dtype=float64&gt;\n\n\n\n\n\nNext, we extend our simple Poisson model to a Poisson Regression Model such that \\(Y_i = \\text{Poisson}(\\lambda_i)\\) where \\(\\lambda_i = \\exp(X_i'\\beta)\\). The interpretation is that the success rate of patent awards is not constant across all firms (\\(\\lambda\\)) but rather is a function of firm characteristics \\(X_i\\). Specifically, we will use the covariates age, age squared, region, and whether the firm is a customer of Blueprinty.\nFirst, we’ll create our new poisson MLE function, incorporating our explanatory variables into the calculation.\n\nimport numpy as np\nfrom scipy.optimize import minimize\n\ndef poisson_regression_log_likelihood(beta, Y, X):\n    eta = np.dot(X, beta)\n    lambda_i = np.exp(eta)\n    log_likelihood = np.sum(Y * eta - lambda_i)\n    return -log_likelihood\n\nNext, we’ll change the format of the data to be able to be used in our functions and analysis.\n\n# convert regions column to boolean columns, dropping the first region to be the default value\n\ndata = pd.get_dummies(data, columns=['region'], drop_first=True)\n\n# creating function to convert boolean column to binary\n\ndef convert_boolean_to_binary(data, column):\n    data[column] = data[column].astype(int)\n    return data\n\n# coverting region's boolean values to binary\n\ndata = convert_boolean_to_binary(data, 'region_Northeast')\ndata = convert_boolean_to_binary(data, 'region_South')\ndata = convert_boolean_to_binary(data, 'region_Southwest')\ndata = convert_boolean_to_binary(data, 'region_Northwest')\n\n# creating an age^2 column in the dataset\n\ndata['age_squared'] = data['age']**2\n\n\n# Load and preprocess your data as before, ensuring that features are scaled\nfrom sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\n# Scaling age and age_squared to prevent precision loss from extremely large numbers\ndata['age'] = scaler.fit_transform(data[['age']])\ndata['age_squared'] = scaler.fit_transform(data[['age_squared']])\n\n\n# Defining X and Y variables\nX = np.c_[np.ones(len(data)), data['age'], data['age_squared'], data['iscustomer'], data['region_Southwest'], data['region_Northwest'],\n          data['region_Northeast'], data['region_South']]\nY = data['patents'].values\n\n# Initial guess for beta (0)\ninitial_beta = np.zeros(X.shape[1])\n\n# Minimization\nresult = minimize(poisson_regression_log_likelihood, initial_beta, args=(Y, X), method='BFGS')\n\nprint(\"Optimal beta:\", result.x)\n\nOptimal beta: [ 1.21543809  1.04645982 -1.14084528  0.11811438  0.05134699 -0.02009421\n  0.098596    0.05717198]\n\n\nNow that we calculated our betas from our self-built function, we’ll verify our betas with a built-in regression function.\n\nimport statsmodels.api as sm\n\n# Fit a Poisson regression model using statsmodels\n\npoisson_model = sm.GLM(Y, X, family=sm.families.Poisson()).fit()\n\n# Print the summary of the model\n\nprint(poisson_model.summary())\n\n                 Generalized Linear Model Regression Results                  \n==============================================================================\nDep. Variable:                      y   No. Observations:                 1500\nModel:                            GLM   Df Residuals:                     1492\nModel Family:                 Poisson   Df Model:                            7\nLink Function:                    Log   Scale:                          1.0000\nMethod:                          IRLS   Log-Likelihood:                -3275.9\nDate:                Wed, 01 May 2024   Deviance:                       2178.8\nTime:                        18:08:07   Pearson chi2:                 2.11e+03\nNo. Iterations:                     5   Pseudo R-squ. (CS):             0.1152\nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          z      P&gt;|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst          1.2154      0.036     33.368      0.000       1.144       1.287\nx1             1.0465      0.100     10.414      0.000       0.850       1.243\nx2            -1.1408      0.102    -11.131      0.000      -1.342      -0.940\nx3             0.1181      0.039      3.035      0.002       0.042       0.194\nx4             0.0513      0.047      1.088      0.277      -0.041       0.144\nx5            -0.0201      0.054     -0.374      0.709      -0.126       0.085\nx6             0.0986      0.042      2.347      0.019       0.016       0.181\nx7             0.0572      0.053      1.085      0.278      -0.046       0.160\n==============================================================================\n\n\nFinally, we’ll calculate our standard errors for each respective beta or coefficient and compare to our built-in regression analysis results.\n\n# Calculate Hessian at the optimal beta\nfrom scipy.linalg import inv\nhessian_inv = result.hess_inv  # Inverse Hessian is returned by BFGS\n\n# Calculating standard errors by taking the square roots of the diagonal elements of the inverse Hessian\nstd_errors = np.sqrt(np.diag(hessian_inv))\n\nprint(\"Standard Errors:\", std_errors)\n\nStandard Errors: [0.02272912 0.14333007 0.14451598 0.03888018 0.03676005 0.04672971\n 0.02526631 0.02849847]\n\n\nWe can conclude, based on our optimal beta’s through our regression model, that Blueprinty’s software has a positive effect on the number of patents awarded to a company. The coefficient or beta calculated for “iscustomer” is 0.11, meaning if they are a customer of the software, the humber of patents earned increases by 0.11.\nWe also see that p-value of “iscustomer” is 0.002, meaning there is only a 0.2% chance that the coefficient has zero affect on the number of patents given the dataset.\nOur standard errors we’re slightly off for some of the variables, which may mean our self-built function has an issue, or we are using a different optimization method (ie. BFGS, L-BFGS-B) than the built-in function."
  },
  {
    "objectID": "projects/HW2/hw2_questions.html#airbnb-case-study",
    "href": "projects/HW2/hw2_questions.html#airbnb-case-study",
    "title": "Poisson Regression Examples",
    "section": "AirBnB Case Study",
    "text": "AirBnB Case Study\n\nIntroduction\nAirBnB is a popular platform for booking short-term rentals. In March 2017, students Annika Awad, Evan Lebo, and Anna Linden scraped of 40,000 Airbnb listings from New York City. The data include the following variables:\n\n\n\n\n\n\nVariable Definitions\n\n\n\n\n\n- `id` = unique ID number for each unit\n- `last_scraped` = date when information scraped\n- `host_since` = date when host first listed the unit on Airbnb\n- `days` = `last_scraped` - `host_since` = number of days the unit has been listed\n- `room_type` = Entire home/apt., Private room, or Shared room\n- `bathrooms` = number of bathrooms\n- `bedrooms` = number of bedrooms\n- `price` = price per night (dollars)\n- `number_of_reviews` = number of reviews for the unit on Airbnb\n- `review_scores_cleanliness` = a cleanliness score from reviews (1-10)\n- `review_scores_location` = a \"quality of location\" score from reviews (1-10)\n- `review_scores_value` = a \"quality of value\" score from reviews (1-10)\n- `instant_bookable` = \"t\" if instantly bookable, \"f\" if not\n\n\n\nUsing the airbnb data, we will build a likelihood function for this poisson regression. However, we first need to read the data and look at its characteristics.\n\n# read airbnb csv\n\nairbnb = pd.read_csv('airbnb.csv')\n\nairbnb.head()\n\n\n\n\n\n\n\n\nUnnamed: 0\nid\ndays\nlast_scraped\nhost_since\nroom_type\nbathrooms\nbedrooms\nprice\nnumber_of_reviews\nreview_scores_cleanliness\nreview_scores_location\nreview_scores_value\ninstant_bookable\n\n\n\n\n0\n1\n2515\n3130\n4/2/2017\n9/6/2008\nPrivate room\n1.0\n1.0\n59\n150\n9.0\n9.0\n9.0\nf\n\n\n1\n2\n2595\n3127\n4/2/2017\n9/9/2008\nEntire home/apt\n1.0\n0.0\n230\n20\n9.0\n10.0\n9.0\nf\n\n\n2\n3\n3647\n3050\n4/2/2017\n11/25/2008\nPrivate room\n1.0\n1.0\n150\n0\nNaN\nNaN\nNaN\nf\n\n\n3\n4\n3831\n3038\n4/2/2017\n12/7/2008\nEntire home/apt\n1.0\n1.0\n89\n116\n9.0\n9.0\n9.0\nf\n\n\n4\n5\n4611\n3012\n4/2/2017\n1/2/2009\nPrivate room\nNaN\n1.0\n39\n93\n9.0\n8.0\n9.0\nt\n\n\n\n\n\n\n\n\n# count number of listings which have 0 reviews\nairbnb[airbnb['number_of_reviews'] == 0]['number_of_reviews'].count()\n\n9481\n\n\n\n# create subset of data which only shows bedrooms which are nan\n\nairbnb.isnull().sum()\n\nUnnamed: 0                       0\nid                               0\ndays                             0\nlast_scraped                     0\nhost_since                      35\nroom_type                        0\nbathrooms                      160\nbedrooms                        76\nprice                            0\nnumber_of_reviews                0\nreview_scores_cleanliness    10195\nreview_scores_location       10254\nreview_scores_value          10256\ninstant_bookable                 0\ndtype: int64\n\n\nLooking at the data initally, we see there are many cells having null values which we need to address. The majority of them are within the review score columns. To address this, we will drop the cells which have 0 days listed (brand new listings). For bedrooms and bathrooms in a shared or private room, we will assume bedrooms is 1 and bathrooms are 0. It is very common for rooms to not have a bathroom. For all other data which is null, we cannot say with reasonable certainty what the value would be. For example, if bedrooms or bathrooms are null for entire home or apartment, we cannot say with reasonable certainty the number of bedrooms or bathrooms.\n\n# fill bedrooms with 1 and bathrooms with 0 if nan and if room type is private or shared room\n\nairbnb.loc[(airbnb['bedrooms'].isnull()) & (airbnb['room_type'] == 'Private room'), 'bedrooms'] = 1\nairbnb.loc[(airbnb['bedrooms'].isnull()) & (airbnb['room_type'] == 'Shared room'), 'bedrooms'] = 1\nairbnb.loc[(airbnb['bathrooms'].isnull()) & (airbnb['room_type'] == 'Private room'), 'bathrooms'] = 0\nairbnb.loc[(airbnb['bathrooms'].isnull()) & (airbnb['room_type'] == 'Private room'), 'bathrooms'] = 0\n\n\n# Dropping all other rows which have null values\nairbnb = airbnb.dropna()\n\n\nairbnb.shape\n\n(30234, 14)\n\n\n\nairbnb.describe()\n\n\n\n\n\n\n\n\nUnnamed: 0\nid\ndays\nbathrooms\nbedrooms\nprice\nnumber_of_reviews\nreview_scores_cleanliness\nreview_scores_location\nreview_scores_value\n\n\n\n\ncount\n30234.000000\n3.023400e+04\n30234.000000\n30234.000000\n30234.000000\n30234.000000\n30234.000000\n30234.000000\n30234.000000\n30234.000000\n\n\nmean\n18633.661937\n8.955396e+06\n1114.714560\n1.118856\n1.150989\n140.062777\n21.245981\n9.200370\n9.414070\n9.332837\n\n\nstd\n11340.011572\n5.388450e+06\n646.007986\n0.389266\n0.698002\n188.205058\n32.137276\n1.114729\n0.843788\n0.900678\n\n\nmin\n1.000000\n2.515000e+03\n7.000000\n0.000000\n0.000000\n10.000000\n1.000000\n2.000000\n2.000000\n2.000000\n\n\n25%\n8554.500000\n4.244554e+06\n585.000000\n1.000000\n1.000000\n70.000000\n3.000000\n9.000000\n9.000000\n9.000000\n\n\n50%\n18187.000000\n9.128754e+06\n1044.000000\n1.000000\n1.000000\n103.000000\n8.000000\n10.000000\n10.000000\n10.000000\n\n\n75%\n28500.500000\n1.391076e+07\n1595.000000\n1.000000\n1.000000\n169.000000\n26.000000\n10.000000\n10.000000\n10.000000\n\n\nmax\n40504.000000\n1.797369e+07\n3317.000000\n6.000000\n10.000000\n10000.000000\n421.000000\n10.000000\n10.000000\n10.000000\n\n\n\n\n\n\n\n\nairbnb.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 30234 entries, 0 to 40503\nData columns (total 14 columns):\n #   Column                     Non-Null Count  Dtype  \n---  ------                     --------------  -----  \n 0   Unnamed: 0                 30234 non-null  int64  \n 1   id                         30234 non-null  int64  \n 2   days                       30234 non-null  int64  \n 3   last_scraped               30234 non-null  object \n 4   host_since                 30234 non-null  object \n 5   room_type                  30234 non-null  object \n 6   bathrooms                  30234 non-null  float64\n 7   bedrooms                   30234 non-null  float64\n 8   price                      30234 non-null  int64  \n 9   number_of_reviews          30234 non-null  int64  \n 10  review_scores_cleanliness  30234 non-null  float64\n 11  review_scores_location     30234 non-null  float64\n 12  review_scores_value        30234 non-null  float64\n 13  instant_bookable           30234 non-null  object \ndtypes: float64(5), int64(5), object(4)\nmemory usage: 3.5+ MB\n\n\nThe average number of reviews is 15.9 for this dataset, however the standard deviation is 29.25, meaning there is a large spread in the number of reviews, with many locations having 0 reviews (Right skewed)\nMost of the listings in the dataset rank very high in cleanliness, location, and value, with all having an average of above 9 out of 10.\nThe price of the listings is also left skewed, with the average price being $145, but the standard deviation being $211, meaning there is a large spread in the price of listings like the number of reviews. However with this spread, due to the fact that the price can’t be negative, the distribution is left skewed.\nWhile looking at the info for the columns, we will need to convert some of the variables for regression analysis. The columns need be an integer or float data type. We will be changing room type and instant bookable columns.\n\n# convert room_type to boolean\n\nairbnb = pd.get_dummies(airbnb, columns=['room_type'], drop_first=True)\n\n\nfor i in airbnb['instant_bookable']:\n    if i == 't':\n        airbnb['instant_bookable'] = 1\n    else:\n        airbnb['instant_bookable'] = 0\n\nPrior to building the regression model, we need to make sure the data is all in the same time interval since we are assuming it’s a poisson distribution.\n\n# Converting number of reviews to reviews per year\n\nairbnb['reviews_per_year'] = airbnb['number_of_reviews'] / airbnb['days'] * 365\n\nWe will also be scaling the data in order to prevent precision loss during our regression analysis.\n\nfrom sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\n# Assuming 'age' and 'age_squared' need scaling\nairbnb['days_scaled'] = scaler.fit_transform(airbnb[['days']])\nairbnb['price'] = scaler.fit_transform(airbnb[['price']])\nairbnb['review_scores_cleanliness'] = scaler.fit_transform(airbnb[['review_scores_cleanliness']])\nairbnb['review_scores_location'] = scaler.fit_transform(airbnb[['review_scores_location']])\nairbnb['review_scores_value'] = scaler.fit_transform(airbnb[['review_scores_value']])\n\nNext, we will use the previously generated MLE function to get the maximum likelihood estimators to predict our reviews per year.\n\nX = np.c_[np.ones(len(airbnb)), airbnb['days_scaled'], airbnb['bathrooms'], airbnb['bedrooms'], airbnb['price'], \n          airbnb['review_scores_cleanliness'], airbnb['review_scores_location'], airbnb['review_scores_value'], \n          airbnb['instant_bookable'], airbnb['room_type_Private room'], airbnb['room_type_Shared room']]\nY = airbnb['reviews_per_year'].values\n\n# Initial guess for beta\ninitial_beta = np.zeros(X.shape[1])\n\n# Minimization\nresult = minimize(poisson_regression_log_likelihood, initial_beta, args=(Y, X), method='BFGS')\n\nprint(\"Optimal beta:\", result.x)\n\nOptimal beta: [ 2.05080599 -0.43566627 -0.02481847  0.0721578  -0.02531794  0.13872818\n -0.05758502 -0.05235691  0.         -0.00724632 -0.07017587]\n\n\nNext, we will confirm our betas with a built-in regression function.\n\n# Fit a Poisson regression model using statsmodels\n\npoisson_model = sm.GLM(Y, X, family=sm.families.Poisson()).fit()\n\n# Print the summary of the model\n\nprint(poisson_model.summary())\n\n                 Generalized Linear Model Regression Results                  \n==============================================================================\nDep. Variable:                      y   No. Observations:                30234\nModel:                            GLM   Df Residuals:                    30224\nModel Family:                 Poisson   Df Model:                            9\nLink Function:                    Log   Scale:                          1.0000\nMethod:                          IRLS   Log-Likelihood:            -2.2032e+05\nDate:                Wed, 01 May 2024   Deviance:                   3.4336e+05\nTime:                        18:08:09   Pearson chi2:                 4.58e+05\nNo. Iterations:                     5   Pseudo R-squ. (CS):             0.8026\nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          z      P&gt;|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst          2.0508      0.007    313.513      0.000       2.038       2.064\nx1            -0.4357      0.002   -201.380      0.000      -0.440      -0.431\nx2            -0.0248      0.005     -4.572      0.000      -0.035      -0.014\nx3             0.0722      0.003     23.812      0.000       0.066       0.078\nx4            -0.0253      0.003     -7.690      0.000      -0.032      -0.019\nx5             0.1387      0.003     54.015      0.000       0.134       0.144\nx6            -0.0576      0.002    -27.251      0.000      -0.062      -0.053\nx7            -0.0524      0.002    -20.945      0.000      -0.057      -0.047\nx8           4.66e-18   3.65e-19     12.771      0.000    3.94e-18    5.37e-18\nx9            -0.0072      0.004     -1.663      0.096      -0.016       0.001\nx10           -0.0702      0.012     -6.036      0.000      -0.093      -0.047\n==============================================================================\n\n\nOur coefficients match, meaning per the data and our manipulation, we calculated the betas correctly.\nBelow, we are grouping the some of the data to potentially get a better explanation and generate a reasonable hypothesis for the outcome of the betas.\n\n# grouping the data based on days (binning the days) and reviews_per_year (binning reviews per year)\n\nairbnb['days_bins'] = pd.cut(airbnb['days'], bins=range(0, 400, 50))\n\nairbnb['reviews_per_year_bins'] = pd.cut(airbnb['reviews_per_year'], bins=range(0, 150, 25))\n\ndays_reviews = airbnb.groupby('days_bins')['reviews_per_year_bins'].value_counts().unstack()\n\ndays_reviews\n\n/tmp/ipykernel_14747/4142779578.py:7: FutureWarning:\n\nThe default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n\n\n\n\n\n\n\n\n\nreviews_per_year_bins\n(0, 25]\n(25, 50]\n(50, 75]\n(75, 100]\n(100, 125]\n\n\ndays_bins\n\n\n\n\n\n\n\n\n\n(0, 50]\n111\n62\n13\n10\n3\n\n\n(50, 100]\n187\n71\n18\n4\n2\n\n\n(100, 150]\n439\n117\n31\n5\n4\n\n\n(150, 200]\n438\n110\n51\n11\n4\n\n\n(200, 250]\n478\n100\n37\n10\n2\n\n\n(250, 300]\n616\n113\n32\n4\n2\n\n\n(300, 350]\n656\n133\n23\n4\n1\n\n\n\n\n\n\n\n\n# making each column in days_reviews proportional to the sum of the number in each row\n\ndays_reviews['sum'] = days_reviews.sum(axis=1)\ndays_reviews = days_reviews.div(days_reviews['sum'], axis=0)\ndays_reviews.drop(columns='sum', inplace=True)\n\ndays_reviews\n\n\n\n\n\n\n\nreviews_per_year_bins\n(0, 25]\n(25, 50]\n(50, 75]\n(75, 100]\n(100, 125]\n\n\ndays_bins\n\n\n\n\n\n\n\n\n\n(0, 50]\n0.557789\n0.311558\n0.065327\n0.050251\n0.015075\n\n\n(50, 100]\n0.663121\n0.251773\n0.063830\n0.014184\n0.007092\n\n\n(100, 150]\n0.736577\n0.196309\n0.052013\n0.008389\n0.006711\n\n\n(150, 200]\n0.713355\n0.179153\n0.083062\n0.017915\n0.006515\n\n\n(200, 250]\n0.762360\n0.159490\n0.059011\n0.015949\n0.003190\n\n\n(250, 300]\n0.803129\n0.147327\n0.041721\n0.005215\n0.002608\n\n\n(300, 350]\n0.802938\n0.162791\n0.028152\n0.004896\n0.001224\n\n\n\n\n\n\n\nLooking at the betas calculated from our Poisson Regression Likelihood model, the first explanatory variable analyzed was days listed. Days listed seems to have a negative affect on the number of reviews, with a beta of -0.91. This is significant considering the other betas. The explanation for this may be that people like to see brand new listings since as time goes on, the listings have more wear and tear, thus review scores start to decrease. However, the other explanation is there is an omitted variable we are missing that was not captured in this dataset.\nIn the days_reviews table above, it shows the proportion of reviews_per_year in comparison to each days bin. It does seem like as the number of days increases, the number of reviews per year decreases with the exception of the first bin (0-25 reviews per year).\nBathrooms also has a slightly negative affect on the number of reviews, with a beta of -0.011. This is not as significant as some of the other variables, and may be leading to omitted variable bias. However, an explanation may be that rooms are more popular than homes for the individuals in this area. If they are staying short term, they may have a bathroom outside the room.\nBedrooms do have a positive effect on the number of reviews, with a beta of 0.10. Meaning an entire house or apartment is more likely to get more reviews than a shared room. This is significant, and may be due to the fact that people are more likely to stay in a place with more bedrooms if they are traveling with a group. They also may be more likely to leave a review if traveling with a group.\nPrice has a negative effect on reviews per year, with a beta of -0.088. This may be explained by the income class of those staying and the increased prices in NYC. Most people try and spend the least amount of money as possible in order to have a satisfactory experience.\nReview scores all have a positive effect on reviews_per_year with value having the largest affect. As stated above, most people want to spend the least amount of money for a satisfactory experience, so if a place is respectively inexpensive and has a high value rating by others, than that will attract others to stay.\nInstant bookable has a significant positive effect on reviews_per_year. This aligns with the American society values. People want to be able to have control at the touch of a button. They lose interest quickly if the have to wait and want immediate feedback. Instant bookings provide that instant feedback.\nPrivate and shared rooms have a slightly negative effect, which means that entire homes or apartments are more desirable. Although price is a concern and entire homes or apartments are more expensive, they provide more privacy and space for the guests. If the guests are traveling in groups, that makes this option more affordable, which may explain why it’s positive."
  },
  {
    "objectID": "projects/HW2/HW2.html",
    "href": "projects/HW2/HW2.html",
    "title": "Variable Definitions",
    "section": "",
    "text": "todo: Read in data.\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Load the data\n\ndata = pd.read_csv('blueprinty.csv')\n\ndata.head()\n\n\n\n\n\n\n\n\nUnnamed: 0\npatents\nregion\nage\niscustomer\n\n\n\n\n0\n1\n0\nMidwest\n32.5\n0\n\n\n1\n786\n3\nSouthwest\n37.5\n0\n\n\n2\n348\n4\nNorthwest\n27.0\n1\n\n\n3\n927\n3\nNortheast\n24.5\n0\n\n\n4\n830\n3\nSouthwest\n37.0\n0\n\n\n\n\n\n\n\n\n# coutn the number of unique values in the column 'Unnamed: 0'\n\ndata['Unnamed: 0'].nunique()\n\n1500\n\n\n\ndata.shape\n\n(1500, 5)\n\n\ntodo: Compare histograms and means of number of patents by customer status. What do you observe?\n\n# count number of customers and non-customers\n\ndata[data['iscustomer']==1]['iscustomer'].count()\n\n197\n\n\n\ndata[data['iscustomer']==0]['iscustomer'].count()\n\n1303\n\n\n\n# histogram of # of patents for Customers\n\n\ndata[data['iscustomer'] == 1]['patents'].hist()\n\nplt.xlabel('Number of Patents')\nplt.ylabel('Frequency')\nplt.title('Frequency of Patents for Customers')\n\nText(0.5, 1.0, 'Frequency of Patents for Customers')\n\n\n\n\n\n\n\n\n\n\n# make histogram of # of patents for Non-Customers\n\ndata[data['iscustomer'] == 0]['patents'].hist(color='orange')\nplt.xlabel('Number of Patents')\nplt.ylabel('Frequency')\nplt.title('Frequency of Patents for Non-Customers')\n\n\nText(0.5, 1.0, 'Frequency of Patents for Non-Customers')\n\n\n\n\n\n\n\n\n\nThe number of patents for customers is slightly skewed right, however it has a more normal distribution than the number of patents for non-customers.\nBoth plots have a large drop off around 6 patents (For customers it is slightly less than 6). The number of non-customers is significantly higher than the number of customers, totaling to 1303 non-customers and 197 customers. The mean number of patents for customers was 4.09 and the mean number of patents for non-customers was 3.62. This is a difference of about 0.5 patents.\n\nmean_patents_customers = data[data['iscustomer'] == 1]['patents'].mean()\nmean_patents_noncustomers = data[data['iscustomer'] == 0]['patents'].mean()\n\nprint('Mean patents for customers:', round(mean_patents_customers,2))\nprint('Mean patents for non-customers:', round(mean_patents_noncustomers,2))\n\nMean patents for customers: 4.09\nMean patents for non-customers: 3.62\n\n\ntodo: Compare regions and ages by customer status. What do you observe?\n\n# group by region and count number of customers and non-customers\n\nregion = data.groupby('region')['iscustomer'].value_counts().unstack()\n\n# Calculate respective proportion of non-customers and customers which make up each region\n\nregion['Prop_Non_cust'] = region[0]/(region[0].sum())\nregion['Prop_Cust'] = region[1]/(region[1].sum())\n\n# Rename columns from 0 to Non-Customers and 1 to Customers\n\nregion.rename(columns={0:'Non-Customers', 1:'Customers'}, inplace=True)\n\nregion\n\n\n\n\n\n\n\niscustomer\nNon-Customers\nCustomers\nProp_Non_cust\nProp_Cust\n\n\nregion\n\n\n\n\n\n\n\n\nMidwest\n207\n17\n0.158864\n0.086294\n\n\nNortheast\n488\n113\n0.374520\n0.573604\n\n\nNorthwest\n171\n16\n0.131236\n0.081218\n\n\nSouth\n171\n20\n0.131236\n0.101523\n\n\nSouthwest\n266\n31\n0.204144\n0.157360\n\n\n\n\n\n\n\n\n# rename column 0 to non-customers, 1 to customers\n\nregion.rename(columns={0:'Non-Customers', 1:'Customers'}, inplace=True)\n\n\n# create a plot of the number of customers and non-customers by region side by side\n\ndata.groupby('region')['iscustomer'].value_counts().unstack().plot(kind='bar', stacked=False)\n\nplt.xlabel('Region')\nplt.ylabel('Count')\nplt.title('Number of Customers and Non-Customers by Region')\nplt.legend(['Non-Customer', 'Customer'])\n\n\n\n\n\n\n\n\nThe Northeast has by far the most customers and non-customers, and the number of non-customers in each region clearly outweighs the number of customers. Although ranking each respective customer and non-customer base by region comes out to be nearly the same ranking, the proportions are different.\nFor the customers, nearly 60% are from the NE, while only 40% of non-customers are from the NE.\n\n# group by age with bins every 5 years and count number of customers and non-customers\n\ndata['age_bins'] = pd.cut(data['age'], bins=range(0, 60, 5))\n\nage = data.groupby('age_bins')['iscustomer'].value_counts().unstack()\nage['Prop_Non_cust'] = age[0]/(age[0].sum())\nage['Prop_Cust'] = age[1]/(age[1].sum())\n\nage.rename(columns={0:'Non-Customers', 1:'Customers'}, inplace=True)\n\nage\n\n/tmp/ipykernel_27929/4171049143.py:5: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n  age = data.groupby('age_bins')['iscustomer'].value_counts().unstack()\n\n\n\n\n\n\n\n\niscustomer\nNon-Customers\nCustomers\nProp_Non_cust\nProp_Cust\n\n\nage_bins\n\n\n\n\n\n\n\n\n(0, 5]\n0\n0\n0.000000\n0.000000\n\n\n(5, 10]\n3\n3\n0.002302\n0.015228\n\n\n(10, 15]\n52\n15\n0.039908\n0.076142\n\n\n(15, 20]\n213\n53\n0.163469\n0.269036\n\n\n(20, 25]\n318\n51\n0.244052\n0.258883\n\n\n(25, 30]\n301\n32\n0.231005\n0.162437\n\n\n(30, 35]\n244\n24\n0.187260\n0.121827\n\n\n(35, 40]\n135\n13\n0.103607\n0.065990\n\n\n(40, 45]\n34\n5\n0.026094\n0.025381\n\n\n(45, 50]\n3\n1\n0.002302\n0.005076\n\n\n(50, 55]\n0\n0\n0.000000\n0.000000\n\n\n\n\n\n\n\n\n# create a plot of the number of customers and non-customers by age side by side\n\ndata.groupby('age_bins')['iscustomer'].value_counts().unstack().plot(kind='bar', stacked=False)\nplt.xlabel('Age of Company')\nplt.ylabel('Count')\nplt.title('Number of Customers and Non-Customers by Age of Company')\nplt.legend(['Non-Customer', 'Customer'])\n\n/tmp/ipykernel_27929/196641801.py:3: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n  data.groupby('age_bins')['iscustomer'].value_counts().unstack().plot(kind='bar', stacked=False)\n\n\n\n\n\n\n\n\n\n\n# find mean age for customers and non-customers\n\nmean_age_customers = data[data['iscustomer'] == 1]['age'].mean()\nmean_age_noncustomers = data[data['iscustomer'] == 0]['age'].mean()\n\n\nprint('Mean age for customers:', round(mean_age_customers,2))\nprint('Mean age for non-customers:', round(mean_age_noncustomers,2))\n\nMean age for customers: 24.15\nMean age for non-customers: 26.69\n\n\n\n# Calculate a 95% confidence interval for the mean age of customers and non-customers\n\nimport numpy as np\n\nstd_age_customers = data[data['iscustomer'] == 1]['age'].std()\nstd_age_noncustomers = data[data['iscustomer'] == 0]['age'].std()\n\nn_customers = data[data['iscustomer'] == 1]['age'].count()\nn_noncustomers = data[data['iscustomer'] == 0]['age'].count()\n\nz = 1.96\n\nci_customers = z * (std_age_customers/np.sqrt(n_customers))\nci_noncustomers = z * (std_age_noncustomers/np.sqrt(n_noncustomers))\n\nprint('95% CI for mean age of customers:', round(mean_age_customers-ci_customers,2), round(mean_age_customers+ci_customers,2))\nprint('95% CI for mean age of non-customers:', round(mean_age_noncustomers-ci_noncustomers,2), round(mean_age_noncustomers+ci_noncustomers,2))\n\n95% CI for mean age of customers: 23.09 25.21\n95% CI for mean age of non-customers: 26.3 27.08\n\n\nThe most customers come from companies which are between 15-20 years old, with the average company age being about 24 years. The most non-customers come from companies which are between 20-25 years old, with the average company age being about 27 years.\nThe distribution of company ages for customers and non-customers resembles a normal distribution, with a slight skew to the right. The largest disparity between the two distributions is at the 10-15 year mark, where the % of non-customers is 10% lower than that of customers (16%, 26%).\nThe confidence interval for the mean age of customers is (23.1, 25.2) and the confidence interval for the mean age of non-customers is (26.3, 27.1).\nWrite down mathematically the likelihood for_ \\(Y \\sim \\text{Poisson}(\\lambda)\\). Note that \\(f(Y|\\lambda) = e^{-\\lambda}\\lambda^Y/Y!\\).\nBelow is the log-likelihood function for the Poisson distribution. The likelihood function of lambda given Y is the exact same as the function of Y given lambda.\nℓ(λ∣Y)=−λ+Ylog(λ)−log(Y!)\nCode the likelihood (or log-likelihood) function for the Poisson model. This is a function of lambda and Y. For example:_\n\ndef poisson_log_likelihood(lam, y):\n    \"\"\"\n    Parameters:\n    - lam (float): The rate parameter (lambda) of the Poisson distribution.\n    - y (array-like): Array of observed counts.\n\n    Returns:\n    - float: The log likelihood of observing the data given lam.\n    \"\"\"\n    y = np.array(y)\n    n = len(y)  # number of observations\n    sum_y = np.sum(y)  # sum of all observed counts\n\n    # Calculate each part of the log likelihood\n    # log(P(Y|lam)) = -n * lam + sum_y * log(lam) - log(y_i!)\n    # We use np.sum(np.log(y_factorials)) to sum log of factorials\n    log_likelihood = -n * lam + sum_y * np.log(lam) - np.sum([np.log(np.math.factorial(i)) for i in y])\n    return log_likelihood\n\n\nyears = range(1,11)\nlog_likelihood_values = []\n\nfor i in years:\n    log_likelihood_value = poisson_log_likelihood(i, data['patents'])\n    log_likelihood_values.append(log_likelihood_value)\n\nlog_likelihood_values\n\n/tmp/ipykernel_27929/1539929848.py:17: DeprecationWarning: `np.math` is a deprecated alias for the standard library `math` module (Deprecated Numpy 1.25). Replace usages of `np.math` with `math`\n  log_likelihood = -n * lam + sum_y * np.log(lam) - np.sum([np.log(np.math.factorial(i)) for i in y])\n\n\n[-6548.886990069443,\n -4217.862523114625,\n -3476.856870600801,\n -3386.838056159808,\n -3653.52364804617,\n -4145.8324036459835,\n -4793.841596240727,\n -5555.81358920499,\n -6404.826751132159,\n -7322.4991810913525]\n\n\n_todo: Use your function to plot lambda on the horizontal axis and the likelihood (or log-likelihood) on the vertical axis for a range of lambdas (use the observed number of patents as the input for Y).\n\n# _todo: Use your function to plot lambda on the horizontal axis and the likelihood (or log-likelihood) on the vertical axis for a range of lambdas (use the observed number of patents as the input for Y).\n\nplt.plot(years, log_likelihood_values)\nplt.xlabel('Years (lambda)')\nplt.ylabel('Likelihood')\nplt.title('Likelihood of observing the data given Years')\nplt.show()\n\n\n\n\n\n\n\n\n\nfrom scipy.optimize import minimize\n\ndef neg_poisson_log_likelihood(lam, y):\n    \"\"\"\n    Compute the log likelihood of observing data y under a Poisson distribution with parameter lam.\n\n    Parameters:\n    - lam (float): The rate parameter (lambda) of the Poisson distribution.\n    - y (array-like): Array of observed counts.\n\n    Returns:\n    - float: The log likelihood of observing the data given lam.\n    \"\"\"\n    y = np.array(y)\n    n = len(y)  # number of observations\n    sum_y = np.sum(y)  # sum of all observed counts\n\n    # Calculate each part of the log likelihood\n    # log(P(Y|lam)) = -n * lam + sum_y * log(lam) - log(y_i!)\n    # We use np.sum(np.log(y_factorials)) to sum log of factorials\n    return -(-n * lam + sum_y * np.log(lam) - np.sum([np.log(np.math.factorial(i)) for i in y]))\n    \n\n\nmean = np.mean(data['patents'])\n\nresult = minimize(neg_poisson_log_likelihood, mean, args=(data['patents']), bounds = [(0, None)])\n\nresult\n\n/tmp/ipykernel_27929/354120658.py:21: DeprecationWarning: `np.math` is a deprecated alias for the standard library `math` module (Deprecated Numpy 1.25). Replace usages of `np.math` with `math`\n  return -(-n * lam + sum_y * np.log(lam) - np.sum([np.log(np.math.factorial(i)) for i in y]))\n\n\n  message: CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_&lt;=_PGTOL\n  success: True\n   status: 0\n      fun: 3367.683772235094\n        x: [ 3.685e+00]\n      nit: 0\n      jac: [ 0.000e+00]\n     nfev: 2\n     njev: 1\n hess_inv: &lt;1x1 LbfgsInvHessProduct with dtype=float64&gt;\n\n\nNext, we extend our simple Poisson model to a Poisson Regression Model such that \\(Y_i = \\text{Poisson}(\\lambda_i)\\) where \\(\\lambda_i = \\exp(X_i'\\beta)\\). The interpretation is that the success rate of patent awards is not constant across all firms (\\(\\lambda\\)) but rather is a function of firm characteristics \\(X_i\\). Specifically, we will use the covariates age, age squared, region, and whether the firm is a customer of Blueprinty.\ntodo: Update your likelihood or log-likelihood function with an additional argument to take in a covariate matrix X. Also change the parameter of the model from lambda to the beta vector. In this model, lambda must be a positive number, so we choose the inverse link function g() to be exp() so that \\(\\lambda_i = e^{X_i'\\beta}\\). For example:\npoisson_regression_likelihood &lt;- function(beta, Y, X){\n   ...\n}\n\n::: {#cell-33 .cell execution_count=73}\n``` {.python .cell-code}\nimport numpy as np\nfrom scipy.optimize import minimize\nfrom scipy.special import factorial\n\ndef poisson_regression_log_likelihood(beta, Y, X):\n \n    # Calculate lambda for each observation\n    lambda_i = np.exp(np.dot(X, beta))\n    \n    # Calculate the log likelihood\n    log_likelihood = np.sum(Y * np.log(lambda_i) - lambda_i - np.log(factorial(Y)))\n    \n    # Return the negative log likelihood\n    return -log_likelihood\n:::\ntodo: Use your function along with R’s optim() or Python’s sp.optimize() to find the MLE vector and the Hessian of the Poisson model with covariates. Specifically, the first column of X should be all 1’s to enable a constant term in the model, and the subsequent columns should be age, age squared, binary variables for all but one of the regions, and the binary customer variable. Use the Hessian to find standard errors of the beta parameter estimates and present a table of coefficients and standard errors.\n\n# convert regions column to binary columns\n\ndata = pd.get_dummies(data, columns=['region'], drop_first=True)\n\n\n# creating function to convert boolean column to binary\n\ndef convert_boolean_to_binary(data, column):\n    data[column] = data[column].astype(int)\n    return data\n\n# coverting region's boolean values to binary\n\ndata = convert_boolean_to_binary(data, 'region_Northeast')\ndata = convert_boolean_to_binary(data, 'region_South')\ndata = convert_boolean_to_binary(data, 'region_Southwest')\ndata = convert_boolean_to_binary(data, 'region_Northwest')\n\n\ndata['age_squared'] = data['age']**2\n\n\n\nn = len(data)  # Number of observations\nX = np.c_[np.ones(n).tolist(), data['age'], data['age_squared'], data['iscustomer'], data['region_Southwest'], data['region_Northwest'],\n          data['region_Northeast'], data['region_South']]\nY = data['patents']\n\n# Initial guess for beta\ninitial_beta = np.zeros(X.shape[1])\n\n# Minimization\nresult = minimize(poisson_regression_log_likelihood, initial_beta, args = (Y, X), method= 'BFGS')\n\nprint(\"Optimal beta:\", result.x)\nprint(\"Success:\", result.success)\nprint(\"Message:\", result.message)\n\nOptimal beta: [0. 0. 0. 0. 0. 0. 0. 0.]\nSuccess: False\nMessage: Desired error not necessarily achieved due to precision loss.\n\n\n/tmp/ipykernel_27929/1165065463.py:8: RuntimeWarning: overflow encountered in exp\n  lambda_i = np.exp(np.dot(X, beta))\n/opt/conda/lib/python3.11/site-packages/numpy/core/_methods.py:49: RuntimeWarning: overflow encountered in reduce\n  return umr_sum(a, axis, dtype, out, keepdims, initial, where)\n/opt/conda/lib/python3.11/site-packages/scipy/optimize/_numdiff.py:576: RuntimeWarning: invalid value encountered in subtract\n  df = fun(x) - f0\n/tmp/ipykernel_27929/1165065463.py:8: RuntimeWarning: overflow encountered in exp\n  lambda_i = np.exp(np.dot(X, beta))\n/opt/conda/lib/python3.11/site-packages/numpy/core/_methods.py:49: RuntimeWarning: overflow encountered in reduce\n  return umr_sum(a, axis, dtype, out, keepdims, initial, where)\n\n\n\nimport numpy as np\nfrom scipy.optimize import minimize\n\n\ndef poisson_regression_log_likelihood(beta, Y, X):\n    eta = np.dot(X, beta)\n    lambda_i = np.exp(eta)\n    log_likelihood = np.sum(Y * eta - lambda_i)\n    return -log_likelihood\n\n# Load and preprocess your data as before, ensuring that features are scaled\nfrom sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\n# Assuming 'age' and 'age_squared' need scaling\ndata['age'] = scaler.fit_transform(data[['age']])\ndata['age_squared'] = scaler.fit_transform(data[['age_squared']])\n\n# Assuming that you already converted regions into dummy variables and set up the data correctly\nX = np.c_[np.ones(len(data)), data['age'], data['age_squared'], data['iscustomer'], data['region_Southwest'], data['region_Northwest'],\n          data['region_Northeast'], data['region_South']]\nY = data['patents'].values\n\n# Initial guess for beta\ninitial_beta = np.zeros(X.shape[1])\n\n# Minimization using a more controlled approach\nresult = minimize(poisson_regression_log_likelihood, initial_beta, args=(Y, X), method='BFGS')\n\nprint(\"Optimal beta:\", result.x)\nprint(\"Success:\", result.success)\nprint(\"Message:\", result.message)\n\nOptimal beta: [ 1.21543809  1.04645982 -1.14084528  0.11811438  0.05134699 -0.02009421\n  0.098596    0.05717198]\nSuccess: False\nMessage: Desired error not necessarily achieved due to precision loss.\n\n\n\n# _todo: Check your results using R's glm() function or Python sm.GLM() function._\n\nimport statsmodels.api as sm\n\n# Fit a Poisson regression model using statsmodels\n\npoisson_model = sm.GLM(Y, X, family=sm.families.Poisson()).fit()\n\n# Print the summary of the model\n\nprint(poisson_model.summary())\n\n                 Generalized Linear Model Regression Results                  \n==============================================================================\nDep. Variable:                      y   No. Observations:                 1500\nModel:                            GLM   Df Residuals:                     1492\nModel Family:                 Poisson   Df Model:                            7\nLink Function:                    Log   Scale:                          1.0000\nMethod:                          IRLS   Log-Likelihood:                -3275.9\nDate:                Wed, 01 May 2024   Deviance:                       2178.8\nTime:                        17:06:55   Pearson chi2:                 2.11e+03\nNo. Iterations:                     5   Pseudo R-squ. (CS):             0.1152\nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          z      P&gt;|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst          1.2154      0.036     33.368      0.000       1.144       1.287\nx1             1.0465      0.100     10.414      0.000       0.850       1.243\nx2            -1.1408      0.102    -11.131      0.000      -1.342      -0.940\nx3             0.1181      0.039      3.035      0.002       0.042       0.194\nx4             0.0513      0.047      1.088      0.277      -0.041       0.144\nx5            -0.0201      0.054     -0.374      0.709      -0.126       0.085\nx6             0.0986      0.042      2.347      0.019       0.016       0.181\nx7             0.0572      0.053      1.085      0.278      -0.046       0.160\n==============================================================================\n\n\n\n# Calculate Hessian at the optimal beta\nfrom scipy.linalg import inv\nhessian_inv = result.hess_inv  # Inverse Hessian is returned by BFGS\n\n# Standard errors are the square roots of the diagonal elements of the inverse Hessian\nstd_errors = np.sqrt(np.diag(hessian_inv))\n\nprint(\"Standard Errors:\", std_errors)\n\nStandard Errors: [0.02272912 0.14333007 0.14451598 0.03888018 0.03676005 0.04672971\n 0.02526631 0.02849847]\n\n\nWe can conclude, based on our optimal beta’s through our regression model, that Blueprinty’s software has a positive effect on the number of patents awarded to a company. The coefficient or beta calculated for “iscustomer” is 0.11, meaning if they are a customer of the software, the humber of patents earned increases by 0.11.\nWe also see that p-value of “iscustomer” is 0.002, meaning there is only a 0.2% chance that the coefficient has zero affect on the number of patents given the dataset.\nAirBnB is a popular platform for booking short-term rentals. In March 2017, students Annika Awad, Evan Lebo, and Anna Linden scraped of 40,000 Airbnb listings from New York City. The data include the following variables:\n\n\n\n\n\n\nNote\n\n\n\n\n\n- `id` = unique ID number for each unit\n- `last_scraped` = date when information scraped\n- `host_since` = date when host first listed the unit on Airbnb\n- `days` = `last_scraped` - `host_since` = number of days the unit has been listed\n- `room_type` = Entire home/apt., Private room, or Shared room\n- `bathrooms` = number of bathrooms\n- `bedrooms` = number of bedrooms\n- `price` = price per night (dollars)\n- `number_of_reviews` = number of reviews for the unit on Airbnb\n- `review_scores_cleanliness` = a cleanliness score from reviews (1-10)\n- `review_scores_location` = a \"quality of location\" score from reviews (1-10)\n- `review_scores_value` = a \"quality of value\" score from reviews (1-10)\n- `instant_bookable` = \"t\" if instantly bookable, \"f\" if not\n\n\n\ntodo: Assume the number of reviews is a good proxy for the number of bookings. Perform some exploratory data analysis to get a feel for the data, handle or drop observations with missing values on relevant variables, build one or more models (e.g., a poisson regression model for the number of bookings as proxied by the number of reviews), and interpret model coefficients to describe variation in the number of reviews as a function of the variables provided.\n\n# read airbnb csv\n\nairbnb = pd.read_csv('airbnb.csv')\n\nairbnb.head()\n\n\n\n\n\n\n\n\nUnnamed: 0\nid\ndays\nlast_scraped\nhost_since\nroom_type\nbathrooms\nbedrooms\nprice\nnumber_of_reviews\nreview_scores_cleanliness\nreview_scores_location\nreview_scores_value\ninstant_bookable\n\n\n\n\n0\n1\n2515\n3130\n4/2/2017\n9/6/2008\nPrivate room\n1.0\n1.0\n59\n150\n9.0\n9.0\n9.0\nf\n\n\n1\n2\n2595\n3127\n4/2/2017\n9/9/2008\nEntire home/apt\n1.0\n0.0\n230\n20\n9.0\n10.0\n9.0\nf\n\n\n2\n3\n3647\n3050\n4/2/2017\n11/25/2008\nPrivate room\n1.0\n1.0\n150\n0\nNaN\nNaN\nNaN\nf\n\n\n3\n4\n3831\n3038\n4/2/2017\n12/7/2008\nEntire home/apt\n1.0\n1.0\n89\n116\n9.0\n9.0\n9.0\nf\n\n\n4\n5\n4611\n3012\n4/2/2017\n1/2/2009\nPrivate room\nNaN\n1.0\n39\n93\n9.0\n8.0\n9.0\nt\n\n\n\n\n\n\n\n\nairbnb.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 40628 entries, 0 to 40627\nData columns (total 14 columns):\n #   Column                     Non-Null Count  Dtype  \n---  ------                     --------------  -----  \n 0   Unnamed: 0                 40628 non-null  int64  \n 1   id                         40628 non-null  int64  \n 2   days                       40628 non-null  int64  \n 3   last_scraped               40628 non-null  object \n 4   host_since                 40593 non-null  object \n 5   room_type                  40628 non-null  object \n 6   bathrooms                  40468 non-null  float64\n 7   bedrooms                   40552 non-null  float64\n 8   price                      40628 non-null  int64  \n 9   number_of_reviews          40628 non-null  int64  \n 10  review_scores_cleanliness  30433 non-null  float64\n 11  review_scores_location     30374 non-null  float64\n 12  review_scores_value        30372 non-null  float64\n 13  instant_bookable           40628 non-null  object \ndtypes: float64(5), int64(5), object(4)\nmemory usage: 4.3+ MB\n\n\n\nairbnb.shape\n\n(40628, 14)\n\n\n\nairbnb.describe()\n\n\n\n\n\n\n\n\nUnnamed: 0\nid\ndays\nbathrooms\nbedrooms\nprice\nnumber_of_reviews\nreview_scores_cleanliness\nreview_scores_location\nreview_scores_value\n\n\n\n\ncount\n40628.000000\n4.062800e+04\n40628.000000\n40468.000000\n40552.000000\n40628.000000\n40628.000000\n30433.000000\n30374.000000\n30372.000000\n\n\nmean\n20314.500000\n9.698889e+06\n1102.368219\n1.124592\n1.147046\n144.760732\n15.904426\n9.198370\n9.413544\n9.331522\n\n\nstd\n11728.437705\n5.460166e+06\n1383.269358\n0.385884\n0.691746\n210.657597\n29.246009\n1.119935\n0.844949\n0.902966\n\n\nmin\n1.000000\n2.515000e+03\n1.000000\n0.000000\n0.000000\n10.000000\n0.000000\n2.000000\n2.000000\n2.000000\n\n\n25%\n10157.750000\n4.889868e+06\n542.000000\n1.000000\n1.000000\n70.000000\n1.000000\n9.000000\n9.000000\n9.000000\n\n\n50%\n20314.500000\n9.862878e+06\n996.000000\n1.000000\n1.000000\n100.000000\n4.000000\n10.000000\n10.000000\n10.000000\n\n\n75%\n30471.250000\n1.466789e+07\n1535.000000\n1.000000\n1.000000\n170.000000\n17.000000\n10.000000\n10.000000\n10.000000\n\n\nmax\n40628.000000\n1.800967e+07\n42828.000000\n8.000000\n10.000000\n10000.000000\n421.000000\n10.000000\n10.000000\n10.000000\n\n\n\n\n\n\n\n\n# create subset of data which only shows bedrooms which are nan\n\nairbnb[airbnb['room_type']=='Entire home/apt'].isnull().sum()\n\n# drop Entire home/apt that are null for bedrooms or bathrooms\n\nairbnb = airbnb.dropna(subset=['bedrooms', 'bathrooms'])\n\nUnnamed: 0                      0\nid                              0\ndays                            0\nlast_scraped                    0\nhost_since                     19\nroom_type                       0\nbathrooms                      41\nbedrooms                       55\nprice                           0\nnumber_of_reviews               0\nreview_scores_cleanliness    4208\nreview_scores_location       4228\nreview_scores_value          4230\ninstant_bookable                0\ndtype: int64\n\n\n\n# fill bedrooms with 1 and bathrooms with 0 if nan and if room type is private or shared room\n\nairbnb.loc[(airbnb['bedrooms'].isnull()) & (airbnb['room_type'] == 'Private room'), 'bedrooms'] = 1\nairbnb.loc[(airbnb['bedrooms'].isnull()) & (airbnb['room_type'] == 'Shared room'), 'bedrooms'] = 1\nairbnb.loc[(airbnb['bathrooms'].isnull()) & (airbnb['room_type'] == 'Private room'), 'bathrooms'] = 0\nairbnb.loc[(airbnb['bathrooms'].isnull()) & (airbnb['room_type'] == 'Private room'), 'bathrooms'] = 0\n\n\n# count number of 0 value reviews\nairbnb[airbnb['number_of_reviews'] == 0]['number_of_reviews'].count()\n\n9481\n\n\n\n# convert bedrooms NaN to 0\n\nairbnb['bedrooms'] = airbnb['bedrooms'].fillna(0)\nairbnb['bathrooms'] = airbnb['bathrooms'].fillna(0)\n\n# convert all columns with review_score as value of NaN to 0\n\nairbnb['review_scores_value'] = airbnb['review_scores_value'].fillna(0)\nairbnb['review_scores_location'] = airbnb['review_scores_location'].fillna(0)\nairbnb['review_scores_cleanliness'] = airbnb['review_scores_cleanliness'].fillna(0)\n\n# convert room_type to boolean\n\nairbnb = pd.get_dummies(airbnb, columns=['room_type'], drop_first=True)\n\n\nfor i in airbnb['instant_bookable']:\n    if i == 't':\n        airbnb['instant_bookable'] = 1\n    else:\n        airbnb['instant_bookable'] = 0\n\n\n# convert room_type_Private_room to binary\n\nairbnb['room_type_Private room'] = airbnb['room_type_Private room'].astype(int)\nairbnb['room_type_Shared room'] = airbnb['room_type_Shared room'].astype(int)\n\nThe average number of reviews is 15.9 for this dataset, however the standard deviation is 29.25, meaning there is a large spread in the number of reviews, with many locations having 0 reviews (Right skewed)\nMost of the listings in the dataset rank very high in cleanliness, location, and value, with all having an average of above 9 out of 10.\nThe price of the listings is also left skewed, with the average price being $145, but the standard deviation being $211, meaning there is a large spread in the price of listings like the number of reviews. However with this spread, due to the fact that the price can’t be negative, the distribution is left skewed.\n\nairbnb['reviews_per_year'] = airbnb['number_of_reviews'] / airbnb['days'] * 365\n\n\n# Load and preprocess your data as before, ensuring that features are scaled\nfrom sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\n# Assuming 'age' and 'age_squared' need scaling\ndata['age'] = scaler.fit_transform(data[['age']])\ndata['age_squared'] = scaler.fit_transform(data[['age_squared']])\n\n\nairbnb.columns\n\n\ncolumns = ['days', 'bathrooms', 'bedrooms', 'price', \n           'review_scores_cleanliness', 'review_scores_location', \n           'review_scores_value', 'instant_bookable', \n           'room_type_Private room', 'room_type_Shared room']\n\n\n# from the days column, drop all rows with a value of 0\n\nairbnb = airbnb[airbnb['days'] != 0]\n\n\nimport numpy as np\nfrom scipy.optimize import minimize\n\n\n\nfrom sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\n# Assuming 'age' and 'age_squared' need scaling\nairbnb['days_scaled'] = scaler.fit_transform(airbnb[['days']])\nairbnb['price'] = scaler.fit_transform(airbnb[['price']])\nairbnb['review_scores_cleanliness'] = scaler.fit_transform(airbnb[['review_scores_cleanliness']])\nairbnb['review_scores_location'] = scaler.fit_transform(airbnb[['review_scores_location']])\nairbnb['review_scores_value'] = scaler.fit_transform(airbnb[['review_scores_value']])\n\n\n# Assuming that you already converted regions into dummy variables and set up the data correctly\nX = np.c_[np.ones(len(airbnb)), airbnb['days_scaled'], airbnb['bathrooms'], airbnb['bedrooms'], airbnb['price'], \n          airbnb['review_scores_cleanliness'], airbnb['review_scores_location'], airbnb['review_scores_value'], \n          airbnb['instant_bookable'], airbnb['room_type_Private room'], airbnb['room_type_Shared room']]\nY = airbnb['reviews_per_year'].values\n\n# Initial guess for beta\ninitial_beta = np.zeros(X.shape[1])\n\n# Minimization using a more controlled approach\nresult = minimize(poisson_regression_log_likelihood, initial_beta, args=(Y, X), method='BFGS')\n\nprint(\"Optimal beta:\", result.x)\nprint(\"Success:\", result.success)\nprint(\"Message:\", result.message)\n\nOptimal beta: [ 0.60331738 -0.91467491 -0.01100322  0.10389168 -0.08852837  0.76816195\n  0.39240359  0.17957291  0.60331738 -0.00616376 -0.04999207]\nSuccess: False\nMessage: Desired error not necessarily achieved due to precision loss.\n\n\n\n# group the data based on days (binning the days) and reviews_per_year (binning reviews per year)\n\nairbnb['days_bins'] = pd.cut(airbnb['days'], bins=range(0, 400, 50))\n\nairbnb['reviews_per_year_bins'] = pd.cut(airbnb['reviews_per_year'], bins=range(0, 150, 25))\n\ndays_reviews = airbnb.groupby('days_bins')['reviews_per_year_bins'].value_counts().unstack()\n\ndays_reviews\n\n/tmp/ipykernel_52532/3527047454.py:7: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n  days_reviews = airbnb.groupby('days_bins')['reviews_per_year_bins'].value_counts().unstack()\n\n\n\n\n\n\n\n\nreviews_per_year_bins\n(0, 25]\n(25, 50]\n(50, 75]\n(75, 100]\n(100, 125]\n\n\ndays_bins\n\n\n\n\n\n\n\n\n\n(0, 50]\n114\n62\n13\n10\n3\n\n\n(50, 100]\n200\n71\n18\n4\n2\n\n\n(100, 150]\n452\n117\n31\n5\n4\n\n\n(150, 200]\n456\n110\n51\n11\n4\n\n\n(200, 250]\n494\n100\n37\n10\n2\n\n\n(250, 300]\n647\n113\n33\n4\n2\n\n\n(300, 350]\n685\n133\n23\n4\n1\n\n\n\n\n\n\n\n\n# make each column in days_reviews proportional to the sum of the number in each row\n\ndays_reviews['sum'] = days_reviews.sum(axis=1)\ndays_reviews = days_reviews.div(days_reviews['sum'], axis=0)\ndays_reviews.drop(columns='sum', inplace=True)\n\ndays_reviews\n\n\n\n\n\n\n\nreviews_per_year_bins\n(0, 25]\n(25, 50]\n(50, 75]\n(75, 100]\n(100, 125]\n\n\ndays_bins\n\n\n\n\n\n\n\n\n\n(0, 50]\n0.564356\n0.306931\n0.064356\n0.049505\n0.014851\n\n\n(50, 100]\n0.677966\n0.240678\n0.061017\n0.013559\n0.006780\n\n\n(100, 150]\n0.742200\n0.192118\n0.050903\n0.008210\n0.006568\n\n\n(150, 200]\n0.721519\n0.174051\n0.080696\n0.017405\n0.006329\n\n\n(200, 250]\n0.768274\n0.155521\n0.057543\n0.015552\n0.003110\n\n\n(250, 300]\n0.809762\n0.141427\n0.041302\n0.005006\n0.002503\n\n\n(300, 350]\n0.809693\n0.157210\n0.027187\n0.004728\n0.001182\n\n\n\n\n\n\n\nLooking at the betas calculated from our Poisson Regression Likelihood model, the first explanatory variable analyzed was days listed. Days listed seems to have a negative affect on the number of reviews, with a beta of -0.91. This is significant considering the other betas. The explanation for this may be that people like to see brand new listings since as time goes on, the listings have more wear and tear, thus review scores start to decrease. However, the other explanation is there is an omitted variable we are missing that was not captured in this dataset.\nIn the days_reviews table above, it shows the proportion of reviews_per_year in comparison to each days bin. It does seem like as the number of days increases, the number of reviews per year decreases with the exception of the first bin (0-25 reviews per year).\nBathrooms also has a slightly negative affect on the number of reviews, with a beta of -0.011. This is not as significant as some of the other variables, and may be leading to omitted variable bias. However, an explanation may be that rooms are more popular than homes for the individuals in this area. If they are staying short term, they may have a bathroom outside the room.\nBedrooms do have a positive effect on the number of reviews, with a beta of 0.10. Meaning an entire house or apartment is more likely to get more reviews than a shared room. This is significant, and may be due to the fact that people are more likely to stay in a place with more bedrooms if they are traveling with a group. They also may be more likely to leave a review if traveling with a group.\nPrice has a negative effect on reviews per year, with a beta of -0.088. This may be explained by the income class of those staying and the increased prices in NYC. Most people try and spend the least amount of money as possible in order to have a satisfactory experience.\nReview scores all have a positive effect on reviews_per_year with value having the largest affect. As stated above, most people want to spend the least amount of money for a satisfactory experience, so if a place is respectively inexpensive and has a high value rating by others, than that will attract others to stay.\nInstant bookable has a significant positive effect on reviews_per_year. This aligns with the American society values. People want to be able to have control at the touch of a button. They lose interest quickly if the have to wait and want immediate feedback. Instant bookings provide that instant feedback.\nPrivate and shared rooms have a slightly negative effect, which means that entire homes or apartments are more desirable. Although price is a concern and entire homes or apartments are more expensive, they provide more privacy and space for the guests. If the guests are traveling in groups, that makes this option more affordable, which may explain why it’s positive."
  },
  {
    "objectID": "projects/HW3/hw3_questions.html",
    "href": "projects/HW3/hw3_questions.html",
    "title": "Poisson",
    "section": "",
    "text": "This assignment uses uses the MNL model to analyze (1) yogurt purchase data made by consumers at a retail location, and (2) conjoint data about consumer preferences for minivans."
  },
  {
    "objectID": "projects/HW3/hw3_questions.html#estimating-yogurt-preferences",
    "href": "projects/HW3/hw3_questions.html#estimating-yogurt-preferences",
    "title": "Poisson",
    "section": "1. Estimating Yogurt Preferences",
    "text": "1. Estimating Yogurt Preferences\n\nLikelihood for the Multi-nomial Logit (MNL) Model\nSuppose we have \\(i=1,\\ldots,n\\) consumers who each select exactly one product \\(j\\) from a set of \\(J\\) products. The outcome variable is the identity of the product chosen \\(y_i \\in \\{1, \\ldots, J\\}\\) or equivalently a vector of \\(J-1\\) zeros and \\(1\\) one, where the \\(1\\) indicates the selected product. For example, if the third product was chosen out of 4 products, then either \\(y=3\\) or \\(y=(0,0,1,0)\\) depending on how we want to represent it. Suppose also that we have a vector of data on each product \\(x_j\\) (eg, size, price, etc.).\nWe model the consumer’s decision as the selection of the product that provides the most utility, and we’ll specify the utility function as a linear function of the product characteristics:\n\\[ U_{ij} = x_j'\\beta + \\epsilon_{ij} \\]\nwhere \\(\\epsilon_{ij}\\) is an i.i.d. extreme value error term.\nThe choice of the i.i.d. extreme value error term leads to a closed-form expression for the probability that consumer \\(i\\) chooses product \\(j\\):\n\\[ \\mathbb{P}_i(j) = \\frac{e^{x_j'\\beta}}{\\sum_{k=1}^Je^{x_k'\\beta}} \\]\nFor example, if there are 4 products, the probability that consumer \\(i\\) chooses product 3 is:\n\\[ \\mathbb{P}_i(3) = \\frac{e^{x_3'\\beta}}{e^{x_1'\\beta} + e^{x_2'\\beta} + e^{x_3'\\beta} + e^{x_4'\\beta}} \\]\nA clever way to write the individual likelihood function for consumer \\(i\\) is the product of the \\(J\\) probabilities, each raised to the power of an indicator variable (\\(\\delta_{ij}\\)) that indicates the chosen product:\n\\[ L_i(\\beta) = \\prod_{j=1}^J \\mathbb{P}_i(j)^{\\delta_{ij}} = \\mathbb{P}_i(1)^{\\delta_{i1}} \\times \\ldots \\times \\mathbb{P}_i(J)^{\\delta_{iJ}}\\]\nNotice that if the consumer selected product \\(j=3\\), then \\(\\delta_{i3}=1\\) while \\(\\delta_{i1}=\\delta_{i2}=\\delta_{i4}=0\\) and the likelihood is:\n\\[ L_i(\\beta) = \\mathbb{P}_i(1)^0 \\times \\mathbb{P}_i(2)^0 \\times \\mathbb{P}_i(3)^1 \\times \\mathbb{P}_i(4)^0 = \\mathbb{P}_i(3) = \\frac{e^{x_3'\\beta}}{\\sum_{k=1}^Je^{x_k'\\beta}} \\]\nThe joint likelihood (across all consumers) is the product of the \\(n\\) individual likelihoods:\n\\[ L_n(\\beta) = \\prod_{i=1}^n L_i(\\beta) = \\prod_{i=1}^n \\prod_{j=1}^J \\mathbb{P}_i(j)^{\\delta_{ij}} \\]\nAnd the joint log-likelihood function is:\n\\[ \\ell_n(\\beta) = \\sum_{i=1}^n \\sum_{j=1}^J \\delta_{ij} \\log(\\mathbb{P}_i(j)) \\]\n\n\nYogurt Dataset\nWe will use the yogurt_data dataset, which provides anonymized consumer identifiers (id), a vector indicating the chosen product (y1:y4), a vector indicating if any products were “featured” in the store as a form of advertising (f1:f4), and the products’ prices (p1:p4). For example, consumer 1 purchased yogurt 4 at a price of 0.079/oz and none of the yogurts were featured/advertised at the time of consumer 1’s purchase. Consumers 2 through 7 each bought yogurt 2, etc.\nFirst, we will import the dataset and take a look at it in a dataframe format.\n\nimport pandas as pd\n\nyogurt = pd.read_csv('yogurt_data.csv')\n\nyogurt.head()\n\n\n\n\n\n\n\n\nid\ny1\ny2\ny3\ny4\nf1\nf2\nf3\nf4\np1\np2\np3\np4\n\n\n\n\n0\n1\n0\n0\n0\n1\n0\n0\n0\n0\n0.108\n0.081\n0.061\n0.079\n\n\n1\n2\n0\n1\n0\n0\n0\n0\n0\n0\n0.108\n0.098\n0.064\n0.075\n\n\n2\n3\n0\n1\n0\n0\n0\n0\n0\n0\n0.108\n0.098\n0.061\n0.086\n\n\n3\n4\n0\n1\n0\n0\n0\n0\n0\n0\n0.108\n0.098\n0.061\n0.086\n\n\n4\n5\n0\n1\n0\n0\n0\n0\n0\n0\n0.125\n0.098\n0.049\n0.079\n\n\n\n\n\n\n\nNext, we’ll get more info on the data to describe it.\n\nyogurt.shape\n\n(2430, 13)\n\n\n\nyogurt.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 2430 entries, 0 to 2429\nData columns (total 13 columns):\n #   Column  Non-Null Count  Dtype  \n---  ------  --------------  -----  \n 0   id      2430 non-null   int64  \n 1   y1      2430 non-null   int64  \n 2   y2      2430 non-null   int64  \n 3   y3      2430 non-null   int64  \n 4   y4      2430 non-null   int64  \n 5   f1      2430 non-null   int64  \n 6   f2      2430 non-null   int64  \n 7   f3      2430 non-null   int64  \n 8   f4      2430 non-null   int64  \n 9   p1      2430 non-null   float64\n 10  p2      2430 non-null   float64\n 11  p3      2430 non-null   float64\n 12  p4      2430 non-null   float64\ndtypes: float64(4), int64(9)\nmemory usage: 246.9 KB\n\n\n\nyogurt.describe()\n\n\n\n\n\n\n\n\nid\ny1\ny2\ny3\ny4\nf1\nf2\nf3\nf4\np1\np2\np3\np4\n\n\n\n\ncount\n2430.0000\n2430.000000\n2430.000000\n2430.000000\n2430.000000\n2430.000000\n2430.000000\n2430.000000\n2430.000000\n2430.000000\n2430.000000\n2430.000000\n2430.000000\n\n\nmean\n1215.5000\n0.341975\n0.401235\n0.029218\n0.227572\n0.055556\n0.039506\n0.037449\n0.037449\n0.106248\n0.081532\n0.053622\n0.079507\n\n\nstd\n701.6249\n0.474469\n0.490249\n0.168452\n0.419351\n0.229109\n0.194836\n0.189897\n0.189897\n0.020587\n0.011047\n0.008054\n0.007714\n\n\nmin\n1.0000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n-0.012000\n0.000000\n0.025000\n0.004000\n\n\n25%\n608.2500\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.103000\n0.081000\n0.050000\n0.079000\n\n\n50%\n1215.5000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.108000\n0.086000\n0.054000\n0.079000\n\n\n75%\n1822.7500\n1.000000\n1.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.115000\n0.086000\n0.061000\n0.086000\n\n\nmax\n2430.0000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n0.193000\n0.111000\n0.086000\n0.104000\n\n\n\n\n\n\n\nAfter conducting some research on the dataset, it is shown that the rate at which products y1 through y4 are selected are between 2% and 40%, with 2% being extremely low compared to the other products (23%, 34%). Each product also has a large standard deviation of being selected, likely because the product is either selected (1) or not selected (0). Also, the averages of each product being selected sum to 1, meaning there is no missing data for the y columns in the dataset.\nThe products were featured approximately same proportion of time (between 3.7%-5.6%), with product 1 being featured more than others at 5.6%, but it was not the most selected product. That product is y2. The summed average of featured products does not sum to 1, meaning a product wasn’t always featured. The standard deviation are also very large, likely for the same reason as mentioned above (binary). They are slightly higher SD’s proportionally to the averages, which is likely because there are much more 0’s than 1’s in columns due to the fact that there doesn’t have to be a product featured.\nAverage prices per ounce range from approximately $0.11 to $0.05. These standard deviations are not nearly as proportionally high as the other columns, likely because the prices are continuous and not binary. Price seems to potentially follow a normal distribution.\nLet the vector of product features include brand dummy variables for yogurts 1-3 (we’ll omit a dummy for product 4 to avoid multi-collinearity), a dummy variable to indicate if a yogurt was featured, and a continuous variable for the yogurts’ prices:\n\\[ x_j' = [\\mathbbm{1}(\\text{Yogurt 1}), \\mathbbm{1}(\\text{Yogurt 2}), \\mathbbm{1}(\\text{Yogurt 3}), X_f, X_p] \\]\nThe “hard part” of the MNL likelihood function is organizing the data, as we need to keep track of 3 dimensions (consumer \\(i\\), covariate \\(k\\), and product \\(j\\)) instead of the typical 2 dimensions for cross-sectional regression models (consumer \\(i\\) and covariate \\(k\\)).\nWhat we would like to do is reorganize the data from a “wide” shape with \\(n\\) rows and multiple columns for each covariate, to a “long” shape with \\(n \\times J\\) rows and a single column for each covariate. As part of this re-organization, we’ll add binary variables to indicate the first 3 products; the variables for featured and price are included in the dataset and simply need to be “pivoted” or “melted” from wide to long.\n\n# Melting data to be in long format\nlong_data = yogurt.melt(id_vars='id', \n                             value_vars=['y1', 'y2', 'y3', 'y4', 'f1', 'f2', 'f3', 'f4', 'p1', 'p2', 'p3', 'p4'],\n                             var_name='variable', \n                             value_name='value')\n\n# Adding separate columns for type and product number\nlong_data['type'] = long_data['variable'].str[0]\nlong_data['product_num'] = long_data['variable'].str[1].astype(int)\n\n# Pivoting to have separate columns for each data type (purchase, featured, price)\nlong_data = long_data.pivot_table(index=['id', 'product_num'], columns='type', values='value', aggfunc='first').reset_index()\n\n# Renaming the columns\nlong_data.columns = ['id', 'product_num', 'featured', 'price', 'purchase']\n\n# Making product number an integer\nlong_data['yogurt_1'] = (long_data['product_num'] == 1).astype(int)\nlong_data['yogurt_2'] = (long_data['product_num'] == 2).astype(int)\nlong_data['yogurt_3'] = (long_data['product_num'] == 3).astype(int)\n\n\n\nEstimation\nBelow, I’ve created a custom function to calculate the log likelihood of a logistic regression. It returns the negative log likelihood in order to minimze the negative (same as maxmimizing if positive). Restrictions with python functions only allow us to minimize.\n\nfrom scipy.optimize import minimize\nfrom sklearn.preprocessing import StandardScaler\nimport numpy as np\n\n# Define the logistic regression likelihood function\ndef logistic_neg_log_likelihood(beta, X, y):\n    # Linear combination: X * beta\n    z = np.dot(X, beta)\n    # Logistic function application\n    probability = 1 / (1 + np.exp(-z))\n    # Log-likelihood\n    log_likelihood = np.sum(y * np.log(probability) + (1 - y) * np.log(1 - probability))\n    # Return negative log-likelihood\n    return -log_likelihood\n\nNow we will optimize our function to find the maximum likelihood estimators for the betas or coefficients of each explanatory variable. First, the variables are scaled, then we add a constant variable and a array of zeros for initial beta estimates.\n\n# Standardize features\nscaler = StandardScaler()\nfeatures_scaled = scaler.fit_transform(long_data[['featured', 'price', 'yogurt_1', 'yogurt_2', 'yogurt_3']])\n\n# Update the feature matrix X to include the scaled features and an intercept\nX_logistic = np.hstack((np.ones((long_data.shape[0], 1)), features_scaled))  # Add intercept\ny_logistic = long_data['purchase'].astype(int).values  # Binary purchase outcomes\n\n# Initial beta estimates\ninitial_beta_logistic = np.zeros(X_logistic.shape[1])\n\n# Running the optimization for logistic regression\nresult_logistic = minimize(logistic_neg_log_likelihood, initial_beta_logistic, args=(X_logistic, y_logistic), method='BFGS')\nresult_logistic\n\n  message: Desired error not necessarily achieved due to precision loss.\n  success: False\n   status: 2\n      fun: 4645.881282128891\n        x: [-1.468e+00  9.508e-02 -7.251e-01  6.140e-01  3.903e-01\n            -1.360e+00]\n      nit: 19\n      jac: [ 6.104e-05  6.104e-05  0.000e+00  0.000e+00  0.000e+00\n             6.104e-05]\n hess_inv: [[ 2.432e-05 -1.092e-05 ...  7.449e-06 -3.793e-06]\n            [-1.092e-05  5.426e-04 ... -2.932e-04  1.295e-05]\n            ...\n            [ 7.449e-06 -2.932e-04 ...  7.850e-04 -5.431e-04]\n            [-3.793e-06  1.295e-05 ... -5.431e-04  1.323e-03]]\n     nfev: 196\n     njev: 28\n\n\n\ncoefficients = result_logistic.x\n\ncoefficients\n\narray([-1.46768525,  0.09508497, -0.72514485,  0.61395611,  0.39025875,\n       -1.35989235])\n\n\n\n\nDiscussion\nWe learn…\nThe most preferred yogurt is Yogurt #1, which has a coefficient or beta of 0.614. The next highest is Yogurt #2, which has a coefficient of 0.390. Yogurt #4 which is not featured is the base case, so it has a coefficient of 0. The least preferred yogurt is Yogurt #3, which has a coefficient of -1.360.\nFor an example of how much more profitable one yogurt is over the other based on our model, we’ll calculate the dollar benefit of yogurt 1 (most preferred) over yogurt 3 (least preferred). We’ll do this by finding the difference in betas between the two, then dividing by the price coefficient.\n\ndollar_benefit = (coefficients[3] - coefficients[5]) /abs(coefficients[2])\n\ndollar_benefit\n\n2.722005784519816\n\n\nThe dollar benefit of yogurt 1 over yogurt 3 is about $2.72 based on our model.\nOne benefit of the MNL model is that we can simulate counterfactuals (eg, what if the price of yogurt 1 was $0.10/oz instead of $0.08/oz).\nNow, lets see how increasing the price of yogurt 1 to $0.10/oz vice $0.08/oz affects its market share.\nFirst we’ll calculate the current market share of each yogurt.\n\ny1 = yogurt['y1'].sum()\ny2 = yogurt['y2'].sum()\ny3 = yogurt['y3'].sum()\ny4 = yogurt['y4'].sum()\n\ny1_share = y1 / (y1 + y2 + y3 + y4)\ny2_share = y2 / (y1 + y2 + y3 + y4)\ny3_share = y3 / (y1 + y2 + y3 + y4)\ny4_share = y4 / (y1 + y2 + y3 + y4)\n\ny1_share, y2_share, y3_share, y4_share\n\n(0.3419753086419753,\n 0.4012345679012346,\n 0.029218106995884775,\n 0.22757201646090536)\n\n\nYogurt 1 is at 34% market share. Now lets see if our logistic regression model can calculate the same.\n\n# consolidate columns y1 to y4 into a single column\n\nyogurt['yogurt_type'] = yogurt[['y1', 'y2', 'y3', 'y4']].idxmax(axis=1).str[1].astype(int)\n\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\nimport numpy as np\n\n\nX = yogurt[['f1', 'f2', 'f3', 'f4', 'p1', 'p2', 'p3', 'p4']].values\ny = yogurt['yogurt_type'].astype(int).values  \n\n\n# Fit the multinomial logistic regression model\nmodel = LogisticRegression(multi_class='multinomial', solver='lbfgs', max_iter=1000)\nmodel.fit(X, y)\n\n#  Predict probabilities\nprobabilities = model.predict_proba(X)\n\n# Calculate original market shares\nmarket_shares_original = probabilities.mean(axis=0)\n\nmarket_shares_original\n\narray([0.34197574, 0.40123432, 0.02921825, 0.22757169])\n\n\nOur initial market shares calculated above match the logistic regression model’s calculation. Lets see how it predicts market shares will change when increasing yogurt 1’s price per ounce.\n\nyogurt['p1_new'] = yogurt['p1'] + 0.10\n\nX_new = yogurt[['f1', 'f2', 'f3', 'f4', 'p1_new', 'p2', 'p3', 'p4']].values\n\n# Predict new probabilities with the adjusted prices\nprobabilities_new = model.predict_proba(X_new)\n\n# Calculate new market shares\nmarket_shares_new = probabilities_new.mean(axis=0)\n\n# Output the results\nprint(\"Original Market Shares:\", market_shares_original)\nprint(\"New Market Shares After Price Increase:\", market_shares_new)\n\nOriginal Market Shares: [0.34197574 0.40123432 0.02921825 0.22757169]\nNew Market Shares After Price Increase: [0.19483994 0.50127048 0.02768742 0.27620216]\n\n\nThe model predicted that yogurt 1 market shares would decrease by nearly 15% when the price was increase to $0.10/oz."
  },
  {
    "objectID": "projects/HW3/hw3_questions.html#estimating-minivan-preferences",
    "href": "projects/HW3/hw3_questions.html#estimating-minivan-preferences",
    "title": "Poisson",
    "section": "2. Estimating Minivan Preferences",
    "text": "2. Estimating Minivan Preferences\nNext, we’ll take a look at a different dataset which we’ll conduct a conjoint analysis on. The dataset is for preference of features in minivans.\n\nData\n\nconjoint = pd.read_csv('conjoint.csv')\n\nconjoint.head()\n\n\n\n\n\n\n\n\nresp.id\nques\nalt\ncarpool\nseat\ncargo\neng\nprice\nchoice\n\n\n\n\n0\n1\n1\n1\nyes\n6\n2ft\ngas\n35\n0\n\n\n1\n1\n1\n2\nyes\n8\n3ft\nhyb\n30\n0\n\n\n2\n1\n1\n3\nyes\n6\n3ft\ngas\n30\n1\n\n\n3\n1\n2\n1\nyes\n6\n2ft\ngas\n30\n0\n\n\n4\n1\n2\n2\nyes\n7\n3ft\ngas\n35\n1\n\n\n\n\n\n\n\n\n# number of unique respondents\nconjoint['resp.id'].nunique()\n\n200\n\n\n\n# number of choice tasks\nconjoint[conjoint['resp.id'] == 1]['ques'].nunique()\n\n15\n\n\n\n# number of alternatives\nconjoint['alt'].nunique()\n\n3\n\n\nThe data within the conjoint dataset is in long format, meaning it is organized in a way that each row represents a single observation. The data is organized by respondent, choice task, and alternative. There are 200 respondents who took the conjoint survey, and each respondent completed 15 choice tasks. Each choice task presented 3 alternatives to the respondent. Each alternative is represented by a set of attributes, such as brand, price, and feature.\nThe attributes (levels) were number of seats (6,7,8), cargo space (2ft, 3ft), engine type (gas, hybrid, electric), and price (in thousands of dollars).\n\n\nModel\nLets estimate a Multinomial Logistic model to see what features were most preferred.\n\n# Converting non-numeric columns to boolean\nconjoint = pd.get_dummies(conjoint, columns=['seat', 'cargo', 'eng'], drop_first=False)\n\n# Dropping first column from each feature to prevent multicollinearity\nconjoint.drop(['seat_6', 'cargo_2ft', 'eng_gas'], axis=1, inplace=True)\n\nX = conjoint[['price', 'seat_7', 'seat_8', 'cargo_3ft', 'eng_hyb', 'eng_elec']]  # predictors\ny = conjoint['choice']  # response variable\n\n\n# Fit the multinomial logistic regression model\nmodel = LogisticRegression(multi_class='multinomial', solver='lbfgs', max_iter=1000)\nmodel.fit(X, y)\n\nmodel.coef_\n\narray([[-0.07952075, -0.26151586, -0.14596134,  0.21885267, -0.37897658,\n        -0.71594544]])\n\n\n\n\nResults\nIn terms of price, a cheaper price is more preferred as it has a negative coefficient.\nFor seats, both 7 and 8 seats have negative coefficients, meaning that seat 6 is the most preferred.\nFor cargo space, 3 ft is a positive coefficient, meaning its the most preferred.\nFinally, for engine type, both electric and hybrid have negative coefficients, meaning that gas is the most preferred.\nLets see how 3ft of crago space compares to 2ft of argo space in terms of dollar benefit.\n\ncargo_3ft = model.coef_[0][3]\nprice = model.coef_[0][0]\n\ndollar_value = (cargo_3ft / abs(price)) * 1000\n\ndollar_value.round(2)\n\n2752.15\n\n\nThe 3 ft of cargo space has a $2752.15 dollar benefit over the 2 ft cargo space.\nUsing the same logic as above for our MNL model, lets predict the market shares for the below data on minivans.\n\n\n\nMinivan\nSeats\nCargo\nEngine\nPrice\n\n\n\n\nA\n7\n2\nHyb\n30\n\n\nB\n6\n2\nGas\n30\n\n\nC\n8\n2\nGas\n30\n\n\nD\n7\n3\nGas\n40\n\n\nE\n6\n2\nElec\n40\n\n\nF\n7\n2\nHyb\n35\n\n\n\nFirst, we’ll create the dataset in python and make it a DataFrame.\n\ndata = {\n    \"minivan\": ['A', 'B', 'C', 'D', 'E', 'F'],\n    \"seat\": [7, 6, 8, 7, 6, 7],\n    \"cargo\": [2, 2, 2, 3, 2, 2],\n    \"eng\": ['hyb', 'gas', 'gas', 'gas', 'elec', 'hyb'],\n    \"price\": [30, 30, 30, 40, 40, 35]\n}\n\nminivans = pd.DataFrame(data)\n\n\nminivans\n\n\n\n\n\n\n\n\nminivan\nseat\ncargo\neng\nprice\n\n\n\n\n0\nA\n7\n2\nhyb\n30\n\n\n1\nB\n6\n2\ngas\n30\n\n\n2\nC\n8\n2\ngas\n30\n\n\n3\nD\n7\n3\ngas\n40\n\n\n4\nE\n6\n2\nelec\n40\n\n\n5\nF\n7\n2\nhyb\n35\n\n\n\n\n\n\n\nNext, we’ll shape the data as we did in previous models.\n\nminivans = pd.get_dummies(minivans, columns=['seat', 'cargo', 'eng'], drop_first=False)\n\nminivans.drop(['seat_6', 'cargo_2', 'eng_gas'], axis=1, inplace=True)\n\n# rename cargo_3 to cargo_3ft\n\nminivans.rename(columns={'cargo_3': 'cargo_3ft'}, inplace=True)\n\nx_minivan = minivans[['price', 'seat_7', 'seat_8', 'cargo_3ft', 'eng_hyb', 'eng_elec']]\n\npredictions = model.predict_proba(x_minivan)[:,1]\n\npredictions\n\narray([0.37160976, 0.68041243, 0.61390225, 0.28494289, 0.09392392,\n       0.21073102])\n\n\n\n# add predictions to the minivans dataframe\n\nminivans['predictions'] = predictions\n\n# calculate market share for each\n\nminivans['market_share'] = minivans['predictions']/minivans['predictions'].sum()\n\nminivans\n\n\n\n\n\n\n\n\nminivan\nprice\nseat_7\nseat_8\ncargo_3ft\neng_elec\neng_hyb\npredictions\nmarket_share\n\n\n\n\n0\nA\n30\nTrue\nFalse\nFalse\nFalse\nTrue\n0.371610\n0.164756\n\n\n1\nB\n30\nFalse\nFalse\nFalse\nFalse\nFalse\n0.680412\n0.301665\n\n\n2\nC\n30\nFalse\nTrue\nFalse\nFalse\nFalse\n0.613902\n0.272177\n\n\n3\nD\n40\nTrue\nFalse\nTrue\nFalse\nFalse\n0.284943\n0.126331\n\n\n4\nE\n40\nFalse\nFalse\nFalse\nTrue\nFalse\n0.093924\n0.041642\n\n\n5\nF\n35\nTrue\nFalse\nFalse\nFalse\nTrue\n0.210731\n0.093429\n\n\n\n\n\n\n\nAs you can see from the results, Minivan B has the highest prediction value and market share with 30%. The lowest is Minuvan E, with only 4.2% market share.\nLooking at this logically, Minivan B does have 6 seats which is the most preferred, it has a gas engine which is most preferred, and 2 ft of cargo space which is the least preferred out of the two choices. However, 2 ft of cargo space has a coefficient of 0, rather than negative."
  },
  {
    "objectID": "projects/HW3/HW3.html",
    "href": "projects/HW3/HW3.html",
    "title": "Nick_Shuckerow",
    "section": "",
    "text": "Suppose we have \\(i=1,\\ldots,n\\) consumers who each select exactly one product \\(j\\) from a set of \\(J\\) products. The outcome variable is the identity of the product chosen \\(y_i \\in \\{1, \\ldots, J\\}\\) or equivalently a vector of \\(J-1\\) zeros and \\(1\\) one, where the \\(1\\) indicates the selected product. For example, if the third product was chosen out of 4 products, then either \\(y=3\\) or \\(y=(0,0,1,0)\\) depending on how we want to represent it. Suppose also that we have a vector of data on each product \\(x_j\\) (eg, size, price, etc.).\nWe model the consumer’s decision as the selection of the product that provides the most utility, and we’ll specify the utility function as a linear function of the product characteristics:\n\\[ U_{ij} = x_j'\\beta + \\epsilon_{ij} \\]\nwhere \\(\\epsilon_{ij}\\) is an i.i.d. extreme value error term.\nThe choice of the i.i.d. extreme value error term leads to a closed-form expression for the probability that consumer \\(i\\) chooses product \\(j\\):\n\\[ \\mathbb{P}_i(j) = \\frac{e^{x_j'\\beta}}{\\sum_{k=1}^Je^{x_k'\\beta}} \\]\nFor example, if there are 4 products, the probability that consumer \\(i\\) chooses product 3 is:\n\\[ \\mathbb{P}_i(3) = \\frac{e^{x_3'\\beta}}{e^{x_1'\\beta} + e^{x_2'\\beta} + e^{x_3'\\beta} + e^{x_4'\\beta}} \\]\nA clever way to write the individual likelihood function for consumer \\(i\\) is the product of the \\(J\\) probabilities, each raised to the power of an indicator variable (\\(\\delta_{ij}\\)) that indicates the chosen product:\n\\[ L_i(\\beta) = \\prod_{j=1}^J \\mathbb{P}_i(j)^{\\delta_{ij}} = \\mathbb{P}_i(1)^{\\delta_{i1}} \\times \\ldots \\times \\mathbb{P}_i(J)^{\\delta_{iJ}}\\]\nNotice that if the consumer selected product \\(j=3\\), then \\(\\delta_{i3}=1\\) while \\(\\delta_{i1}=\\delta_{i2}=\\delta_{i4}=0\\) and the likelihood is:\n\\[ L_i(\\beta) = \\mathbb{P}_i(1)^0 \\times \\mathbb{P}_i(2)^0 \\times \\mathbb{P}_i(3)^1 \\times \\mathbb{P}_i(4)^0 = \\mathbb{P}_i(3) = \\frac{e^{x_3'\\beta}}{\\sum_{k=1}^Je^{x_k'\\beta}} \\]\nThe joint likelihood (across all consumers) is the product of the \\(n\\) individual likelihoods:\n\\[ L_n(\\beta) = \\prod_{i=1}^n L_i(\\beta) = \\prod_{i=1}^n \\prod_{j=1}^J \\mathbb{P}_i(j)^{\\delta_{ij}} \\]\nAnd the joint log-likelihood function is:\n\\[ \\ell_n(\\beta) = \\sum_{i=1}^n \\sum_{j=1}^J \\delta_{ij} \\log(\\mathbb{P}_i(j)) \\]\nWe will use the yogurt_data dataset, which provides anonymized consumer identifiers (id), a vector indicating the chosen product (y1:y4), a vector indicating if any products were “featured” in the store as a form of advertising (f1:f4), and the products’ prices (p1:p4). For example, consumer 1 purchased yogurt 4 at a price of 0.079/oz and none of the yogurts were featured/advertised at the time of consumer 1’s purchase. Consumers 2 through 7 each bought yogurt 2, etc.\ntodo: import the data, maybe show the first few rows, and describe the data a bit.\n\nimport pandas as pd\n\nyogurt = pd.read_csv('yogurt_data.csv')\n\nyogurt.head()\n\n\n\n\n\n\n\n\nid\ny1\ny2\ny3\ny4\nf1\nf2\nf3\nf4\np1\np2\np3\np4\n\n\n\n\n0\n1\n0\n0\n0\n1\n0\n0\n0\n0\n0.108\n0.081\n0.061\n0.079\n\n\n1\n2\n0\n1\n0\n0\n0\n0\n0\n0\n0.108\n0.098\n0.064\n0.075\n\n\n2\n3\n0\n1\n0\n0\n0\n0\n0\n0\n0.108\n0.098\n0.061\n0.086\n\n\n3\n4\n0\n1\n0\n0\n0\n0\n0\n0\n0.108\n0.098\n0.061\n0.086\n\n\n4\n5\n0\n1\n0\n0\n0\n0\n0\n0\n0.125\n0.098\n0.049\n0.079\n\n\n\n\n\n\n\n\nyogurt.shape\n\n(2430, 13)\n\n\n\nyogurt.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 2430 entries, 0 to 2429\nData columns (total 13 columns):\n #   Column  Non-Null Count  Dtype  \n---  ------  --------------  -----  \n 0   id      2430 non-null   int64  \n 1   y1      2430 non-null   int64  \n 2   y2      2430 non-null   int64  \n 3   y3      2430 non-null   int64  \n 4   y4      2430 non-null   int64  \n 5   f1      2430 non-null   int64  \n 6   f2      2430 non-null   int64  \n 7   f3      2430 non-null   int64  \n 8   f4      2430 non-null   int64  \n 9   p1      2430 non-null   float64\n 10  p2      2430 non-null   float64\n 11  p3      2430 non-null   float64\n 12  p4      2430 non-null   float64\ndtypes: float64(4), int64(9)\nmemory usage: 246.9 KB\n\n\n\nyogurt.describe()\n\n\n\n\n\n\n\n\nid\ny1\ny2\ny3\ny4\nf1\nf2\nf3\nf4\np1\np2\np3\np4\n\n\n\n\ncount\n2430.0000\n2430.000000\n2430.000000\n2430.000000\n2430.000000\n2430.000000\n2430.000000\n2430.000000\n2430.000000\n2430.000000\n2430.000000\n2430.000000\n2430.000000\n\n\nmean\n1215.5000\n0.341975\n0.401235\n0.029218\n0.227572\n0.055556\n0.039506\n0.037449\n0.037449\n0.106248\n0.081532\n0.053622\n0.079507\n\n\nstd\n701.6249\n0.474469\n0.490249\n0.168452\n0.419351\n0.229109\n0.194836\n0.189897\n0.189897\n0.020587\n0.011047\n0.008054\n0.007714\n\n\nmin\n1.0000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n-0.012000\n0.000000\n0.025000\n0.004000\n\n\n25%\n608.2500\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.103000\n0.081000\n0.050000\n0.079000\n\n\n50%\n1215.5000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.108000\n0.086000\n0.054000\n0.079000\n\n\n75%\n1822.7500\n1.000000\n1.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.115000\n0.086000\n0.061000\n0.086000\n\n\nmax\n2430.0000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n0.193000\n0.111000\n0.086000\n0.104000\n\n\n\n\n\n\n\nAfter conducting some research on the dataset, it is shown that the rate at which products y1 through y4 are selected are between 2% and 40%, with 2% being extremely low compared to the other products (23%, 34%). Each product also has a large standard deviation of being selected, likely because the product is either selected (1) or not selected (0). Also, the averages of each product being selected sum to 1, meaning there is no missing data for the y columns in the dataset.\nThe products were featured approximately same proportion of time (between 3.7%-5.6%), with product 1 being featured more than others at 5.6%, but it was not the most selected product. That product is y2. The summed average of featured products does not sum to 1, meaning a product wasn’t always featured. The standard deviation are also very large, likely for the same reason as mentioned above (binary). They are slightly higher SD’s proportionally to the averages, which is likely because there are much more 0’s than 1’s in columns due to the fact that there doesn’t have to be a product featured.\nAverage prices per ounce range from approximately $0.11 to $0.05. These standard deviations are not nearly as proportionally high as the other columns, likely because the prices are continuous and not binary. Price seems to potentially follow a normal distribution.\nLet the vector of product features include brand dummy variables for yogurts 1-3 (we’ll omit a dummy for product 4 to avoid multi-collinearity), a dummy variable to indicate if a yogurt was featured, and a continuous variable for the yogurts’ prices:\n\\[ x_j' = [\\mathbf{1}_{\\text{Yogurt 1}}, \\mathbf{1}_{\\text{Yogurt 2}}, \\mathbf{1}_{\\text{Yogurt 3}}, X_f, X_p] \\]\nThe “hard part” of the MNL likelihood function is organizing the data, as we need to keep track of 3 dimensions (consumer \\(i\\), covariate \\(k\\), and product \\(j\\)) instead of the typical 2 dimensions for cross-sectional regression models (consumer \\(i\\) and covariate \\(k\\)).\nWhat we would like to do is reorganize the data from a “wide” shape with \\(n\\) rows and multiple columns for each covariate, to a “long” shape with \\(n \\times J\\) rows and a single column for each covariate. As part of this re-organization, we’ll add binary variables to indicate the first 3 products; the variables for featured and price are included in the dataset and simply need to be “pivoted” or “melted” from wide to long.\ntodo: reshape and prep the data\n\n# Melting and restructuring the data in one step\nlong_data = yogurt.melt(id_vars='id', \n                             value_vars=['y1', 'y2', 'y3', 'y4', 'f1', 'f2', 'f3', 'f4', 'p1', 'p2', 'p3', 'p4'],\n                             var_name='variable', \n                             value_name='value')\n\n# Adding separate columns for type and product number directly\nlong_data['type'] = long_data['variable'].str[0]\nlong_data['product_num'] = long_data['variable'].str[1].astype(int)\n\n# Pivoting to have separate columns for each data type (purchase, featured, price)\nlong_data = long_data.pivot_table(index=['id', 'product_num'], columns='type', values='value', aggfunc='first').reset_index()\n\n# Renaming the columns for clarity\nlong_data.columns = ['id', 'product_num', 'featured', 'price', 'purchase']\n\n# Creating binary indicators for the first three products\nlong_data['yogurt_1'] = (long_data['product_num'] == 1).astype(int)\nlong_data['yogurt_2'] = (long_data['product_num'] == 2).astype(int)\nlong_data['yogurt_3'] = (long_data['product_num'] == 3).astype(int)\n\n\nlong_data.head()\n\n\n\n\n\n\n\n\nid\nproduct_num\nfeatured\nprice\npurchase\nyogurt_1\nyogurt_2\nyogurt_3\n\n\n\n\n0\n1\n1\n0.0\n0.108\n0.0\n1\n0\n0\n\n\n1\n1\n2\n0.0\n0.081\n0.0\n0\n1\n0\n\n\n2\n1\n3\n0.0\n0.061\n0.0\n0\n0\n1\n\n\n3\n1\n4\n0.0\n0.079\n1.0\n0\n0\n0\n\n\n4\n2\n1\n0.0\n0.108\n0.0\n1\n0\n0\n\n\n\n\n\n\n\ntodo: Code up the log-likelihood function.\n\nfrom scipy.optimize import minimize\nfrom sklearn.preprocessing import StandardScaler\nimport numpy as np\n\n# Define the logistic regression likelihood function\ndef logistic_neg_log_likelihood(beta, X, y):\n \n    # Linear combination: X * beta\n    z = np.dot(X, beta)\n    # Logistic function application\n    probability = 1 / (1 + np.exp(-z))\n    # Log-likelihood\n    log_likelihood = np.sum(y * np.log(probability) + (1 - y) * np.log(1 - probability))\n    # Return negative log-likelihood\n    return -log_likelihood\n\ntodo: Use optim() in R or optimize() in Python to find the MLEs for the 5 parameters (\\(\\beta_1, \\beta_2, \\beta_3, \\beta_f, \\beta_p\\)). (Hint: you should find 2 positive and 1 negative product intercepts, a small positive coefficient estimate for featured, and a large negative coefficient estimate for price.)\n\n# Standardize features\nscaler = StandardScaler()\nfeatures_scaled = scaler.fit_transform(long_data[['featured', 'price', 'yogurt_1', 'yogurt_2', 'yogurt_3']])\n\n# Update the feature matrix X to include the scaled features and an intercept\nX_logistic = np.hstack((np.ones((long_data.shape[0], 1)), features_scaled))  \ny_logistic = long_data['purchase'].astype(int).values\n\n# Initial beta estimates\ninitial_beta_logistic = np.zeros(X_logistic.shape[1])\n\n# Running the optimization for logistic regression\nresult_logistic = minimize(logistic_neg_log_likelihood, initial_beta_logistic, args=(X_logistic, y_logistic), method='BFGS')\nresult_logistic\n\n  message: Desired error not necessarily achieved due to precision loss.\n  success: False\n   status: 2\n      fun: 4645.881282128891\n        x: [-1.468e+00  9.508e-02 -7.251e-01  6.140e-01  3.903e-01\n            -1.360e+00]\n      nit: 19\n      jac: [ 6.104e-05  6.104e-05  0.000e+00  0.000e+00  0.000e+00\n             6.104e-05]\n hess_inv: [[ 2.432e-05 -1.092e-05 ...  7.449e-06 -3.793e-06]\n            [-1.092e-05  5.426e-04 ... -2.932e-04  1.295e-05]\n            ...\n            [ 7.449e-06 -2.932e-04 ...  7.850e-04 -5.431e-04]\n            [-3.793e-06  1.295e-05 ... -5.431e-04  1.323e-03]]\n     nfev: 196\n     njev: 28\n\n\n\ncoefficients = result_logistic.x\n\ncoefficients\n\narray([-1.46768525,  0.09508497, -0.72514485,  0.61395611,  0.39025875,\n       -1.35989235])\n\n\ntodo: interpret the 3 product intercepts (which yogurt is most preferred?).\nThe most preferred yogurt is Yogurt #1, which has a coefficient or beta of 0.614. The next highest is Yogurt #2, which has a coefficient of 0.390. Yogurt #4 which is not featured is the base case, so it has a coefficient of 0. The least preferred yogurt is Yogurt #3, which has a coefficient of -1.360.\ntodo: use the estimated price coefficient as a dollar-per-util conversion factor. Use this conversion factor to calculate the dollar benefit between the most-preferred yogurt (the one with the highest intercept) and the least preferred yogurt (the one with the lowest intercept). This is a per-unit monetary measure of brand value.\n\ndollar_benefit = (coefficients[3] - coefficients[5]) /abs(coefficients[2])\n\ndollar_benefit\n\n2.722005784519816\n\n\nThe dollar benefit between the most-preferred yogurt and the least preferred yogurt is $1.43. This means that the most preferred yogurt is worth $1.43 more than the least preferred yogurt.\nOne benefit of the MNL model is that we can simulate counterfactuals (eg, what if the price of yogurt 1 was $0.10/oz instead of $0.08/oz).\ntodo: calculate the market shares in the market at the time the data were collected. Then, increase the price of yogurt 1 by $0.10 and use your fitted model to predict p(y|x) for each consumer and each product (this should be a matrix of \\(N \\times 4\\) estimated choice probabilities. Take the column averages to get the new, expected market shares that result from the $0.10 price increase to yogurt 1. Do the yogurt 1 market shares decrease?\n\ny1 = yogurt['y1'].sum()\ny2 = yogurt['y2'].sum()\ny3 = yogurt['y3'].sum()\ny4 = yogurt['y4'].sum()\n\ny1_share = y1 / (y1 + y2 + y3 + y4)\ny2_share = y2 / (y1 + y2 + y3 + y4)\ny3_share = y3 / (y1 + y2 + y3 + y4)\ny4_share = y4 / (y1 + y2 + y3 + y4)\n\ny1_share, y2_share, y3_share, y4_share\n\n(0.3419753086419753,\n 0.4012345679012346,\n 0.029218106995884775,\n 0.22757201646090536)\n\n\n\n# consolidate columns y1 to y4 into a single column\n\nyogurt['yogurt_type'] = yogurt[['y1', 'y2', 'y3', 'y4']].idxmax(axis=1).str[1].astype(int)\n\n\n\n\n\n\n\n\nid\ny1\ny2\ny3\ny4\nf1\nf2\nf3\nf4\np1\np2\np3\np4\nyogurt_type\n\n\n\n\n0\n1\n0\n0\n0\n1\n0\n0\n0\n0\n0.108\n0.081\n0.061\n0.079\n4\n\n\n1\n2\n0\n1\n0\n0\n0\n0\n0\n0\n0.108\n0.098\n0.064\n0.075\n2\n\n\n2\n3\n0\n1\n0\n0\n0\n0\n0\n0\n0.108\n0.098\n0.061\n0.086\n2\n\n\n3\n4\n0\n1\n0\n0\n0\n0\n0\n0\n0.108\n0.098\n0.061\n0.086\n2\n\n\n4\n5\n0\n1\n0\n0\n0\n0\n0\n0\n0.125\n0.098\n0.049\n0.079\n2\n\n\n\n\n\n\n\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\nimport numpy as np\nimport pandas as pd\n\n# Assuming 'long_data' is your DataFrame and it's already loaded\n# The 'purchase' column should be categorical with each category representing a different type of yogurt\n\n# Prepare the features and target\nX = yogurt[['f1', 'f2', 'f3', 'f4', 'p1', 'p2', 'p3', 'p4']].values\ny = yogurt['yogurt_type'].astype(int).values  # Ensure y is an integer type representing different categories\n\n\n# Fit the multinomial logistic regression model\nmodel = LogisticRegression(multi_class='multinomial', solver='lbfgs', max_iter=1000)\nmodel.fit(X, y)\n\n#  Predict probabilities\nprobabilities = model.predict_proba(X)\n\n# Calculate original market shares\nmarket_shares_original = probabilities.mean(axis=0)\n\nmarket_shares_original\n\narray([0.34197574, 0.40123432, 0.02921825, 0.22757169])\n\n\n\nyogurt['p1_new'] = yogurt['p1'] + 0.10\n\nX_new = yogurt[['f1', 'f2', 'f3', 'f4', 'p1_new', 'p2', 'p3', 'p4']].values\n\n# Predict new probabilities with the adjusted prices\nprobabilities_new = model.predict_proba(X_new)\n\n# Calculate new market shares\nmarket_shares_new = probabilities_new.mean(axis=0)\n\n# Output the results\nprint(\"Original Market Shares:\", market_shares_original)\nprint(\"New Market Shares After Price Increase:\", market_shares_new)\n\nOriginal Market Shares: [0.34197574 0.40123432 0.02921825 0.22757169]\nNew Market Shares After Price Increase: [0.19483994 0.50127048 0.02768742 0.27620216]\n\n\nMarket shares do decrease for yogurt 1 by nearly 15% when price is increased by $0.10.\ntodo: describe the data a bit. How many respondents took the conjoint survey? How many choice tasks did each respondent complete? How many alternatives were presented on each choice task? For each alternative.\n\nconjoint = pd.read_csv('conjoint.csv')\n\nconjoint\n\n\n\n\n\n\n\n\nresp.id\nques\nalt\ncarpool\nseat\ncargo\neng\nprice\nchoice\n\n\n\n\n0\n1\n1\n1\nyes\n6\n2ft\ngas\n35\n0\n\n\n1\n1\n1\n2\nyes\n8\n3ft\nhyb\n30\n0\n\n\n2\n1\n1\n3\nyes\n6\n3ft\ngas\n30\n1\n\n\n3\n1\n2\n1\nyes\n6\n2ft\ngas\n30\n0\n\n\n4\n1\n2\n2\nyes\n7\n3ft\ngas\n35\n1\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n8995\n200\n14\n2\nno\n7\n3ft\ngas\n35\n1\n\n\n8996\n200\n14\n3\nno\n7\n3ft\nhyb\n35\n0\n\n\n8997\n200\n15\n1\nno\n7\n2ft\ngas\n35\n0\n\n\n8998\n200\n15\n2\nno\n8\n3ft\nelec\n40\n0\n\n\n8999\n200\n15\n3\nno\n6\n3ft\ngas\n35\n1\n\n\n\n\n9000 rows × 9 columns\n\n\n\n\n# number of unique respondents\n\nconjoint['resp.id'].nunique()\n\n200\n\n\n\n# number of choice tasks\n\nconjoint[conjoint['resp.id'] == 1]['ques'].nunique()\n\n15\n\n\n\n# number of alternatives\n\nconjoint['alt'].nunique()\n\n3\n\n\nThe data within the conjoint dataset is in long format, meaning it is organized in a way that each row represents a single observation. The data is organized by respondent, choice task, and alternative. There are 200 respondents who took the conjoint survey, and each respondent completed 15 choice tasks. Each choice task presented 3 alternatives to the respondent. Each alternative is represented by a set of attributes, such as brand, price, and feature.\ntodo: estimate a MNL model omitting the following levels to avoide multicollinearity (6 seats, 2ft cargo, and gas engine). Include price as a continuous variable. Show a table of coefficients and standard errors. You may use your own likelihood function from above, or you may use a function from a package/library to perform the estimation.\n\nconjoint = pd.get_dummies(conjoint, columns=['seat', 'cargo', 'eng'], drop_first=False)\n\n\nconjoint.drop(['seat_6', 'cargo_2ft', 'eng_gas'], axis=1, inplace=True)\n\n\nX = conjoint[['price', 'seat_7', 'seat_8', 'cargo_3ft', 'eng_hyb', 'eng_elec']]  # predictors\ny = conjoint['choice']  # response variable\n\n\n# Fit the multinomial logistic regression model\nmodel = LogisticRegression(multi_class='multinomial', solver='lbfgs', max_iter=1000)\nmodel.fit(X, y)\n\nmodel.coef_\n\narray([[-0.07952075, -0.26151586, -0.14596134,  0.21885267, -0.37897658,\n        -0.71594544]])\n\n\ntodo: Interpret the coefficients. Which features are more preferred?\nIn terms of price, a cheaper price is more preferred as it has a negative coefficient.\nFor seats, both 7 and 8 seats have negative coefficients, meaning that seat 6 is the most preferred.\nFor cargo space, 3 ft is a positive coefficient, meaning its the most preferred.\nFinally, for engine type, both electric and hybrid have negative coefficients, meaning that gas is the most preferred.\ntodo: Use the price coefficient as a dollar-per-util conversion factor. What is the dollar value of 3ft of cargo space as compared to 2ft of cargo space?\n\ncargo_3ft = model.coef_[0][3]\nprice = model.coef_[0][0]\n\ndollar_value = (cargo_3ft / abs(price)) * 1000\n\ndollar_value.round(2)\n\n2752.15\n\n\ntodo: assume the market consists of the following 6 minivans. Predict the market shares of each minivan in the market.\n\ndata = {\n    \"minivan\": ['A', 'B', 'C', 'D', 'E', 'F'],\n    \"seat\": [7, 6, 8, 7, 6, 7],\n    \"cargo\": [2, 2, 2, 3, 2, 2],\n    \"eng\": ['hyb', 'gas', 'gas', 'gas', 'elec', 'hyb'],\n    \"price\": [30, 30, 30, 40, 40, 35]\n}\n\nminivans = pd.DataFrame(data)\n\n\nminivans\n\n\n\n\n\n\n\n\nminivan\nseat\ncargo\neng\nprice\n\n\n\n\n0\nA\n7\n2\nhyb\n30\n\n\n1\nB\n6\n2\ngas\n30\n\n\n2\nC\n8\n2\ngas\n30\n\n\n3\nD\n7\n3\ngas\n40\n\n\n4\nE\n6\n2\nelec\n40\n\n\n5\nF\n7\n2\nhyb\n35\n\n\n\n\n\n\n\n\nminivans = pd.get_dummies(minivans, columns=['seat', 'cargo', 'eng'], drop_first=False)\n\n\nminivans.drop(['seat_6', 'cargo_2', 'eng_gas'], axis=1, inplace=True)\n\n\n# rename cargo_3 to cargo_3ft\n\nminivans.rename(columns={'cargo_3': 'cargo_3ft'}, inplace=True)\n\n\nx_minivan = minivans[['price', 'seat_7', 'seat_8', 'cargo_3ft', 'eng_hyb', 'eng_elec']]\n\npredictions = model.predict_proba(x_minivan)[:,1]\n\npredictions\n\narray([0.37160976, 0.68041243, 0.61390225, 0.28494289, 0.09392392,\n       0.21073102])\n\n\n\n# add predictions to the minivans dataframe\n\nminivans['predictions'] = predictions\n\n\n# calculate market share for each\n\nminivans['market_share'] = minivans['predictions']/minivans['predictions'].sum()\n\n\nminivans\n\n\n\n\n\n\n\n\nminivan\nprice\nseat_7\nseat_8\ncargo_3ft\neng_elec\neng_hyb\npredictions\nmarket_share\n\n\n\n\n0\nA\n30\nTrue\nFalse\nFalse\nFalse\nTrue\n0.371610\n0.164756\n\n\n1\nB\n30\nFalse\nFalse\nFalse\nFalse\nFalse\n0.680412\n0.301665\n\n\n2\nC\n30\nFalse\nTrue\nFalse\nFalse\nFalse\n0.613902\n0.272177\n\n\n3\nD\n40\nTrue\nFalse\nTrue\nFalse\nFalse\n0.284943\n0.126331\n\n\n4\nE\n40\nFalse\nFalse\nFalse\nTrue\nFalse\n0.093924\n0.041642\n\n\n5\nF\n35\nTrue\nFalse\nFalse\nFalse\nTrue\n0.210731\n0.093429"
  },
  {
    "objectID": "projects/HW4/hw4_questions.html",
    "href": "projects/HW4/hw4_questions.html",
    "title": "Key Drivers Analysis",
    "section": "",
    "text": "This post implements a few measure of variable importance, interpreted as a key drivers analysis, for certain aspects of a payment card on customer satisfaction with that payment card.\n\n1. Dataset Research\nBefore we go into any complex analysis, we first want to get familar with the data.\n\nimport pandas as pd\n\ndata = pd.read_csv('data_for_drivers_analysis.csv')\n\ndata.head()\n\n\n\n\n\n\n\n\nbrand\nid\nsatisfaction\ntrust\nbuild\ndiffers\neasy\nappealing\nrewarding\npopular\nservice\nimpact\n\n\n\n\n0\n1\n98\n3\n1\n0\n1\n1\n1\n0\n0\n1\n0\n\n\n1\n1\n179\n5\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n2\n1\n197\n3\n1\n0\n0\n1\n1\n1\n0\n1\n1\n\n\n3\n1\n317\n1\n0\n0\n0\n0\n1\n0\n1\n1\n1\n\n\n4\n1\n356\n4\n1\n1\n1\n1\n1\n1\n1\n1\n1\n\n\n\n\n\n\n\n\nprint(f'The number of rows in the dataset are {data.shape[0]}.')\nprint(f'The number of unique IDs in the dataset are {data.id.nunique()}.')\nprint(f'The number of unique brands in the dataset are {data.brand.nunique()}.')\n\nThe number of rows in the dataset are 2553.\nThe number of unique IDs in the dataset are 940.\nThe number of unique brands in the dataset are 10.\n\n\n\ndata.describe()\n\n\n\n\n\n\n\n\nbrand\nid\nsatisfaction\ntrust\nbuild\ndiffers\neasy\nappealing\nrewarding\npopular\nservice\nimpact\n\n\n\n\ncount\n2553.000000\n2553.000000\n2553.000000\n2553.000000\n2553.000000\n2553.000000\n2553.000000\n2553.000000\n2553.000000\n2553.000000\n2553.000000\n2553.000000\n\n\nmean\n4.857423\n8931.480611\n3.386604\n0.549550\n0.461810\n0.334508\n0.536232\n0.451234\n0.451234\n0.536232\n0.467293\n0.330983\n\n\nstd\n2.830096\n5114.287849\n1.172006\n0.497636\n0.498637\n0.471911\n0.498783\n0.497714\n0.497714\n0.498783\n0.499027\n0.470659\n\n\nmin\n1.000000\n88.000000\n1.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n\n\n25%\n3.000000\n4310.000000\n3.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n\n\n50%\n4.000000\n8924.000000\n4.000000\n1.000000\n0.000000\n0.000000\n1.000000\n0.000000\n0.000000\n1.000000\n0.000000\n0.000000\n\n\n75%\n6.000000\n13545.000000\n4.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n\n\nmax\n10.000000\n18088.000000\n5.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n\n\n\n\n\n\n\nAfter looking at our data, we can see that satisfaction is the dependent variable, which is scored based on a range of 1-5. There are numerous different categories which affect the satisfaction score. All those categories are binary based on what the customer thinks about that card brand.\nNow we’re ready to get into some deeper analysis\n\n\n2. Pearson Correlations\nThe analysis we’ll conduct is determining the Pearson Correlations for this dataset. That is, how each indepenent variable is correlated to satisfaction.\nWe’ll be using the corr function in python to determine our correlations in respect to satisfaction.\n\n# Calculate pearson correlations with satisfaction being the y variable and the rest being the x variables\n\nsatisfaction = data.drop(['id', 'brand'], axis=1)\n\ntable = satisfaction.corr()['satisfaction'].sort_values(ascending=False)\n\n# drop satisfaction from table\n\ntable = table.drop('satisfaction')\n\ntable = pd.DataFrame(table)\n\ntable = table.rename(columns={'satisfaction': 'Pearson_Corr'})\n\ntable['Pearson_Corr_%'] = round(table['Pearson_Corr']/table['Pearson_Corr'].sum(), 3)*100\n\ntable = table.drop('Pearson_Corr', axis = 1)\ntable\n\n\n\n\n\n\n\n\nPearson_Corr_%\n\n\n\n\ntrust\n13.3\n\n\nimpact\n13.2\n\n\nservice\n13.0\n\n\neasy\n11.1\n\n\nappealing\n10.8\n\n\nrewarding\n10.1\n\n\nbuild\n10.0\n\n\ndiffers\n9.6\n\n\npopular\n8.9\n\n\n\n\n\n\n\nFrom our calculations, we see that trust has the highest correlation to satisfaction, followed by impact and service. Popularity seems to be least correlated with satisfaction. This does not mean that trust is the most important variable when looking at satisfaction, however. It simply means it follows the most similar path (up or down) as satisfaction with respect to all the other variables.\n\n\n3. Polychoric Correlations\nPolychoric correlations are most notably used for ordinal variables, or variables that have a specific rank order, for instance military ranks (Captain, Colonel, General, etc).\nOur satisfaction variable is an ordinal variable which ranges from 1-5.\nFor this calculation, we used an R package within python. R has a much simpler package for conducting these calculations.\n\nimport rpy2.robjects as robjects\nfrom rpy2.robjects import pandas2ri\n\n# Activate the pandas2ri conversion\npandas2ri.activate()\n\n\n# Specify the columns of interest\ncolumns_of_interest = ['trust', 'build', 'differs', 'easy', 'appealing', 'rewarding', 'popular', 'service', 'impact']\n\n# Initialize a list to store the results\ncorrelation = []\n\n# Define the R code for calculating polychoric correlation\nr_code = \"\"\"\nlibrary(polycor)\npolychoric_corr &lt;- function(x, y) {\n    result &lt;- polychor(x, y)\n    return(result)\n}\n\"\"\"\n\n# Load the R code into the R environment\nrobjects.r(r_code)\n\n# Get the polychoric_corr function\npolychoric_corr = robjects.globalenv['polychoric_corr']\n\n# Calculate polychoric correlations between 'satisfaction' and each specified column\nfor col in columns_of_interest:\n    r_corr = polychoric_corr(data['satisfaction'], data[col])\n    correlation.append(r_corr[0])\n\n# Convert correlations to a pandas DataFrame\ncorrelation_df = pd.DataFrame({\n    'Variable': columns_of_interest,\n    'Polychoric_Corr': correlation\n})\n\n# Calculate the sum of the polychoric correlations\nsum_polychoric_correlations = correlation_df['Polychoric_Corr'].sum()\n\n# Calculate the percentage of each polychoric correlation\ncorrelation_df['Polychoric_Corr_%'] = (correlation_df['Polychoric_Corr'] / sum_polychoric_correlations).round(3)*100\n\n\n# make variable column the index\n\ncorrelation_df.set_index('Variable', inplace=True)\n\n# merge correlation_df with correlations\n\ntable = table.merge(correlation_df, left_index=True, right_index=True).drop('Polychoric_Corr', axis=1)\n\n\ntable\n\n\n\n\n\n\n\n\nPearson_Corr_%\nPolychoric_Corr_%\n\n\n\n\ntrust\n13.3\n12.9\n\n\nimpact\n13.2\n13.8\n\n\nservice\n13.0\n13.0\n\n\neasy\n11.1\n10.9\n\n\nappealing\n10.8\n10.6\n\n\nrewarding\n10.1\n10.1\n\n\nbuild\n10.0\n9.9\n\n\ndiffers\n9.6\n10.0\n\n\npopular\n8.9\n8.9\n\n\n\n\n\n\n\nAs we can see from the data, the groups are very similar to Pearson’s Correlations, however they are in slightly different orders. The top three correlated variables from Pearson’s (Trust, Impact, Service) are still the top 3, however the order is Impact, Service, and Trust.\nAlso, in our bottom 3 variables (Build, Differs, and Popular) from Pearson’s, Polychoric varies slightly with Differs, Build and Popular being the order from most to least. Overall, both Pearson’s and Polychoric correlations are very similar.\n\n\n4. Standardized Multiple Regression Coefficients\nStandardized regression coefficients are used for a different purpose than our last two methods. Pearson’s and Polychoric used correlations, which do not necessarily relate to importance of each variable on the dependent variable. Standardized regression coefficients measure importance for each variable in a regression analysis by scaling all the independent variables so they are now all on equal scales. The scaled indepenent variables are then fit to a regression model where their betas or standardized coefficients are generated.\nFirst, we’ll scale and fit our data to get our coefficients, then find their importance.\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LinearRegression\n\n# Initialize the StandardScaler\n\nscaler = StandardScaler()\n\n# Fit the scaler to the data\n\nscaler.fit(data.drop(['id', 'brand', 'satisfaction'], axis=1))\n\n# Transform the data\n\nscaled_data = scaler.transform(data.drop(['id', 'brand', 'satisfaction'], axis=1))\n\n# Convert the scaled data to a DataFrame\n\nscaled_data = pd.DataFrame(scaled_data, columns=data.drop(['id', 'brand', 'satisfaction'], axis=1).columns)\n\n# Add the 'satisfaction' column to the scaled data\n\nscaled_data['satisfaction'] = data['satisfaction']\n\n# Initialize the LinearRegression model\n\nmodel = LinearRegression()\n\n# Fit the model to the scaled data\n\nmodel.fit(scaled_data.drop('satisfaction', axis=1), scaled_data['satisfaction'])\n\ncoefficients = model.coef_\n\ntable['Std_Coefficients'] = coefficients\n\ntable['Std_Coef_%'] = (table['Std_Coefficients']/table['Std_Coefficients'].sum())*100\n\ntable = table.drop('Std_Coefficients', axis=1)\n\ntable\n\n\n\n\n\n\n\n\nPearson_Corr_%\nPolychoric_Corr_%\nStd_Coef_%\n\n\n\n\ntrust\n13.3\n12.9\n25.280109\n\n\nimpact\n13.2\n13.8\n4.363461\n\n\nservice\n13.0\n13.0\n6.081798\n\n\neasy\n11.1\n10.9\n4.798205\n\n\nappealing\n10.8\n10.6\n7.389479\n\n\nrewarding\n10.1\n10.1\n1.106526\n\n\nbuild\n10.0\n9.9\n3.628859\n\n\ndiffers\n9.6\n10.0\n19.304269\n\n\npopular\n8.9\n8.9\n28.047294\n\n\n\n\n\n\n\nThe ranked order for importance based on Standardized Regression Coefficients changes greatly when compared to our correlations. Popularity, which had the lowest correlational values for the first two calculations, now has the highest value. Trust is still at the top in second place, but Differs is now in the top 3 as well. Impact is also towards the bottom of the pack which was towards the top in correlations.\n\n\n5. Shapley Values for Linear Regression\nShapley Values, like the Standardized Regression Coefficients, also measure importance, but using a different method. They have been popularized in the use of machine learning, but here we will be using them with linear regression. Shapley values measure importance through the R^2 value, which measures how well the variance in the data is explained by the coefficients generated during a linear regression analysis.\nOnce the coefficients are attained through the regression analysis, they are then used to create as many unique combinations of dependent variables in our linear regression analysis. To go into this further, if our regression had 3 explanatory variables labeled a, b, and c, each one would have a coefficient. We would then measure the R2 value for every combination of variables to explain y.\nFor example, one combination would be y = beta-aa + beat-bb + beta-cc. Another combination would be y = beta-aa + beta-bb. Another one would be y = beta-aa. We would calculate the R2 values for both of these models. We would subtract the R2 value from of the equation with variable c from the R2 value from the equaion without c to get the difference. We would do this for every combination of variable, then average the differences in R2 for every variable which we got.\nLets get into the code for calculating Shapley values in python.\nFirst, we’ll make the linear regression model.\n\nfrom sklearn.ensemble import RandomForestRegressor\n\n# Load dataset\n\nX = data[columns_of_interest]\ny = data['satisfaction']\n\n# Train a model\nmodel = LinearRegression()\nmodel.fit(X, y)\n\nLinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LinearRegressionLinearRegression()\n\n\nNext, we’ll generate the Shapley Values using the SHAP package on python\n\nimport shap\n\n# Initialize SHAP explainer\nexplainer = shap.LinearExplainer(model, X)\n\n# Calculate Shapley values\nshap_values = explainer.shap_values(X)\n\n# Plot the summary\nshap.summary_plot(shap_values, X, plot_type=\"bar\")\n\n\n\n\n\n\n\n\nFinally, we’ll put our Shapley Values into our table.\n\nshap_values = pd.DataFrame(shap_values, columns=columns_of_interest)\n\n# mean of absolute values of each column\n\nshap_values_avg = shap_values.abs().mean()\n\nshap_values_avg = pd.DataFrame(shap_values_avg)\n\nshap_values_avg = shap_values_avg.rename(columns={0: 'Shapley_Value'})\n\ntable = table.merge(shap_values_avg, left_index=True, right_index=True)\n\ntable['Shapley_Value_%'] = round(table['Shapley_Value'] / table['Shapley_Value'].sum(), 3)*100\n\ntable = table.drop('Shapley_Value', axis=1)\n\ntable\n\n\n\n\n\n\n\n\nPearson_Corr_%\nPolychoric_Corr_%\nStd_Coef_%\nShapley_Value_%\n\n\n\n\ntrust\n13.3\n12.9\n25.280109\n26.7\n\n\nimpact\n13.2\n13.8\n4.363461\n25.5\n\n\nservice\n13.0\n13.0\n6.081798\n19.9\n\n\neasy\n11.1\n10.9\n4.798205\n5.1\n\n\nappealing\n10.8\n10.6\n7.389479\n7.6\n\n\nrewarding\n10.1\n10.1\n1.106526\n1.1\n\n\nbuild\n10.0\n9.9\n3.628859\n4.5\n\n\ndiffers\n9.6\n10.0\n19.304269\n5.6\n\n\npopular\n8.9\n8.9\n28.047294\n3.8\n\n\n\n\n\n\n\nThe Shapley Values produced a slightly different order than our standard coefficients. Trust, impact, and service are the top 3 most important features. The remaiining order is much different than previous calculations, with appealing, differs, and easy the following 3, and build, popular, and rewarding following them.\n\n\n6. Johnson’s Epsilon\nJohnson’s Epsilon, also known as relative weight analysis, is an approximation of the Shapley Values which uses a different calculation approach. It uses Eigenvectors and Eigenvalues to create a set of uncorrelated independent variables which can be used to calculate the partial effect of each independent variable.\nThe code is relatively simple in python using the relativeImp function as seen below.\n\nfrom relativeImp import relativeImp\n\n\ny = 'satisfaction'\nX = ['trust', 'build', 'differs', 'easy', 'appealing', 'rewarding', 'popular', 'service', 'impact']\n\n# Perform relative weights analysis\nrel_Imp = relativeImp(data, outcomeName=y, driverNames=X)\n\nrel_Imp.set_index('driver', inplace=True)\n\ntable = table.merge(rel_Imp, left_index=True, right_index=True)\n\ntable = table.drop('rawRelaImpt', axis=1)\n\ntable = table.rename(columns = {'normRelaImpt':'Johnson_Ep_%'})\n\ntable\n\n\n\n\n\n\n\n\nPearson_Corr_%\nPolychoric_Corr_%\nStd_Coef_%\nShapley_Value_%\nJohnson_Ep_%\n\n\n\n\ntrust\n13.3\n12.9\n25.280109\n26.7\n19.835524\n\n\nimpact\n13.2\n13.8\n4.363461\n25.5\n21.966601\n\n\nservice\n13.0\n13.0\n6.081798\n19.9\n16.635164\n\n\neasy\n11.1\n10.9\n4.798205\n5.1\n8.239683\n\n\nappealing\n10.8\n10.6\n7.389479\n7.6\n8.346395\n\n\nrewarding\n10.1\n10.1\n1.106526\n1.1\n5.997431\n\n\nbuild\n10.0\n9.9\n3.628859\n4.5\n6.620792\n\n\ndiffers\n9.6\n10.0\n19.304269\n5.6\n6.966081\n\n\npopular\n8.9\n8.9\n28.047294\n3.8\n5.392328\n\n\n\n\n\n\n\nFrom our results, we can see that ranked order is similar with the top, middle, and bottom 3 variables remaining the same, but values are different.\n\n\n7. Mean Decrease in RF Gini Coefficient\nThe mean decrease in Gini coefficient also measures importance, but for Random Forests and Decision trees. It does so by measuring the impurity before and after a split in the decision tree. If the split performs well, the decrease in impurity will be higher.\n\nfrom sklearn.ensemble import RandomForestClassifier\n\nmodel = RandomForestClassifier(n_estimators=100, random_state=42)\nmodel.fit(data[columns_of_interest], data['satisfaction'])\n\ngini_importance = model.feature_importances_\n\n# Create a DataFrame for better readability\nfeature_importance_df = pd.DataFrame({\n    'Feature': columns_of_interest,\n    'Gini Importance': gini_importance\n})\n\n# Sort the features by importance\nfeature_importance_df = feature_importance_df.sort_values(by='Gini Importance', ascending=False)\n\nfeature_importance_df.set_index('Feature', inplace = True)\n\nfeature_importance_df['Gini Importance'] = feature_importance_df['Gini Importance']*100\n\ntable = table.merge(feature_importance_df, left_index=True, right_index=True)\n\ntable\n\n\n\n\n\n\n\n\nPearson_Corr_%\nPolychoric_Corr_%\nStd_Coef_%\nShapley_Value_%\nJohnson_Ep_%\nGini Importance\n\n\n\n\ntrust\n13.3\n12.9\n25.280109\n26.7\n19.835524\n8.982532\n\n\nimpact\n13.2\n13.8\n4.363461\n25.5\n21.966601\n9.270374\n\n\nservice\n13.0\n13.0\n6.081798\n19.9\n16.635164\n10.588220\n\n\neasy\n11.1\n10.9\n4.798205\n5.1\n8.239683\n11.249596\n\n\nappealing\n10.8\n10.6\n7.389479\n7.6\n8.346395\n10.699951\n\n\nrewarding\n10.1\n10.1\n1.106526\n1.1\n5.997431\n11.851860\n\n\nbuild\n10.0\n9.9\n3.628859\n4.5\n6.620792\n12.371855\n\n\ndiffers\n9.6\n10.0\n19.304269\n5.6\n6.966081\n11.466956\n\n\npopular\n8.9\n8.9\n28.047294\n3.8\n5.392328\n13.518656\n\n\n\n\n\n\n\nThe Gini Coefficient showed much different results than any of our other previous calculations. Trust and impact are now at the bottom of the pack, while popularity, build, and rewarding are at the top.\nFrom all the above calculations, we can see that there are numerous ways to measure correlation, usefulness, and importance for dependent variables in a model, dependent the type of model we are looking at. They can all be useful and worthwhile depending on the specific models and information which the customer and/or data analyst ar seeking."
  },
  {
    "objectID": "projects/HW4/HW4.html",
    "href": "projects/HW4/HW4.html",
    "title": "Pearson Correlations",
    "section": "",
    "text": "This post implements a few measure of variable importance, interpreted as a key drivers analysis, for certain aspects of a payment card on customer satisfaction with that payment card.\ntodo: replicate the table on slide 19 of the session 4 slides. This involves calculating pearson correlations, standardized regression coefficients, “usefulness”, Shapley values for a linear regression, Johnson’s relative weights, and the mean decrease in the gini coefficient from a random forest. You may use packages built into R or Python.\nIf you want a challenge, either (1) implement one or more of the measures yourself. “Usefulness” is rather easy to program up. Shapley values for linear regression are a bit more work. Or (2) add additional measures to the table such as the importance scores from XGBoost.\n\nimport pandas as pd\n\ndata = pd.read_csv('data_for_drivers_analysis.csv')\n\ndata.head()\n\n\n\n\n\n\n\n\nbrand\nid\nsatisfaction\ntrust\nbuild\ndiffers\neasy\nappealing\nrewarding\npopular\nservice\nimpact\n\n\n\n\n0\n1\n98\n3\n1\n0\n1\n1\n1\n0\n0\n1\n0\n\n\n1\n1\n179\n5\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n2\n1\n197\n3\n1\n0\n0\n1\n1\n1\n0\n1\n1\n\n\n3\n1\n317\n1\n0\n0\n0\n0\n1\n0\n1\n1\n1\n\n\n4\n1\n356\n4\n1\n1\n1\n1\n1\n1\n1\n1\n1\n\n\n\n\n\n\n\n\ndata.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 2553 entries, 0 to 2552\nData columns (total 12 columns):\n #   Column        Non-Null Count  Dtype\n---  ------        --------------  -----\n 0   brand         2553 non-null   int64\n 1   id            2553 non-null   int64\n 2   satisfaction  2553 non-null   int64\n 3   trust         2553 non-null   int64\n 4   build         2553 non-null   int64\n 5   differs       2553 non-null   int64\n 6   easy          2553 non-null   int64\n 7   appealing     2553 non-null   int64\n 8   rewarding     2553 non-null   int64\n 9   popular       2553 non-null   int64\n 10  service       2553 non-null   int64\n 11  impact        2553 non-null   int64\ndtypes: int64(12)\nmemory usage: 239.5 KB\n\n\n\ndata.describe()\n\n\n\n\n\n\n\n\nbrand\nid\nsatisfaction\ntrust\nbuild\ndiffers\neasy\nappealing\nrewarding\npopular\nservice\nimpact\n\n\n\n\ncount\n2553.000000\n2553.000000\n2553.000000\n2553.000000\n2553.000000\n2553.000000\n2553.000000\n2553.000000\n2553.000000\n2553.000000\n2553.000000\n2553.000000\n\n\nmean\n4.857423\n8931.480611\n3.386604\n0.549550\n0.461810\n0.334508\n0.536232\n0.451234\n0.451234\n0.536232\n0.467293\n0.330983\n\n\nstd\n2.830096\n5114.287849\n1.172006\n0.497636\n0.498637\n0.471911\n0.498783\n0.497714\n0.497714\n0.498783\n0.499027\n0.470659\n\n\nmin\n1.000000\n88.000000\n1.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n\n\n25%\n3.000000\n4310.000000\n3.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n\n\n50%\n4.000000\n8924.000000\n4.000000\n1.000000\n0.000000\n0.000000\n1.000000\n0.000000\n0.000000\n1.000000\n0.000000\n0.000000\n\n\n75%\n6.000000\n13545.000000\n4.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n\n\nmax\n10.000000\n18088.000000\n5.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n\n\n\n\n\n\n\n\ndata.brand.unique()\n\narray([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10])\n\n\n\ndata.id.nunique()\n\n940\n\n\n\ndata.shape\n\n(2553, 12)\n\n\n\n# Calculate pearson correlations with satisfaction being the y variable and the rest being the x variables\n\nsatisfaction = data.drop(['id', 'brand'], axis=1)\n\ncorrelations = satisfaction.corr()['satisfaction'].sort_values(ascending=False)\n\n# drop satisfaction from correlations\n\ncorrelations = correlations.drop('satisfaction')\n\ncorrelations = pd.DataFrame(correlations)\n\n\n\n\ncorrelations = correlations.rename(columns={'satisfaction': 'Pearson_Corr'})\n\n\ncorrelations['Pearson_Corr_%'] = round(correlations['Pearson_Corr']/correlations['Pearson_Corr'].sum(), 3)\n\n# rename satisfaction column to Pearson_Corr\n\n# drop PEARSON_CORR from correlations\n\ncorrelations = correlations.drop('Pearson_Corr', axis=1)\n\n\n\n\ndata.columns\n\nIndex(['brand', 'id', 'satisfaction', 'trust', 'build', 'differs', 'easy',\n       'appealing', 'rewarding', 'popular', 'service', 'impact'],\n      dtype='object')\n\n\n\nPolychoric Correlations\n\nimport pandas as pd\nimport rpy2.robjects as robjects\nfrom rpy2.robjects import pandas2ri\n\n# Activate the pandas2ri conversion\npandas2ri.activate()\n\n\n# Specify the columns of interest\ncolumns_of_interest = ['trust', 'build', 'differs', 'easy', 'appealing', 'rewarding', 'popular', 'service', 'impact']\n\n# Initialize a list to store the results\ncorrelation = []\n\n# Define the R code for calculating polychoric correlation\nr_code = \"\"\"\nlibrary(polycor)\npolychoric_corr &lt;- function(x, y) {\n    result &lt;- polychor(x, y)\n    return(result)\n}\n\"\"\"\n\n# Load the R code into the R environment\nrobjects.r(r_code)\n\n# Get the polychoric_corr function\npolychoric_corr = robjects.globalenv['polychoric_corr']\n\n# Calculate polychoric correlations between 'satisfaction' and each specified column\nfor col in columns_of_interest:\n    r_corr = polychoric_corr(data['satisfaction'], data[col])\n    correlation.append(r_corr[0])\n\n# Convert correlations to a pandas DataFrame\ncorrelation_df = pd.DataFrame({\n    'Variable': columns_of_interest,\n    'Polychoric_Correlation': correlation\n})\n\n# Calculate the sum of the polychoric correlations\nsum_polychoric_correlations = correlation_df['Polychoric_Correlation'].sum()\n\n# Calculate the percentage of each polychoric correlation\ncorrelation_df['Polychoric_Corr_Percent'] = (correlation_df['Polychoric_Correlation'] / sum_polychoric_correlations).round(3)\n\n\n# make variable column the index\n\ncorrelation_df.set_index('Variable', inplace=True)\n\n\n# merge correlation_df with correlations\n\ncorrelations = correlations.merge(correlation_df, left_index=True, right_index=True)\n\ncorrelations\n\n\n\n\n\n\n\n\nPearson_Corr_%\nPolychoric_Correlation\nPolychoric_Corr_Percent\n\n\n\n\ntrust\n0.133\n0.325066\n0.129\n\n\nimpact\n0.132\n0.348443\n0.138\n\n\nservice\n0.130\n0.327699\n0.130\n\n\neasy\n0.111\n0.273529\n0.109\n\n\nappealing\n0.108\n0.266616\n0.106\n\n\nrewarding\n0.101\n0.255609\n0.101\n\n\nbuild\n0.100\n0.249185\n0.099\n\n\ndiffers\n0.096\n0.251008\n0.100\n\n\npopular\n0.089\n0.223711\n0.089\n\n\n\n\n\n\n\n\n\nStandardized Multiple Regression Coefficients\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LinearRegression\n\n# Initialize the StandardScaler\n\nscaler = StandardScaler()\n\n# Fit the scaler to the data\n\nscaler.fit(data.drop(['id', 'brand', 'satisfaction'], axis=1))\n\n# Transform the data\n\nscaled_data = scaler.transform(data.drop(['id', 'brand', 'satisfaction'], axis=1))\n\n# Convert the scaled data to a DataFrame\n\nscaled_data = pd.DataFrame(scaled_data, columns=data.drop(['id', 'brand', 'satisfaction'], axis=1).columns)\n\n# Add the 'satisfaction' column to the scaled data\n\nscaled_data['satisfaction'] = data['satisfaction']\n\n# Initialize the LinearRegression model\n\nmodel = LinearRegression()\n\n# Fit the model to the scaled data\n\nmodel.fit(scaled_data.drop('satisfaction', axis=1), scaled_data['satisfaction'])\n\nLinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LinearRegressionLinearRegression()\n\n\n\ncoefficients = model.coef_\n\n\ncorrelations['Std_Coefficients'] = coefficients\n\n\nscaled_data.head()\n\n\n\n\n\n\n\n\ntrust\nbuild\ndiffers\neasy\nappealing\nrewarding\npopular\nservice\nimpact\nsatisfaction\n\n\n\n\n0\n0.905357\n-0.926325\n1.410483\n0.929981\n1.102790\n-0.906791\n-1.075291\n1.067700\n-0.703371\n3\n\n\n1\n-1.104536\n-0.926325\n-0.708977\n-1.075291\n-0.906791\n-0.906791\n-1.075291\n-0.936593\n-0.703371\n5\n\n\n2\n0.905357\n-0.926325\n-0.708977\n0.929981\n1.102790\n1.102790\n-1.075291\n1.067700\n1.421725\n3\n\n\n3\n-1.104536\n-0.926325\n-0.708977\n-1.075291\n1.102790\n-0.906791\n0.929981\n1.067700\n1.421725\n1\n\n\n4\n0.905357\n1.079534\n1.410483\n0.929981\n1.102790\n1.102790\n0.929981\n1.067700\n1.421725\n4\n\n\n\n\n\n\n\n\n# get standard deviation of each column in scaled_data\n\nstd_dev = scaled_data.std()\n\nstd_dev = pd.DataFrame(std_dev)\n\nstd_dev\n\n\n\n\n\n\n\n\n0\n\n\n\n\ntrust\n1.000196\n\n\nbuild\n1.000196\n\n\ndiffers\n1.000196\n\n\neasy\n1.000196\n\n\nappealing\n1.000196\n\n\nrewarding\n1.000196\n\n\npopular\n1.000196\n\n\nservice\n1.000196\n\n\nimpact\n1.000196\n\n\nsatisfaction\n1.172006\n\n\n\n\n\n\n\n\nsat = std_dev.iloc[-1,-1]\n\nsat\n\n1.1720059326669825\n\n\n\n# drop satisfaction from std_dev\n\nstd_dev = std_dev.drop('satisfaction')\n\n\n# add coefficients to std_dev DataFrame\n\n\n\nstd_dev = std_dev.rename(columns={0: 'Standard_Deviation'})\n\nstd_dev['Coefficient'] = coefficients\n\nstd_dev\n\n\n\n\n\n\n\n\nStandard_Deviation\nCoefficient\n\n\n\n\ntrust\n1.000196\n0.135635\n\n\nbuild\n1.000196\n0.023411\n\n\ndiffers\n1.000196\n0.032631\n\n\neasy\n1.000196\n0.025744\n\n\nappealing\n1.000196\n0.039647\n\n\nrewarding\n1.000196\n0.005937\n\n\npopular\n1.000196\n0.019470\n\n\nservice\n1.000196\n0.103573\n\n\nimpact\n1.000196\n0.150482\n\n\n\n\n\n\n\n\nstd_dev['Std_Coef'] = std_dev['Coefficient'] * std_dev['Standard_Deviation'] / sat\n\nstd_dev['Std_Coef_%'] = round(std_dev['Std_Coef'] / std_dev['Std_Coef'].sum(), 3)\n\nstd_dev\n\n\n\n\n\n\n\n\nStandard_Deviation\nCoefficient\nStd_Coef\nStd_Coef_%\n\n\n\n\ntrust\n1.000196\n0.135635\n0.115752\n0.253\n\n\nbuild\n1.000196\n0.023411\n0.019979\n0.044\n\n\ndiffers\n1.000196\n0.032631\n0.027847\n0.061\n\n\neasy\n1.000196\n0.025744\n0.021970\n0.048\n\n\nappealing\n1.000196\n0.039647\n0.033835\n0.074\n\n\nrewarding\n1.000196\n0.005937\n0.005067\n0.011\n\n\npopular\n1.000196\n0.019470\n0.016616\n0.036\n\n\nservice\n1.000196\n0.103573\n0.088390\n0.193\n\n\nimpact\n1.000196\n0.150482\n0.128422\n0.280\n\n\n\n\n\n\n\n\n# merge std_dev with correlations\n\ncorrelations = correlations.merge(std_dev, left_index=True, right_index=True)\n\ncorrelations\n\n\n\n\n\n\n\n\nPearson_Corr_%\nPolychoric_Correlation\nPolychoric_Corr_Percent\nStd_Coefficients\nStandard_Deviation\nCoefficient\nStd_Coef\nStd_Coef_%\n\n\n\n\ntrust\n0.133\n0.325066\n0.129\n0.135635\n1.000196\n0.135635\n0.115752\n0.253\n\n\nimpact\n0.132\n0.348443\n0.138\n0.023411\n1.000196\n0.150482\n0.128422\n0.280\n\n\nservice\n0.130\n0.327699\n0.130\n0.032631\n1.000196\n0.103573\n0.088390\n0.193\n\n\neasy\n0.111\n0.273529\n0.109\n0.025744\n1.000196\n0.025744\n0.021970\n0.048\n\n\nappealing\n0.108\n0.266616\n0.106\n0.039647\n1.000196\n0.039647\n0.033835\n0.074\n\n\nrewarding\n0.101\n0.255609\n0.101\n0.005937\n1.000196\n0.005937\n0.005067\n0.011\n\n\nbuild\n0.100\n0.249185\n0.099\n0.019470\n1.000196\n0.023411\n0.019979\n0.044\n\n\ndiffers\n0.096\n0.251008\n0.100\n0.103573\n1.000196\n0.032631\n0.027847\n0.061\n\n\npopular\n0.089\n0.223711\n0.089\n0.150482\n1.000196\n0.019470\n0.016616\n0.036\n\n\n\n\n\n\n\n\n\nShapley Values for Linear Regression\n\ndata.columns\n\nIndex(['brand', 'id', 'satisfaction', 'trust', 'build', 'differs', 'easy',\n       'appealing', 'rewarding', 'popular', 'service', 'impact'],\n      dtype='object')\n\n\n\ncolumns_of_interest\n\n['trust',\n 'build',\n 'differs',\n 'easy',\n 'appealing',\n 'rewarding',\n 'popular',\n 'service',\n 'impact']\n\n\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestRegressor\n\n# Load dataset\n\nX = data[['trust', 'build', 'differs', 'easy',\n       'appealing', 'rewarding', 'popular', 'service', 'impact']]\ny = data['satisfaction']\n\n# Train a model\nmodel = LinearRegression()\nmodel.fit(X, y)\n\nLinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LinearRegressionLinearRegression()\n\n\n\nimport shap\n\n# Initialize SHAP explainer\nexplainer = shap.LinearExplainer(model, X, feature_perturbation=\"interventional\")\n\n# Calculate Shapley values\nshap_values = explainer.shap_values(X)\n\n# Plot the summary\nshap.summary_plot(shap_values, X, plot_type=\"bar\")\n\n\n\nThe feature_perturbation option is now deprecated in favor of using the appropriate masker (maskers.Independent, or maskers.Impute)\n\n\n\n\n\n\n\n\n\n\nshap_values = pd.DataFrame(shap_values, columns=columns_of_interest)\n\n# mean of absolute values of each column\n\nshap_values_avg = shap_values.abs().mean()\n\nshap_values_avg = pd.DataFrame(shap_values_avg)\n\nshap_values_avg = shap_values_avg.rename(columns={0: 'Shapley_Value'})\n\n\ncorrelations = correlations.merge(shap_values_avg, left_index=True, right_index=True)\n\n\ncorrelations['Shapley_Value_%'] = round(correlations['Shapley_Value'] / correlations['Shapley_Value'].sum(), 3)\n\ncorrelations\n\n\n\n\n\n\n\n\nPearson_Corr_%\nPolychoric_Correlation\nPolychoric_Corr_Percent\nStd_Coefficients\nStandard_Deviation\nCoefficient\nStd_Coef\nStd_Coef_%\nShapley_Value\nShapley_Value_%\n\n\n\n\ntrust\n0.133\n0.325066\n0.129\n0.135635\n1.000196\n0.135635\n0.115752\n0.253\n0.136576\n0.267\n\n\nimpact\n0.132\n0.348443\n0.138\n0.023411\n1.000196\n0.150482\n0.128422\n0.280\n0.130708\n0.255\n\n\nservice\n0.130\n0.327699\n0.130\n0.032631\n1.000196\n0.103573\n0.088390\n0.193\n0.102030\n0.199\n\n\neasy\n0.111\n0.273529\n0.109\n0.025744\n1.000196\n0.025744\n0.021970\n0.048\n0.025924\n0.051\n\n\nappealing\n0.108\n0.266616\n0.106\n0.039647\n1.000196\n0.039647\n0.033835\n0.074\n0.039060\n0.076\n\n\nrewarding\n0.101\n0.255609\n0.101\n0.005937\n1.000196\n0.005937\n0.005067\n0.011\n0.005861\n0.011\n\n\nbuild\n0.100\n0.249185\n0.099\n0.019470\n1.000196\n0.023411\n0.019979\n0.044\n0.023157\n0.045\n\n\ndiffers\n0.096\n0.251008\n0.100\n0.103573\n1.000196\n0.032631\n0.027847\n0.061\n0.028857\n0.056\n\n\npopular\n0.089\n0.223711\n0.089\n0.150482\n1.000196\n0.019470\n0.016616\n0.036\n0.019465\n0.038\n\n\n\n\n\n\n\n\n\nJohnson’s Epilon\n\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler\nimport numpy as np\nfrom scipy.stats import johnsonsu\nfrom sklearn.linear_model import LinearRegression\n\n\n# Specify the predictor variables and the target variable\npredictors = ['trust', 'build', 'differs', 'easy', 'appealing', 'rewarding', 'popular', 'service', 'impact']\ntarget = 'satisfaction'\n\n# Fit Johnson SU distribution to each predictor\njohnson_params = {}\nfor col in predictors:\n    params = johnsonsu.fit(data[col])\n    johnson_params[col] = params\n\n# Transform predictors using the fitted Johnson SU parameters\nX_transformed = pd.DataFrame()\nfor col in predictors:\n    gamma, delta, xi, lambda_ = johnson_params[col]\n    transformed = johnsonsu.cdf(data[col], gamma, delta, xi, lambda_)\n    X_transformed[col] = transformed\n\n# Fit the linear regression model\nmodel = LinearRegression()\nmodel.fit(X_transformed, data[target])\n\n# Display the model coefficients\njohnson_coef = pd.DataFrame({\n    'Predictor': predictors,\n    'Johnson_Epsilon': model.coef_\n})\n\n\n\njohnson_coef = pd.DataFrame(johnson_coef)\n\njohnson_coef.set_index('Predictor', inplace=True)\n\n\njohnson_coef['Johnson_Epsilon_%'] = round(johnson_coef['Johnson_Epsilon'] / johnson_coef['Johnson_Epsilon'].sum(), 3)\n\ncorrelations = correlations.merge(johnson_coef, left_index=True, right_index=True)\n\n\ncorrelations\n\n\n\n\n\n\n\n\nPearson_Corr\nPearson_Corr_%\nPolychoric_Correlation\nPolychoric_Corr_Percent\nStandard_Deviation\nCoefficient\nStd_Coef\nStd_Coef_%\nShapley_Value\nShapley_Value_%\nJohnson_Epsilon\nJohnson_Epsilon_%\n\n\n\n\ntrust\n0.255706\n0.133\n0.325066\n0.129\n1.000196\n0.135635\n0.115752\n0.253\n0.126132\n0.168\n0.381407\n0.253\n\n\nimpact\n0.254539\n0.132\n0.348443\n0.138\n1.000196\n0.150482\n0.128422\n0.280\n0.162314\n0.217\n0.403438\n0.268\n\n\nservice\n0.251098\n0.130\n0.327699\n0.130\n1.000196\n0.103573\n0.088390\n0.193\n0.121793\n0.163\n0.399815\n0.265\n\n\neasy\n0.212985\n0.111\n0.273529\n0.109\n1.000196\n0.025744\n0.021970\n0.048\n0.050894\n0.068\n0.052695\n0.035\n\n\nappealing\n0.207997\n0.108\n0.266616\n0.106\n1.000196\n0.039647\n0.033835\n0.074\n0.068315\n0.091\n0.095863\n0.064\n\n\nrewarding\n0.194561\n0.101\n0.255609\n0.101\n1.000196\n0.005937\n0.005067\n0.011\n0.047909\n0.064\n0.012569\n0.008\n\n\nbuild\n0.191896\n0.100\n0.249185\n0.099\n1.000196\n0.023411\n0.019979\n0.044\n0.064744\n0.086\n0.049673\n0.033\n\n\ndiffers\n0.184801\n0.096\n0.251008\n0.100\n1.000196\n0.032631\n0.027847\n0.061\n0.055374\n0.074\n0.070583\n0.047\n\n\npopular\n0.171425\n0.089\n0.223711\n0.089\n1.000196\n0.019470\n0.016616\n0.036\n0.051395\n0.069\n0.039853\n0.026\n\n\n\n\n\n\n\n\n\nMean Decrease in RF Gini Coefficient\n\nfrom sklearn.ensemble import RandomForestClassifier\n\nmodel = RandomForestClassifier(n_estimators=100, random_state=42)\nmodel.fit(data[columns_of_interest], data['satisfaction'])\n\ngini_importance = model.feature_importances_\n\n# Create a DataFrame for better readability\nfeature_importance_df = pd.DataFrame({\n    'Feature': columns_of_interest,\n    'Gini Importance': gini_importance\n})\n\n# Sort the features by importance\nfeature_importance_df = feature_importance_df.sort_values(by='Gini Importance', ascending=False)\n\n# Display the feature importances\nprint(feature_importance_df)\n\n     Feature  Gini Importance\n6    popular         0.135187\n1      build         0.123719\n5  rewarding         0.118519\n2    differs         0.114670\n3       easy         0.112496\n4  appealing         0.107000\n7    service         0.105882\n8     impact         0.092704\n0      trust         0.089825"
  }
]