---
title: "Poissonnew2"
author: "Nicholas Shuckerow"
date: today
callout-appearance: minimal # this hides the blue "i" icon on .callout-notes
editor_options: 
  chunk_output_type: console
---


This assignment uses uses the MNL model to analyze (1) yogurt purchase data made by consumers at a retail location, and (2) conjoint data about consumer preferences for minivans.


## 1. Estimating Yogurt Preferences

### Likelihood for the Multi-nomial Logit (MNL) Model

Suppose we have $i=1,\ldots,n$ consumers who each select exactly one product $j$ from a set of $J$ products. The outcome variable is the identity of the product chosen $y_i \in \{1, \ldots, J\}$ or equivalently a vector of $J-1$ zeros and $1$ one, where the $1$ indicates the selected product. For example, if the third product was chosen out of 4 products, then either $y=3$ or $y=(0,0,1,0)$ depending on how we want to represent it. Suppose also that we have a vector of data on each product $x_j$ (eg, size, price, etc.). 

We model the consumer's decision as the selection of the product that provides the most utility, and we'll specify the utility function as a linear function of the product characteristics:

$$ U_{ij} = x_j'\beta + \epsilon_{ij} $$

where $\epsilon_{ij}$ is an i.i.d. extreme value error term. 

The choice of the i.i.d. extreme value error term leads to a closed-form expression for the probability that consumer $i$ chooses product $j$:

$$ \mathbb{P}_i(j) = \frac{e^{x_j'\beta}}{\sum_{k=1}^Je^{x_k'\beta}} $$

For example, if there are 4 products, the probability that consumer $i$ chooses product 3 is:

$$ \mathbb{P}_i(3) = \frac{e^{x_3'\beta}}{e^{x_1'\beta} + e^{x_2'\beta} + e^{x_3'\beta} + e^{x_4'\beta}} $$

A clever way to write the individual likelihood function for consumer $i$ is the product of the $J$ probabilities, each raised to the power of an indicator variable ($\delta_{ij}$) that indicates the chosen product:

$$ L_i(\beta) = \prod_{j=1}^J \mathbb{P}_i(j)^{\delta_{ij}} = \mathbb{P}_i(1)^{\delta_{i1}} \times \ldots \times \mathbb{P}_i(J)^{\delta_{iJ}}$$

Notice that if the consumer selected product $j=3$, then $\delta_{i3}=1$ while $\delta_{i1}=\delta_{i2}=\delta_{i4}=0$ and the likelihood is:

$$ L_i(\beta) = \mathbb{P}_i(1)^0 \times \mathbb{P}_i(2)^0 \times \mathbb{P}_i(3)^1 \times \mathbb{P}_i(4)^0 = \mathbb{P}_i(3) = \frac{e^{x_3'\beta}}{\sum_{k=1}^Je^{x_k'\beta}} $$

The joint likelihood (across all consumers) is the product of the $n$ individual likelihoods:

$$ L_n(\beta) = \prod_{i=1}^n L_i(\beta) = \prod_{i=1}^n \prod_{j=1}^J \mathbb{P}_i(j)^{\delta_{ij}} $$

And the joint log-likelihood function is:

$$ \ell_n(\beta) = \sum_{i=1}^n \sum_{j=1}^J \delta_{ij} \log(\mathbb{P}_i(j)) $$


### Yogurt Dataset

We will use the `yogurt_data` dataset, which provides anonymized consumer identifiers (`id`), a vector indicating the chosen product (`y1`:`y4`), a vector indicating if any products were "featured" in the store as a form of advertising (`f1`:`f4`), and the products' prices (`p1`:`p4`). For example, consumer 1 purchased yogurt 4 at a price of 0.079/oz and none of the yogurts were featured/advertised at the time of consumer 1's purchase.  Consumers 2 through 7 each bought yogurt 2, etc.

First, we will import the dataset and take a look at it in a dataframe format.

```{python}
import pandas as pd

yogurt = pd.read_csv('yogurt_data.csv')

yogurt.head()

```

Next, we'll get more info on the data to describe it.

```{python}
yogurt.shape
```

```{python}
yogurt.info()
```

```{python}
yogurt.describe()
```

After conducting some research on the dataset, it is shown that the rate at which products y1 through y4 are selected are between 2% and 40%, with 2% being extremely low compared to the other products (23%, 34%). Each product also has a large standard deviation of being selected, likely because the product is either selected (1) or not selected (0). Also, the averages of each product being selected sum to 1, meaning there is no missing data for the y columns in the dataset.

The products were featured approximately same proportion of time (between 3.7%-5.6%), with product 1 being featured more than others at 5.6%, but it was not the most selected product. That product is y2. The summed average of featured products does not sum to 1, meaning a product wasn't always featured. The standard deviation are also very large, likely for the same reason as mentioned above (binary). They are slightly higher SD's proportionally to the averages, which is likely because there are much more 0's than 1's in columns due to the fact that there doesn't have to be a product featured. 

Average prices per ounce range from approximately $0.11 to $0.05. These standard deviations are not nearly as proportionally high as the other columns, likely because the prices are continuous and not binary. Price seems to potentially follow a normal distribution.

Let the vector of product features include brand dummy variables for yogurts 1-3 (we'll omit a dummy for product 4 to avoid multi-collinearity), a dummy variable to indicate if a yogurt was featured, and a continuous variable for the yogurts' prices:  

$$ x_j' = [\mathbbm{1}(\text{Yogurt 1}), \mathbbm{1}(\text{Yogurt 2}), \mathbbm{1}(\text{Yogurt 3}), X_f, X_p] $$

The "hard part" of the MNL likelihood function is organizing the data, as we need to keep track of 3 dimensions (consumer $i$, covariate $k$, and product $j$) instead of the typical 2 dimensions for cross-sectional regression models (consumer $i$ and covariate $k$). 

What we would like to do is reorganize the data from a "wide" shape with $n$ rows and multiple columns for each covariate, to a "long" shape with $n \times J$ rows and a single column for each covariate.  As part of this re-organization, we'll add binary variables to indicate the first 3 products; the variables for featured and price are included in the dataset and simply need to be "pivoted" or "melted" from wide to long.  

```{python}
# Melting data to be in long format
long_data = yogurt.melt(id_vars='id', 
                             value_vars=['y1', 'y2', 'y3', 'y4', 'f1', 'f2', 'f3', 'f4', 'p1', 'p2', 'p3', 'p4'],
                             var_name='variable', 
                             value_name='value')

# Adding separate columns for type and product number
long_data['type'] = long_data['variable'].str[0]
long_data['product_num'] = long_data['variable'].str[1].astype(int)

# Pivoting to have separate columns for each data type (purchase, featured, price)
long_data = long_data.pivot_table(index=['id', 'product_num'], columns='type', values='value', aggfunc='first').reset_index()

# Renaming the columns
long_data.columns = ['id', 'product_num', 'featured', 'price', 'purchase']

# Making product number an integer
long_data['yogurt_1'] = (long_data['product_num'] == 1).astype(int)
long_data['yogurt_2'] = (long_data['product_num'] == 2).astype(int)
long_data['yogurt_3'] = (long_data['product_num'] == 3).astype(int)

```

### Estimation

Below, I've created a custom function to calculate the log likelihood of a logistic regression. It returns the negative log likelihood in order to minimze the negative (same as maxmimizing if positive). Restrictions with python functions only allow us to minimize. 

```{python}
from scipy.optimize import minimize
from sklearn.preprocessing import StandardScaler
import numpy as np

# Define the logistic regression likelihood function
def logistic_neg_log_likelihood(beta, X, y):
    # Linear combination: X * beta
    z = np.dot(X, beta)
    # Logistic function application
    probability = 1 / (1 + np.exp(-z))
    # Log-likelihood
    log_likelihood = np.sum(y * np.log(probability) + (1 - y) * np.log(1 - probability))
    # Return negative log-likelihood
    return -log_likelihood
```

Now we will optimize our function to find the maximum likelihood estimators for the betas or coefficients of each explanatory variable. First, the variables are scaled, then we add a constant variable and a array of zeros for initial beta estimates.

```{python}
# Standardize features
scaler = StandardScaler()
features_scaled = scaler.fit_transform(long_data[['featured', 'price', 'yogurt_1', 'yogurt_2', 'yogurt_3']])

# Update the feature matrix X to include the scaled features and an intercept
X_logistic = np.hstack((np.ones((long_data.shape[0], 1)), features_scaled))  # Add intercept
y_logistic = long_data['purchase'].astype(int).values  # Binary purchase outcomes

# Initial beta estimates
initial_beta_logistic = np.zeros(X_logistic.shape[1])

# Running the optimization for logistic regression
result_logistic = minimize(logistic_neg_log_likelihood, initial_beta_logistic, args=(X_logistic, y_logistic), method='BFGS')
result_logistic
```

```{python}
coefficients = result_logistic.x

coefficients
```

### Discussion

We learn...

The most preferred yogurt is Yogurt #1, which has a coefficient or beta of 0.614. The next highest is Yogurt #2, which has a coefficient of 0.390. Yogurt #4 which is not featured is the base case, so it has a coefficient of 0. The least preferred yogurt is Yogurt #3, which has a coefficient of -1.360.

For an example of how much more profitable one yogurt is over the other based on our model, we'll calculate the dollar benefit of yogurt 1 (most preferred) over yogurt 3 (least preferred). We'll do this by finding the difference in betas between the two, then dividing by the price coefficient.

```{python}
dollar_benefit = (coefficients[3] - coefficients[5]) /abs(coefficients[2])

dollar_benefit
```

The dollar benefit of yogurt 1 over yogurt 3 is about $2.72 based on our model.

One benefit of the MNL model is that we can simulate counterfactuals (eg, what if the price of yogurt 1 was $0.10/oz instead of $0.08/oz).

Now, lets see how increasing the price of yogurt 1 to $0.10/oz vice $0.08/oz affects its market share. 

First we'll calculate the current market share of each yogurt.

```{python}
y1 = yogurt['y1'].sum()
y2 = yogurt['y2'].sum()
y3 = yogurt['y3'].sum()
y4 = yogurt['y4'].sum()

y1_share = y1 / (y1 + y2 + y3 + y4)
y2_share = y2 / (y1 + y2 + y3 + y4)
y3_share = y3 / (y1 + y2 + y3 + y4)
y4_share = y4 / (y1 + y2 + y3 + y4)

y1_share, y2_share, y3_share, y4_share
```

Yogurt 1 is at 34% market share. Now lets see if our logistic regression model can calculate the same. 

```{python}
# consolidate columns y1 to y4 into a single column

yogurt['yogurt_type'] = yogurt[['y1', 'y2', 'y3', 'y4']].idxmax(axis=1).str[1].astype(int)

```

```{python}
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
import numpy as np


X = yogurt[['f1', 'f2', 'f3', 'f4', 'p1', 'p2', 'p3', 'p4']].values
y = yogurt['yogurt_type'].astype(int).values  


# Fit the multinomial logistic regression model
model = LogisticRegression(multi_class='multinomial', solver='lbfgs', max_iter=1000)
model.fit(X, y)

#  Predict probabilities
probabilities = model.predict_proba(X)

# Calculate original market shares
market_shares_original = probabilities.mean(axis=0)

market_shares_original
```

Our initial market shares calculated above match the logistic regression model's calculation. Lets see how it predicts market shares will change when increasing yogurt 1's price per ounce.

```{python}
yogurt['p1_new'] = yogurt['p1'] + 0.10

X_new = yogurt[['f1', 'f2', 'f3', 'f4', 'p1_new', 'p2', 'p3', 'p4']].values

# Predict new probabilities with the adjusted prices
probabilities_new = model.predict_proba(X_new)

# Calculate new market shares
market_shares_new = probabilities_new.mean(axis=0)

# Output the results
print("Original Market Shares:", market_shares_original)
print("New Market Shares After Price Increase:", market_shares_new)
```

The model predicted that yogurt 1 market shares would decrease by nearly 15% when the price was increase to $0.10/oz.

## 2. Estimating Minivan Preferences

Next, we'll take a look at a different dataset which we'll conduct a conjoint analysis on. The dataset is for preference of features in minivans.

### Data

```{python}
conjoint = pd.read_csv('conjoint.csv')

conjoint.head()
```

```{python}
# number of unique respondents
conjoint['resp.id'].nunique()
```

```{python}
# number of choice tasks
conjoint[conjoint['resp.id'] == 1]['ques'].nunique()
```

```{python}
# number of alternatives
conjoint['alt'].nunique()
```

The data within the conjoint dataset is in long format, meaning it is organized in a way that each row represents a single observation. The data is organized by respondent, choice task, and alternative. There are 200 respondents who took the conjoint survey, and each respondent completed 15 choice tasks. Each choice task presented 3 alternatives to the respondent. Each alternative is represented by a set of attributes, such as brand, price, and feature.

The attributes (levels) were number of seats (6,7,8), cargo space (2ft, 3ft), engine type (gas, hybrid, electric), and price (in thousands of dollars).


### Model

Lets estimate a Multinomial Logistic model to see what features were most preferred.

```{python}
# Converting non-numeric columns to boolean
conjoint = pd.get_dummies(conjoint, columns=['seat', 'cargo', 'eng'], drop_first=False)

# Dropping first column from each feature to prevent multicollinearity
conjoint.drop(['seat_6', 'cargo_2ft', 'eng_gas'], axis=1, inplace=True)

X = conjoint[['price', 'seat_7', 'seat_8', 'cargo_3ft', 'eng_hyb', 'eng_elec']]  # predictors
y = conjoint['choice']  # response variable


# Fit the multinomial logistic regression model
model = LogisticRegression(multi_class='multinomial', solver='lbfgs', max_iter=1000)
model.fit(X, y)

model.coef_
```

### Results

In terms of price, a cheaper price is more preferred as it has a negative coefficient. 

For seats, both 7 and 8 seats have negative coefficients, meaning that seat 6 is the most preferred. 

For cargo space, 3 ft is a positive coefficient, meaning its the most preferred.

Finally, for engine type, both electric and hybrid have negative coefficients, meaning that gas is the most preferred.



Lets see how 3ft of crago space compares to 2ft of argo space in terms of dollar benefit.

```{python}
cargo_3ft = model.coef_[0][3]
price = model.coef_[0][0]

dollar_value = (cargo_3ft / abs(price)) * 1000

dollar_value.round(2)
```

The 3 ft of cargo space has a $2752.15 dollar benefit over the 2 ft cargo space. 



Using the same logic as above for our MNL model, lets predict the market shares for the below data on minivans.

| Minivan | Seats | Cargo | Engine | Price |
|---------|-------|-------|--------|-------|
| A       | 7     | 2     | Hyb    | 30    |
| B       | 6     | 2     | Gas    | 30    |
| C       | 8     | 2     | Gas    | 30    |
| D       | 7     | 3     | Gas    | 40    |
| E       | 6     | 2     | Elec   | 40    |
| F       | 7     | 2     | Hyb    | 35    |

First, we'll create the dataset in python and make it a DataFrame.

```{python}
data = {
    "minivan": ['A', 'B', 'C', 'D', 'E', 'F'],
    "seat": [7, 6, 8, 7, 6, 7],
    "cargo": [2, 2, 2, 3, 2, 2],
    "eng": ['hyb', 'gas', 'gas', 'gas', 'elec', 'hyb'],
    "price": [30, 30, 30, 40, 40, 35]
}

minivans = pd.DataFrame(data)


minivans

```

Next, we'll shape the data as we did in previous models.

```{python}
minivans = pd.get_dummies(minivans, columns=['seat', 'cargo', 'eng'], drop_first=False)

minivans.drop(['seat_6', 'cargo_2', 'eng_gas'], axis=1, inplace=True)

# rename cargo_3 to cargo_3ft

minivans.rename(columns={'cargo_3': 'cargo_3ft'}, inplace=True)

x_minivan = minivans[['price', 'seat_7', 'seat_8', 'cargo_3ft', 'eng_hyb', 'eng_elec']]

predictions = model.predict_proba(x_minivan)[:,1]

predictions

```


```{python}
# add predictions to the minivans dataframe

minivans['predictions'] = predictions

# calculate market share for each

minivans['market_share'] = minivans['predictions']/minivans['predictions'].sum()

minivans
```

As you can see from the results, Minivan B has the highest prediction value and market share with 30%. The lowest is Minuvan E, with only 4.2% market share.

Looking at this logically, Minivan B does have 6 seats which is the most preferred, it has a gas engine which is most preferred, and 2 ft of cargo space which is the least preferred out of the two choices. However, 2 ft of cargo space has a coefficient of 0, rather than negative. 









