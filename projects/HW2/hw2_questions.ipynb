{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "---\n",
        "title: \"Poisson Regression Examples\"\n",
        "author: \"Nicholas Shuckerow\"\n",
        "date: today\n",
        "callout-appearance: minimal # this hides the blue \"i\" icon on .callout-notes\n",
        "editor_options: \n",
        "  chunk_output_type: console\n",
        "---"
      ],
      "id": "b5f1a721"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Blueprinty Case Study\n",
        "\n",
        "### Introduction\n",
        "\n",
        "Blueprinty is a small firm that makes software for developing blueprints specifically for submitting patent applications to the US patent office. Their marketing team would like to make the claim that patent applicants using Blueprinty's software are more successful in getting their patent applications approved. Ideal data to study such an effect might include the success rate of patent applications before using Blueprinty's software and after using it. unfortunately, such data is not available. \n",
        "\n",
        "However, Blueprinty has collected data on 1,500 mature (non-startup) engineering firms. The data include each firm's number of patents awarded over the last 5 years, regional location, age since incorporation, and whether or not the firm uses Blueprinty's software. The marketing team would like to use this data to make the claim that firms using Blueprinty's software are more successful in getting their patent applications approved.\n",
        "\n",
        "\n",
        "### Data\n",
        "\n",
        "_todo: Read in data._"
      ],
      "id": "6f7ef6c2"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from scipy.optimize import minimize\n",
        "\n",
        "# Load the data\n",
        "\n",
        "data = pd.read_csv('blueprinty.csv')\n",
        "\n",
        "data.head()"
      ],
      "id": "ae0705c1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "After reading in the data, we want to confirm that all businesses are unique and are not listed twice in the data\n"
      ],
      "id": "f1613a56"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# count the number of unique values in the column 'Unnamed: 0'\n",
        "\n",
        "data['Unnamed: 0'].nunique()"
      ],
      "id": "45d499da",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Each row is fact for an business which is not repeated in the dataset.\n",
        "\n",
        "_todo: Compare histograms and means of number of patents by customer status. What do you observe?_\n"
      ],
      "id": "c0c7969f"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# count number of customers and non-customers\n",
        "\n",
        "customers = data[data['iscustomer']==1]['iscustomer'].count()\n",
        "non_customers = data[data['iscustomer']==0]['iscustomer'].count()\n",
        "\n",
        "print(f' There are {customers} customers and {non_customers} non-customers in the dataset.')"
      ],
      "id": "1ac37c23",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we will compare histograms based on number of patents for customers and non-customers on two separate plots.\n"
      ],
      "id": "e30cbbd1"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# histogram of # of patents for Customers\n",
        "\n",
        "\n",
        "data[data['iscustomer'] == 1]['patents'].hist()\n",
        "\n",
        "plt.xlabel('Number of Patents')\n",
        "plt.ylabel('Frequency')\n",
        "plt.title('Frequency of Patents for Customers')"
      ],
      "id": "370b0bb5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# histogram of # of patents for non-customers\n",
        "\n",
        "\n",
        "data[data['iscustomer'] == 0]['patents'].hist()\n",
        "\n",
        "plt.xlabel('Number of Patents')\n",
        "plt.ylabel('Frequency')\n",
        "plt.title('Frequency of Patents for Customers')"
      ],
      "id": "20179020",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Next, we'll look at mean number of patents for customers and non-customers so we have a baseline for the histograms\n"
      ],
      "id": "c069cb53"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "mean_patents_customers = data[data['iscustomer'] == 1]['patents'].mean()\n",
        "mean_patents_noncustomers = data[data['iscustomer'] == 0]['patents'].mean()\n",
        "\n",
        "print('Mean patents for customers:', round(mean_patents_customers,2))\n",
        "print('Mean patents for non-customers:', round(mean_patents_noncustomers,2))"
      ],
      "id": "450eeb0c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The number of patents for customers is slightly skewed right, however it has a more normal distribution than the number of patents for non-customers.\n",
        "\n",
        "Both plots have a large drop off around 6 patents (For customers it is slightly less than 6). The number of non-customers is significantly higher than the number of customers, totaling to 1303 non-customers and 197 customers. The mean number of patents for customers was 4.09 and the mean number of patents for non-customers was 3.62. This is a difference of about 0.5 patents. The number of customers and non-customers is important to keep into account when conducting regression models, as non-customers have a higher weight due to the higher frequency of occurence in the dataset. \n",
        "\n",
        "\n",
        "\n",
        "Blueprinty customers are not selected at random. It may be important to account for systematic differences in the age and regional location of customers vs non-customers.\n",
        "\n",
        "_todo: Compare regions and ages by customer status. What do you observe?_\n",
        "\n",
        "First, we'll create a table showing the counts for each region based on customers and non-customers. We'll looking at the proportion which each region has respectively for customers and non-customers.\n"
      ],
      "id": "d13b676e"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# group by region and count number of customers and non-customers\n",
        "\n",
        "region = data.groupby('region')['iscustomer'].value_counts().unstack()\n",
        "\n",
        "# Calculate respective proportion of non-customers and customers which make up each region\n",
        "\n",
        "region['Prop_Non_cust'] = region[0]/(region[0].sum())\n",
        "region['Prop_Cust'] = region[1]/(region[1].sum())\n",
        "\n",
        "# Rename columns from 0 to Non-Customers and 1 to Customers\n",
        "\n",
        "region.rename(columns={0:'Non-Customers', 1:'Customers'}, inplace=True)\n",
        "\n",
        "region"
      ],
      "id": "d0ccd77d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Below, you'll see a plot of the customers and non-customers by region.\n"
      ],
      "id": "6cac64fc"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# create a plot of the number of customers and non-customers by region side by side\n",
        "\n",
        "data.groupby('region')['iscustomer'].value_counts().unstack().plot(kind='bar', stacked=False)\n",
        "\n",
        "plt.xlabel('Region')\n",
        "plt.ylabel('Count')\n",
        "plt.title('Number of Customers and Non-Customers by Region')\n",
        "plt.legend(['Non-Customer', 'Customer'])"
      ],
      "id": "1f0156d4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The Northeast has by far the most customers and non-customers, and the number of non-customers in each region clearly outweighs the number of customers. Although ranking each respective customer and non-customer base by region comes out to be nearly the same ranking, the proportions are different. \n",
        "\n",
        "For the customers, nearly 60% are from the NE, while only 40% of non-customers are from the NE. \n",
        "\n",
        "Next, we'll do the same as we did for regions, except with age. One variation between the two will be binning the age groups for every 5 years. In this case, 0-5 years is one group, 5-10 years is another group, etc.\n"
      ],
      "id": "79762534"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# group by age with bins every 5 years and count number of customers and non-customers\n",
        "\n",
        "data['age_bins'] = pd.cut(data['age'], bins=range(0, 60, 5))\n",
        "\n",
        "age = data.groupby('age_bins')['iscustomer'].value_counts().unstack()\n",
        "age['Prop_Non_cust'] = age[0]/(age[0].sum())\n",
        "age['Prop_Cust'] = age[1]/(age[1].sum())\n",
        "\n",
        "age.rename(columns={0:'Non-Customers', 1:'Customers'}, inplace=True)\n",
        "\n",
        "age"
      ],
      "id": "9aef09c9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# create a plot of the number of customers and non-customers by age side by side\n",
        "\n",
        "data.groupby('age_bins')['iscustomer'].value_counts().unstack().plot(kind='bar', stacked=False)\n",
        "plt.xlabel('Age of Company')\n",
        "plt.ylabel('Count')\n",
        "plt.title('Number of Customers and Non-Customers by Age of Company')\n",
        "plt.legend(['Non-Customer', 'Customer'])"
      ],
      "id": "185e7994",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For age, we'll also look at the mean age of customers and non-customers, and calculate the 95% confidence intervals.\n"
      ],
      "id": "dd3a8c34"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# find mean age for customers and non-customers\n",
        "\n",
        "mean_age_customers = data[data['iscustomer'] == 1]['age'].mean()\n",
        "mean_age_noncustomers = data[data['iscustomer'] == 0]['age'].mean()\n",
        "\n",
        "print('Mean age for customers:', round(mean_age_customers,2))\n",
        "print('Mean age for non-customers:', round(mean_age_noncustomers,2))"
      ],
      "id": "dbc3336f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Calculate a 95% confidence interval for the mean age of customers and non-customers\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "std_age_customers = data[data['iscustomer'] == 1]['age'].std()\n",
        "std_age_noncustomers = data[data['iscustomer'] == 0]['age'].std()\n",
        "\n",
        "n_customers = data[data['iscustomer'] == 1]['age'].count()\n",
        "n_noncustomers = data[data['iscustomer'] == 0]['age'].count()\n",
        "\n",
        "z = 1.96\n",
        "\n",
        "ci_customers = z * (std_age_customers/np.sqrt(n_customers))\n",
        "ci_noncustomers = z * (std_age_noncustomers/np.sqrt(n_noncustomers))\n",
        "\n",
        "print('95% CI for mean age of customers:', round(mean_age_customers-ci_customers,2), round(mean_age_customers+ci_customers,2))\n",
        "print('95% CI for mean age of non-customers:', round(mean_age_noncustomers-ci_noncustomers,2), round(mean_age_noncustomers+ci_noncustomers,2))"
      ],
      "id": "57371f8f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The most customers come from companies which are between 15-20 years old, with the average company age being about 24 years. The most non-customers come from companies which are between 20-25 years old, with the average company age being about 27 years.\n",
        "\n",
        "The distribution of company ages for customers and non-customers resembles a normal distribution, with a slight skew to the right. The largest disparity between the two distributions is at the 10-15 year mark, where the % of non-customers is 10% lower than that of customers (16%, 26%).\n",
        "\n",
        "The confidence interval for the mean age of customers is (23.1, 25.2) and the confidence interval for the mean age of non-customers is (26.3, 27.1). \n",
        "\n",
        "### Estimation of Simple Poisson Model\n",
        "\n",
        "Since our outcome variable of interest can only be small integer values per a set unit of time, we can use a Poisson density to model the number of patents awarded to each engineering firm over the last 5 years. We start by estimating a simple Poisson model via Maximum Likelihood.\n",
        "\n",
        "_todo: Write down mathematically the likelihood for_ $Y \\sim \\text{Poisson}(\\lambda)$. Note that $f(Y|\\lambda) = e^{-\\lambda}\\lambda^Y/Y!$.\n",
        "\n",
        "Below is the log-likelihood function for the Poisson distribution. The likelihood function of lambda given Y is the exact same as the function of Y given lambda.\n",
        "\n",
        "ℓ(λ∣Y)=−λ+Ylog(λ)−log(Y!)\n",
        "\n",
        "_todo: Code the likelihood (or log-likelihood) function for the Poisson model. This is a function of lambda and Y. For example:_\n",
        "\n",
        "Below is the code for the log-likelihood function for a poisson distribution:\n"
      ],
      "id": "1ae7821f"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def poisson_log_likelihood(lam, y):\n",
        "    \"\"\"\n",
        "    Parameters:\n",
        "    - lam (float): The rate parameter (lambda) of the Poisson distribution.\n",
        "    - y (array-like): Array of observed counts.\n",
        "\n",
        "    Returns:\n",
        "    - float: The log likelihood of observing the data given lam.\n",
        "    \"\"\"\n",
        "    y = np.array(y)\n",
        "    n = len(y)  # number of observations\n",
        "    sum_y = np.sum(y)  # sum of all observed counts\n",
        "\n",
        "    # Calculate each part of the log likelihood\n",
        "    # log(P(Y|lam)) = -n * lam + sum_y * log(lam) - log(y_i!)\n",
        "    # We use np.sum(np.log(y_factorials)) to sum log of factorials\n",
        "    log_likelihood = -n * lam + sum_y * np.log(lam) - np.sum([np.log(np.math.factorial(i)) for i in y])\n",
        "    return log_likelihood"
      ],
      "id": "99092de5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "_todo: Use your function to plot lambda on the horizontal axis and the likelihood (or log-likelihood) on the vertical axis for a range of lambdas (use the observed number of patents as the input for Y)._\n",
        "\n",
        "\n",
        "Next, we'll plot the log-likelihoods using our observed number of lambdas as Y and then a range of values for lambda (1-10).\n"
      ],
      "id": "63a7be7b"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "years = range(1,11)\n",
        "log_likelihood_values = []\n",
        "\n",
        "for i in years:\n",
        "    log_likelihood_value = poisson_log_likelihood(i, data['patents'])\n",
        "    log_likelihood_values.append(log_likelihood_value)\n",
        "\n",
        "log_likelihood_values"
      ],
      "id": "91781a0b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "plt.plot(years, log_likelihood_values)\n",
        "plt.xlabel('Lambda')\n",
        "plt.ylabel('Likelihood')\n",
        "plt.title('Likelihood of observing the data given Lambda')\n",
        "plt.show()"
      ],
      "id": "a0816556",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "_todo: If you're feeling mathematical, take the first derivative of your likelihood or log-likelihood, set it equal to zero and solve for lambda. You will find lambda_mle is Ybar, which \"feels right\" because the mean of a Poisson distribution is lambda._\n",
        "\n",
        "_todo: Find the MLE by optimizing your likelihood function with optim() in R or sp.optimize() in Python._\n"
      ],
      "id": "b2e51d3e"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from scipy.optimize import minimize\n",
        "\n",
        "def neg_poisson_log_likelihood(lam, y):\n",
        "    \"\"\"\n",
        "    Parameters:\n",
        "    - lam (float): The rate parameter (lambda) of the Poisson distribution.\n",
        "    - y (array-like): Array of observed counts.\n",
        "\n",
        "    Returns:\n",
        "    - float: The log likelihood of observing the data given lam.\n",
        "    \"\"\"\n",
        "    y = np.array(y)\n",
        "    n = len(y)  # number of observations\n",
        "    sum_y = np.sum(y)  # sum of all observed counts\n",
        "\n",
        "    # Calculate each part of the log likelihood\n",
        "    # log(P(Y|lam)) = -n * lam + sum_y * log(lam) - log(y_i!)\n",
        "    # We use np.sum(np.log(y_factorials)) to sum log of factorials\n",
        "    return -(-n * lam + sum_y * np.log(lam) - np.sum([np.log(np.math.factorial(i)) for i in y]))"
      ],
      "id": "1985b0fa",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "mean = np.mean(data['patents'])\n",
        "\n",
        "result = minimize(neg_poisson_log_likelihood, mean, args=(data['patents']), bounds = [(0, None)])\n",
        "\n",
        "result"
      ],
      "id": "41c7abb8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Estimation of Poisson Regression Model\n",
        "\n",
        "Next, we extend our simple Poisson model to a Poisson Regression Model such that $Y_i = \\text{Poisson}(\\lambda_i)$ where $\\lambda_i = \\exp(X_i'\\beta)$. The interpretation is that the success rate of patent awards is not constant across all firms ($\\lambda$) but rather is a function of firm characteristics $X_i$. Specifically, we will use the covariates age, age squared, region, and whether the firm is a customer of Blueprinty.\n",
        "\n",
        "_todo: Update your likelihood or log-likelihood function with an additional argument to take in a covariate matrix X. Also change the parameter of the model from lambda to the beta vector. In this model, lambda must be a positive number, so we choose the inverse link function g() to be exp() so that_ $\\lambda_i = e^{X_i'\\beta}$. _For example:_\n"
      ],
      "id": "81c888b0"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import numpy as np\n",
        "from scipy.optimize import minimize\n",
        "\n",
        "def poisson_regression_log_likelihood(beta, Y, X):\n",
        "    eta = np.dot(X, beta)\n",
        "    lambda_i = np.exp(eta)\n",
        "    log_likelihood = np.sum(Y * eta - lambda_i)\n",
        "    return -log_likelihood"
      ],
      "id": "f9b88d69",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "_todo: Use your function along with R's optim() or Python's sp.optimize() to find the MLE vector and the Hessian of the Poisson model with covariates. Specifically, the first column of X should be all 1's to enable a constant term in the model, and the subsequent columns should be age, age squared, binary variables for all but one of the regions, and the binary customer variable. Use the Hessian to find standard errors of the beta parameter estimates and present a table of coefficients and standard errors._\n"
      ],
      "id": "e55fc494"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# convert regions column to boolean columns, dropping the first region to be the default value\n",
        "\n",
        "data = pd.get_dummies(data, columns=['region'], drop_first=True)\n",
        "\n",
        "# creating function to convert boolean column to binary\n",
        "\n",
        "def convert_boolean_to_binary(data, column):\n",
        "    data[column] = data[column].astype(int)\n",
        "    return data\n",
        "\n",
        "# coverting region's boolean values to binary\n",
        "\n",
        "data = convert_boolean_to_binary(data, 'region_Northeast')\n",
        "data = convert_boolean_to_binary(data, 'region_South')\n",
        "data = convert_boolean_to_binary(data, 'region_Southwest')\n",
        "data = convert_boolean_to_binary(data, 'region_Northwest')\n",
        "\n",
        "# creating an age^2 column in the dataset\n",
        "\n",
        "data['age_squared'] = data['age']**2"
      ],
      "id": "754fd67a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Load and preprocess your data as before, ensuring that features are scaled\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "scaler = StandardScaler()\n",
        "# Scaling age and age_squared to prevent precision loss from extremely large numbers\n",
        "data['age'] = scaler.fit_transform(data[['age']])\n",
        "data['age_squared'] = scaler.fit_transform(data[['age_squared']])\n",
        "\n",
        "\n",
        "# Defining X and Y variables\n",
        "X = np.c_[np.ones(len(data)), data['age'], data['age_squared'], data['iscustomer'], data['region_Southwest'], data['region_Northwest'],\n",
        "          data['region_Northeast'], data['region_South']]\n",
        "Y = data['patents'].values\n",
        "\n",
        "# Initial guess for beta (0)\n",
        "initial_beta = np.zeros(X.shape[1])\n",
        "\n",
        "# Minimization\n",
        "result = minimize(poisson_regression_log_likelihood, initial_beta, args=(Y, X), method='BFGS')\n",
        "\n",
        "print(\"Optimal beta:\", result.x)"
      ],
      "id": "864f19ad",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "_todo: Check your results using R's glm() function or Python sm.GLM() function._\n"
      ],
      "id": "1538b386"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import statsmodels.api as sm\n",
        "\n",
        "# Fit a Poisson regression model using statsmodels\n",
        "\n",
        "poisson_model = sm.GLM(Y, X, family=sm.families.Poisson()).fit()\n",
        "\n",
        "# Print the summary of the model\n",
        "\n",
        "print(poisson_model.summary())"
      ],
      "id": "4c33ad51",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Calculate Hessian at the optimal beta\n",
        "from scipy.linalg import inv\n",
        "hessian_inv = result.hess_inv  # Inverse Hessian is returned by BFGS\n",
        "\n",
        "# Calculating standard errors by taking the square roots of the diagonal elements of the inverse Hessian\n",
        "std_errors = np.sqrt(np.diag(hessian_inv))\n",
        "\n",
        "print(\"Standard Errors:\", std_errors)"
      ],
      "id": "d7cd4278",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "_todo: Interpret the results. What do you conclude about the effect of Blueprinty's software on patent success?_\n",
        "\n",
        "We can conclude, based on our optimal beta's through our regression model, that Blueprinty's software has a positive effect on the number of patents awarded to a company. The coefficient or beta calculated for \"iscustomer\" is 0.11, meaning if they are a customer of the software, the humber of patents earned increases by 0.11. \n",
        "\n",
        "We also see that p-value of \"iscustomer\" is 0.002, meaning there is only a 0.2% chance that the coefficient has zero affect on the number of patents given the dataset. \n",
        "\n",
        "\n",
        "## AirBnB Case Study\n",
        "\n",
        "### Introduction\n",
        "\n",
        "AirBnB is a popular platform for booking short-term rentals. In March 2017, students Annika Awad, Evan Lebo, and Anna Linden scraped of 40,000 Airbnb listings from New York City.  The data include the following variables:\n",
        "\n",
        ":::: {.callout-note collapse=\"true\"}\n",
        "### Variable Definitions\n",
        "\n",
        "    - `id` = unique ID number for each unit\n",
        "    - `last_scraped` = date when information scraped\n",
        "    - `host_since` = date when host first listed the unit on Airbnb\n",
        "    - `days` = `last_scraped` - `host_since` = number of days the unit has been listed\n",
        "    - `room_type` = Entire home/apt., Private room, or Shared room\n",
        "    - `bathrooms` = number of bathrooms\n",
        "    - `bedrooms` = number of bedrooms\n",
        "    - `price` = price per night (dollars)\n",
        "    - `number_of_reviews` = number of reviews for the unit on Airbnb\n",
        "    - `review_scores_cleanliness` = a cleanliness score from reviews (1-10)\n",
        "    - `review_scores_location` = a \"quality of location\" score from reviews (1-10)\n",
        "    - `review_scores_value` = a \"quality of value\" score from reviews (1-10)\n",
        "    - `instant_bookable` = \"t\" if instantly bookable, \"f\" if not\n",
        "\n",
        "::::\n",
        "\n",
        "\n",
        "_todo: Assume the number of reviews is a good proxy for the number of bookings. Perform some exploratory data analysis to get a feel for the data, handle or drop observations with missing values on relevant variables, build one or more models (e.g., a poisson regression model for the number of bookings as proxied by the number of reviews), and interpret model coefficients to describe variation in the number of reviews as a function of the variables provided._\n",
        "\n",
        "Using the airbnb data, we will build a likelihood function for this poisson regression. However, we first need to read the data and look at its characteristics.\n"
      ],
      "id": "828b5cc0"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# read airbnb csv\n",
        "\n",
        "airbnb = pd.read_csv('airbnb.csv')\n",
        "\n",
        "airbnb.head()\n",
        "\n",
        "````\n",
        "```{python}\n",
        "# count number of listings which have 0 reviews\n",
        "airbnb[airbnb['number_of_reviews'] == 0]['number_of_reviews'].count()"
      ],
      "id": "0233915f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# create subset of data which only shows bedrooms which are nan\n",
        "\n",
        "airbnb.isnull().sum()"
      ],
      "id": "6a0f29e8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Looking at the data initally, we see there are many cells having null values which we need to address. The majority of them are within the review score columns. To address this, we will drop the cells which have 0 days listed (brand new listings). For bedrooms and bathrooms in a shared or private room, we will assume bedrooms is 1 and bathrooms are 0. It is very common for rooms to not have a bathroom. For all other data which is null, we cannot say with reasonable certainty what the value would be. For example, if bedrooms or bathrooms are null for entire home or apartment, we cannot say with reasonable certainty the number of bedrooms or bathrooms.\n"
      ],
      "id": "c0376c4a"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# fill bedrooms with 1 and bathrooms with 0 if nan and if room type is private or shared room\n",
        "\n",
        "airbnb.loc[(airbnb['bedrooms'].isnull()) & (airbnb['room_type'] == 'Private room'), 'bedrooms'] = 1\n",
        "airbnb.loc[(airbnb['bedrooms'].isnull()) & (airbnb['room_type'] == 'Shared room'), 'bedrooms'] = 1\n",
        "airbnb.loc[(airbnb['bathrooms'].isnull()) & (airbnb['room_type'] == 'Private room'), 'bathrooms'] = 0\n",
        "airbnb.loc[(airbnb['bathrooms'].isnull()) & (airbnb['room_type'] == 'Private room'), 'bathrooms'] = 0"
      ],
      "id": "1def58c3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Dropping all other rows which have null values\n",
        "airbnb = airbnb.dropna()"
      ],
      "id": "98094e00",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "airbnb.shape"
      ],
      "id": "87388885",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "airbnb.describe()"
      ],
      "id": "785c6ded",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "airbnb.info()"
      ],
      "id": "4ab411e1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The average number of reviews is 15.9 for this dataset, however the standard deviation is 29.25, meaning there is a large spread in the number of reviews, with many locations having 0 reviews (Right skewed)\n",
        "\n",
        "Most of the listings in the dataset rank very high in cleanliness, location, and value, with all having an average of above 9 out of 10.\n",
        "\n",
        "The price of the listings is also left skewed, with the average price being $145, but the standard deviation being $211, meaning there is a large spread in the price of listings like the number of reviews. However with this spread, due to the fact that the price can't be negative, the distribution is left skewed.\n",
        "\n",
        "While looking at the info for the columns, we will need to convert some of the variables for regression analysis. The columns need be an integer or float data type. We will be changing room type and instant bookable columns.\n"
      ],
      "id": "aec2e16e"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# convert room_type to boolean\n",
        "\n",
        "airbnb = pd.get_dummies(airbnb, columns=['room_type'], drop_first=True)"
      ],
      "id": "ac8c5132",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "for i in airbnb['instant_bookable']:\n",
        "    if i == 't':\n",
        "        airbnb['instant_bookable'] = 1\n",
        "    else:\n",
        "        airbnb['instant_bookable'] = 0"
      ],
      "id": "81c469e0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Prior to building the regression model, we need to make sure the data is all in the same time interval since we are assuming it's a poisson distribution.\n"
      ],
      "id": "70ffaca4"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Converting number of reviews to reviews per year\n",
        "\n",
        "airbnb['reviews_per_year'] = airbnb['number_of_reviews'] / airbnb['days'] * 365"
      ],
      "id": "9188465e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We will also be scaling the data in order to prevent precision loss during our regression analysis.\n"
      ],
      "id": "6b024c49"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "scaler = StandardScaler()\n",
        "# Assuming 'age' and 'age_squared' need scaling\n",
        "airbnb['days_scaled'] = scaler.fit_transform(airbnb[['days']])\n",
        "airbnb['price'] = scaler.fit_transform(airbnb[['price']])\n",
        "airbnb['review_scores_cleanliness'] = scaler.fit_transform(airbnb[['review_scores_cleanliness']])\n",
        "airbnb['review_scores_location'] = scaler.fit_transform(airbnb[['review_scores_location']])\n",
        "airbnb['review_scores_value'] = scaler.fit_transform(airbnb[['review_scores_value']])"
      ],
      "id": "d67ea920",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Next, we will use the previously generated MLE function to get the maximum likelihood estimators to predict our reviews per year. \n"
      ],
      "id": "b5fbd86e"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "X = np.c_[np.ones(len(airbnb)), airbnb['days_scaled'], airbnb['bathrooms'], airbnb['bedrooms'], airbnb['price'], \n",
        "          airbnb['review_scores_cleanliness'], airbnb['review_scores_location'], airbnb['review_scores_value'], \n",
        "          airbnb['instant_bookable'], airbnb['room_type_Private room'], airbnb['room_type_Shared room']]\n",
        "Y = airbnb['reviews_per_year'].values\n",
        "\n",
        "# Initial guess for beta\n",
        "initial_beta = np.zeros(X.shape[1])\n",
        "\n",
        "# Minimization\n",
        "result = minimize(poisson_regression_log_likelihood, initial_beta, args=(Y, X), method='BFGS')\n",
        "\n",
        "print(\"Optimal beta:\", result.x)"
      ],
      "id": "512df2cb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Next, we will confirm our betas with a built-in regression function.\n"
      ],
      "id": "0cbe8921"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Fit a Poisson regression model using statsmodels\n",
        "\n",
        "poisson_model = sm.GLM(Y, X, family=sm.families.Poisson()).fit()\n",
        "\n",
        "# Print the summary of the model\n",
        "\n",
        "print(poisson_model.summary())"
      ],
      "id": "4e7e821f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Our coefficients match, meaning per the data and our manipulation, we calculated the betas correctly.\n",
        "\n",
        "Below, we are grouping the some of the data to potentially get a better explanation and generate a reasonable hypothesis for the outcome of the betas.\n"
      ],
      "id": "ade55f95"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# grouping the data based on days (binning the days) and reviews_per_year (binning reviews per year)\n",
        "\n",
        "airbnb['days_bins'] = pd.cut(airbnb['days'], bins=range(0, 400, 50))\n",
        "\n",
        "airbnb['reviews_per_year_bins'] = pd.cut(airbnb['reviews_per_year'], bins=range(0, 150, 25))\n",
        "\n",
        "days_reviews = airbnb.groupby('days_bins')['reviews_per_year_bins'].value_counts().unstack()\n",
        "\n",
        "days_reviews"
      ],
      "id": "74ff6329",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# making each column in days_reviews proportional to the sum of the number in each row\n",
        "\n",
        "days_reviews['sum'] = days_reviews.sum(axis=1)\n",
        "days_reviews = days_reviews.div(days_reviews['sum'], axis=0)\n",
        "days_reviews.drop(columns='sum', inplace=True)\n",
        "\n",
        "days_reviews"
      ],
      "id": "2ef79907",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Looking at the betas calculated from our Poisson Regression Likelihood model, the first explanatory variable analyzed was days listed. Days listed seems to have a negative affect on the number of reviews, with a beta of -0.91. This is significant considering the other betas. The explanation for this may be that people like to see brand new listings since as time goes on, the listings have more wear and tear, thus review scores start to decrease. However, the other explanation is there is an omitted variable we are missing that was not captured in this dataset.\n",
        "\n",
        "In the days_reviews table above, it shows the proportion of reviews_per_year in comparison to each days bin. It does seem like as the number of days increases, the number of reviews per year decreases with the exception of the first bin (0-25 reviews per year). \n",
        "\n",
        "\n",
        "Bathrooms also has a slightly negative affect on the number of reviews, with a beta of -0.011. This is not as significant as some of the other variables, and may be leading to omitted variable bias. However, an explanation may be that rooms are more popular than homes for the individuals in this area. If they are staying short term, they may have a bathroom outside the room.\n",
        "\n",
        "Bedrooms do have a positive effect on the number of reviews, with a beta of 0.10. Meaning an entire house or apartment is more likely to get more reviews than a shared room. This is significant, and may be due to the fact that people are more likely to stay in a place with more bedrooms if they are traveling with a group. They also may be more likely to leave a review if traveling with a group.\n",
        "\n",
        "Price has a negative effect on reviews per year, with a beta of -0.088. This may be explained by the income class of those staying and the increased prices in NYC. Most people try and spend the least amount of money as possible in order to have a satisfactory experience. \n",
        "\n",
        "Review scores all have a positive effect on reviews_per_year with value having the largest affect. As stated above, most people want to spend the least amount of money for a satisfactory experience, so if a place is respectively inexpensive and has a high value rating by others, than that will attract others to stay.\n",
        "\n",
        "Instant bookable has a significant positive effect on reviews_per_year. This aligns with the American society values. People want to be able to have control at the touch of a button. They lose interest quickly if the have to wait and want immediate feedback. Instant bookings provide that instant feedback.\n",
        "\n",
        "Private and shared rooms have a slightly negative effect, which means that entire homes or apartments are more desirable. Although price is a concern and entire homes or apartments are more expensive, they provide more privacy and space for the guests. If the guests are traveling in groups, that makes this option more affordable, which may explain why it's positive. "
      ],
      "id": "15519908"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}